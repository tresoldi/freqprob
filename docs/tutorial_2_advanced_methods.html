# FreqProb Tutorial 2: Advanced Smoothing Methods

This tutorial covers advanced smoothing techniques that are essential for modern NLP applications:

1. **Simple Good-Turing smoothing** - Using frequency-of-frequencies
2. **Kneser-Ney smoothing** - The gold standard for n-gram models
3. **Modified Kneser-Ney** - Enhanced version with count-dependent discounting
4. **Interpolated smoothing** - Combining multiple models
5. **Bayesian smoothing** - Principled probabilistic approach

## Setup
```python

from collections import Counter, defaultdict

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

import freqprob

# Set up plotting
plt.style.use("default")
sns.set_palette("husl")
np.random.seed(1305)
```

## Dataset Preparation

We'll use a larger, more realistic dataset to demonstrate advanced smoothing methods.
The corpus is built from Python's built-in module docstrings, which provide natural
language text with realistic frequency distributions.
```python

# Create corpus from built-in Python module docstrings
import sys
import re

# Collect docstrings from sys and re modules
corpus_text = ""

# Add sys module docstrings
corpus_text += sys.__doc__ or ""
for name in dir(sys):
    obj = getattr(sys, name)
    if hasattr(obj, "__doc__") and obj.__doc__ and isinstance(obj.__doc__, str):
        corpus_text += "\n" + obj.__doc__

# Add re module docstrings
corpus_text += "\n" + (re.__doc__ or "")
for name in dir(re):
    obj = getattr(re, name)
    if hasattr(obj, "__doc__") and obj.__doc__ and isinstance(obj.__doc__, str):
        corpus_text += "\n" + obj.__doc__

# Tokenize: simple whitespace splitting and lowercase
all_words = corpus_text.lower().split()

freqdist = Counter(all_words)
print("Corpus statistics:")
print(f"Total tokens: {len(all_words)}")
print(f"Unique words: {len(freqdist)}")
print(f"Average frequency: {len(all_words) / len(freqdist):.2f}")

# Show frequency distribution
print("\nMost common words:")
for word, count in freqdist.most_common(10):
    print(f"{word}: {count}")

# Analyze frequency-of-frequencies (crucial for Good-Turing)
freq_of_freqs = Counter(freqdist.values())
print("\nFrequency-of-frequencies (r -> Nr):")
for r in sorted(freq_of_freqs.keys()):
    print(f"r={r}: {freq_of_freqs[r]} words appear {r} time(s)")
```

Output:
```
Corpus statistics:
Total tokens: 7460
Unique words: 1175
Average frequency: 6.35

Most common words:
the: 589
a: 211
is: 174
to: 173
of: 150
if: 149
or: 131
be: 103
and: 96
->: 91

Frequency-of-frequencies (r -> Nr):
r=1: 521 words appear 1 time(s)
r=2: 226 words appear 2 time(s)
r=3: 72 words appear 3 time(s)
r=4: 81 words appear 4 time(s)
r=5: 36 words appear 5 time(s)
r=6: 49 words appear 6 time(s)
r=7: 34 words appear 7 time(s)
r=8: 18 words appear 8 time(s)
r=9: 14 words appear 9 time(s)
r=10: 9 words appear 10 time(s)
r=11: 5 words appear 11 time(s)
r=12: 7 words appear 12 time(s)
r=13: 4 words appear 13 time(s)
r=14: 7 words appear 14 time(s)
r=15: 3 words appear 15 time(s)
r=16: 2 words appear 16 time(s)
r=17: 5 words appear 17 time(s)
r=18: 2 words appear 18 time(s)
r=19: 2 words appear 19 time(s)
r=20: 1 words appear 20 time(s)
r=21: 6 words appear 21 time(s)
r=23: 2 words appear 23 time(s)
r=24: 2 words appear 24 time(s)
r=25: 20 words appear 25 time(s)
r=26: 2 words appear 26 time(s)
r=27: 1 words appear 27 time(s)
r=28: 2 words appear 28 time(s)
r=29: 1 words appear 29 time(s)
r=31: 2 words appear 31 time(s)
r=32: 1 words appear 32 time(s)
r=33: 2 words appear 33 time(s)
r=34: 2 words appear 34 time(s)
r=35: 1 words appear 35 time(s)
r=36: 1 words appear 36 time(s)
r=37: 1 words appear 37 time(s)
r=40: 1 words appear 40 time(s)
r=42: 1 words appear 42 time(s)
r=45: 1 words appear 45 time(s)
r=46: 1 words appear 46 time(s)
r=48: 2 words appear 48 time(s)
r=50: 1 words appear 50 time(s)
r=52: 1 words appear 52 time(s)
r=53: 1 words appear 53 time(s)
r=54: 1 words appear 54 time(s)
r=55: 1 words appear 55 time(s)
r=56: 1 words appear 56 time(s)
r=62: 1 words appear 62 time(s)
r=64: 1 words appear 64 time(s)
r=65: 1 words appear 65 time(s)
r=66: 1 words appear 66 time(s)
r=68: 1 words appear 68 time(s)
r=73: 1 words appear 73 time(s)
r=80: 1 words appear 80 time(s)
r=87: 1 words appear 87 time(s)
r=89: 1 words appear 89 time(s)
r=91: 1 words appear 91 time(s)
r=96: 1 words appear 96 time(s)
r=103: 1 words appear 103 time(s)
r=131: 1 words appear 131 time(s)
r=149: 1 words appear 149 time(s)
r=150: 1 words appear 150 time(s)
r=173: 1 words appear 173 time(s)
r=174: 1 words appear 174 time(s)
r=211: 1 words appear 211 time(s)
r=589: 1 words appear 589 time(s)
```

## Simple Good-Turing Smoothing

Simple Good-Turing (SGT) is a frequency-based smoothing method that uses frequency-of-frequencies
statistics to estimate probabilities. The key insight: if we observed N₁ words that appear once,
we should expect about N₁ words we haven't seen yet.

### Understanding Total Mass vs Per-Word Probability

Good-Turing estimates p₀, the **total probability mass** for ALL unseen words combined.
To get the probability for a single unseen word, we need to divide by the estimated number
of unseen types. The `bins` parameter controls this calculation:

- **bins**: Total vocabulary size (observed + unobserved types)
- **Default**: bins = V + N₁ (observed types + singleton count)
- **Per-word unseen probability**: p₀ / (bins - V)

This ensures probabilities are meaningful: P(word₁) + P(word₂) makes sense for different words.
```python

# Create Simple Good-Turing model
try:
    # Default behavior: bins = V + N₁ (observed types + singleton count)
    sgt = freqprob.SimpleGoodTuring(freqdist, logprob=False)
    print("Simple Good-Turing model created successfully")
    print(f"Default bins: {len(freqdist)} (observed) + {freq_of_freqs.get(1, 0)} (singletons) = {len(freqdist) + freq_of_freqs.get(1, 0)}")

    # Test on sample words from the corpus (sys/re module docstrings)
    # Use common programming terms and one unseen word
    test_words = ["the", "string", "pattern", "xyzabc"]  # Last one is unseen

    print("\nSimple Good-Turing probabilities:")
    for word in test_words:
        original_count = freqdist.get(word, 0)
        sgt_prob = sgt(word)
        status = "unseen" if original_count == 0 else f"count: {original_count}"
        print(f"P({word:<10}) = {sgt_prob:.6f} ({status})")

    # Compare with MLE
    mle_comparison = freqprob.MLE(freqdist, logprob=False)

    print("\nMLE vs Simple Good-Turing comparison:")
    print(f"{'Word':<10} {'MLE':<10} {'SGT':<10} {'Difference':<10}")
    print("-" * 40)
    for word in test_words:
        mle_prob = mle_comparison(word)
        sgt_prob = sgt(word)
        diff = sgt_prob - mle_prob
        print(f"{word:<10} {mle_prob:<10.6f} {sgt_prob:<10.6f} {diff:<10.6f}")

    # Demonstrate total mass vs per-word probability
    print("\n" + "=" * 60)
    print("TOTAL MASS vs PER-WORD PROBABILITY")
    print("=" * 60)

    per_word_prob = sgt("unseen_word")
    total_unseen_mass = sgt.total_unseen_mass
    total_observed_mass = sum(sgt(word) for word in freqdist)

    print(f"\nTotal probability mass (p₀): {total_unseen_mass:.6f}")
    print(f"Per-word unseen probability: {per_word_prob:.6f}")
    print(f"Ratio (total/per-word): {total_unseen_mass / per_word_prob:.0f} estimated unseen types")
    print(f"\nTotal probability for observed words: {total_observed_mass:.6f}")
    print(f"Total probability for all unseen words (p₀): {total_unseen_mass:.6f}")
    print(f"Sum (should be ≈ 1.0): {total_observed_mass + total_unseen_mass:.6f}")

    # Verify probability semantics
    print("\n" + "=" * 60)
    print("VERIFYING PROBABILITY SEMANTICS")
    print("=" * 60)

    # Two different unseen words should have same probability
    unseen1 = sgt("xyzabc")
    unseen2 = sgt("qwerty")
    print(f"\nP(xyzabc) = {unseen1:.6f}")
    print(f"P(qwerty) = {unseen2:.6f}")
    print(f"P(xyzabc) + P(qwerty) = {unseen1 + unseen2:.6f}")
    print(f"This sum is meaningful because each returns PER-WORD probability")

    # Demonstrate effect of bins parameter
    print("\n" + "=" * 60)
    print("EFFECT OF BINS PARAMETER")
    print("=" * 60)

    bins_values = [
        len(freqdist) + freq_of_freqs.get(1, 0),  # Default: V + N₁
        len(freqdist) * 2,                         # 2x observed vocabulary
        len(freqdist) * 5,                         # 5x observed vocabulary
        10000,                                     # Fixed large vocabulary
    ]

    print(f"\nObserved vocabulary size (V): {len(freqdist)}")
    print(f"Singleton count (N₁): {freq_of_freqs.get(1, 0)}")
    print(f"\n{'bins':<10} {'P(unseen)':<12} {'Est. Unseen Types':<20} {'Description':<30}")
    print("-" * 80)

    for bins in bins_values:
        sgt_test = freqprob.SimpleGoodTuring(freqdist, bins=bins, logprob=False)
        unseen_prob = sgt_test("xyzabc")
        est_unseen = int(sgt_test.total_unseen_mass / unseen_prob)

        if bins == bins_values[0]:
            desc = "Default (V + N₁)"
        elif bins == bins_values[1]:
            desc = "2x observed vocabulary"
        elif bins == bins_values[2]:
            desc = "5x observed vocabulary"
        else:
            desc = "Fixed large vocabulary"

        print(f"{bins:<10} {unseen_prob:<12.6f} {est_unseen:<20} {desc:<30}")

    print("\nKey insight: Larger bins → smaller per-word unseen probability")
    print("Choose bins based on your domain knowledge of total vocabulary size")

    # Show compatibility with perplexity calculation
    print("\n" + "=" * 60)
    print("COMPATIBILITY WITH PERPLEXITY")
    print("=" * 60)

    # Create log-probability version for perplexity
    sgt_log = freqprob.SimpleGoodTuring(freqdist, logprob=True)

    # Small test set with words relevant to sys/re documentation
    test_sample = ["the", "string", "module", "function", "xyzabc"]

    print(f"\nTest words: {test_sample}")
    print(f"\nLog-probabilities:")
    for word in test_sample:
        logprob = sgt_log(word)
        prob = np.exp(logprob)
        status = "(unseen)" if word == "xyzabc" else "(observed)"
        print(f"  {word:<10}: log P = {logprob:8.4f}, P = {prob:.6f} {status}")

    # Calculate perplexity
    perp = freqprob.perplexity(sgt_log, test_sample)
    print(f"\nPerplexity: {perp:.2f}")
    print("Lower perplexity = better model fit to this test data")

except Exception as e:
    print(f"Error creating Simple Good-Turing model: {e}")
    print("This can happen with small datasets or irregular frequency patterns")
    sgt = None
```

Output:
```
Simple Good-Turing model created successfully
Default bins: 1175 (observed) + 521 (singletons) = 1696

Simple Good-Turing probabilities:
P(the       ) = 0.077690 (count: 589)
P(string    ) = 0.008869 (count: 68)
P(pattern   ) = 0.002532 (count: 20)
P(xyzabc    ) = 0.000134 (unseen)

MLE vs Simple Good-Turing comparison:
Word       MLE        SGT        Difference
----------------------------------------
the        0.078954   0.077690   -0.001264 
string     0.009115   0.008869   -0.000246 
pattern    0.002681   0.002532   -0.000149 
xyzabc     0.000000   0.000134   0.000134  

============================================================
TOTAL MASS vs PER-WORD PROBABILITY
============================================================

Total probability mass (p₀): 0.069839
Per-word unseen probability: 0.000134
Ratio (total/per-word): 521 estimated unseen types

Total probability for observed words: 0.930161
Total probability for all unseen words (p₀): 0.069839
Sum (should be ≈ 1.0): 1.000000

============================================================
VERIFYING PROBABILITY SEMANTICS
============================================================

P(xyzabc) = 0.000134
P(qwerty) = 0.000134
P(xyzabc) + P(qwerty) = 0.000268
This sum is meaningful because each returns PER-WORD probability

============================================================
EFFECT OF BINS PARAMETER
============================================================

Observed vocabulary size (V): 1175
Singleton count (N₁): 521

bins       P(unseen)    Est. Unseen Types    Description                   
--------------------------------------------------------------------------------
1696       0.000134     521                  Default (V + N₁)              
2350       0.000059     1175                 2x observed vocabulary        
5875       0.000015     4700                 5x observed vocabulary        
10000      0.000008     8825                 Fixed large vocabulary        

Key insight: Larger bins → smaller per-word unseen probability
Choose bins based on your domain knowledge of total vocabulary size

============================================================
COMPATIBILITY WITH PERPLEXITY
============================================================

Test words: ['the', 'string', 'module', 'function', 'xyzabc']

Log-probabilities:
  the       : log P =  -2.5550, P = 0.077690 (observed)
  string    : log P =  -4.7252, P = 0.008869 (observed)
  module    : log P =  -5.9279, P = 0.002664 (observed)
  function  : log P =  -5.9279, P = 0.002664 (observed)
  xyzabc    : log P =  -8.9173, P = 0.000134 (unseen)

Perplexity: 273.33
Lower perplexity = better model fit to this test data
```

## N-gram Language Models Setup

For Kneser-Ney smoothing, we need to work with n-grams. Let's create bigram data.
```python

# Generate bigrams from our corpus
# We'll use sentence boundaries based on punctuation

def generate_bigrams(words):
    """Generate bigrams from a list of words, treating periods as sentence boundaries."""
    bigrams = []
    # Add sentence start marker
    prev_word = "<s>"
    for word in words:
        # Check if this word ends with sentence-ending punctuation
        if word.endswith(('.', '!', '?', ':', ';')):
            # Add bigram with current word
            bigrams.append((prev_word, word))
            # Next bigram starts a new sentence
            prev_word = "<s>"
        else:
            bigrams.append((prev_word, word))
            prev_word = word
    # Add final sentence end marker
    bigrams.append((prev_word, "</s>"))
    return bigrams


bigrams = generate_bigrams(all_words)
bigram_freqdist = Counter(bigrams)

print("Bigram statistics:")
print(f"Total bigrams: {len(bigrams)}")
print(f"Unique bigrams: {len(bigram_freqdist)}")

print("\nMost common bigrams:")
for bigram, count in bigram_freqdist.most_common(10):
    print(f"{bigram}: {count}")

# Also create context counts for Kneser-Ney
context_counts = Counter()
word_contexts = defaultdict(set)

for (context, word), count in bigram_freqdist.items():
    context_counts[context] += count
    word_contexts[word].add(context)

print(f"\nNumber of unique contexts: {len(context_counts)}")
print("Most frequent contexts:")
for context, count in context_counts.most_common(5):
    print(f"'{context}': {count}")
```

Output:
```
Bigram statistics:
Total bigrams: 7461
Unique bigrams: 2562

Most common bigrams:
('<s>', 'if'): 91
('the', 'given'): 66
('defaults', 'to'): 65
('of', 'the'): 61
('<s>', 'the'): 58
('->', 'str'): 50
('in', 'the'): 39
('from', 'the'): 37
('will', 'be'): 37
('a', 'new'): 34

Number of unique contexts: 956
Most frequent contexts:
'<s>': 662
'the': 589
'a': 211
'is': 174
'to': 173
```

## Kneser-Ney Smoothing

Kneser-Ney is the gold standard for n-gram language models. It uses absolute discounting and continuation probabilities.
```python

# Create Kneser-Ney model
try:
    # Default discount of 0.75 is commonly used
    kn = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=False)
    print("Kneser-Ney model created successfully")

    # Test on various bigrams from programming documentation
    test_bigrams = [
        ("the", "string"),
        ("the", "function"),
        ("in", "the"),
        ("of", "module"),
        ("xyzabc", "qwerty"),  # Unseen bigram
    ]

    print("\nKneser-Ney probabilities:")
    for bigram in test_bigrams:
        original_count = bigram_freqdist.get(bigram, 0)
        kn_prob = kn(bigram)
        print(f"{bigram!s:<20} (count={original_count}): P = {kn_prob:.6f}")

    # Compare with bigram MLE
    bigram_mle = freqprob.MLE(bigram_freqdist, logprob=False)

    observed_bigrams = [bg for bg in test_bigrams if bg in bigram_freqdist]
    kn_probs = [kn(bg) for bg in observed_bigrams]
    mle_probs = [bigram_mle(bg) for bg in observed_bigrams]

    plt.figure(figsize=(12, 6))
    x = np.arange(len(observed_bigrams))
    width = 0.35

    plt.bar(x - width / 2, mle_probs, width, label="Bigram MLE", alpha=0.8)
    plt.bar(x + width / 2, kn_probs, width, label="Kneser-Ney", alpha=0.8)

    plt.title("Bigram MLE vs Kneser-Ney")
    plt.xlabel("Bigrams")
    plt.ylabel("Probability")
    plt.xticks(x, [str(bg) for bg in observed_bigrams], rotation=45)
    plt.legend()
    plt.tight_layout()

    # Demonstrate continuation probability concept
    print("\nContinuation probability insight:")
    words_to_analyze = ["the", "string", "module"]

    for word in words_to_analyze:
        # Count how many different contexts this word appears in
        contexts = len(word_contexts[word])
        total_count = sum(count for (ctx, w), count in bigram_freqdist.items() if w == word)
        print(f"'{word}': appears {total_count} times in {contexts} different contexts")

    print("\nKneser-Ney favors words that appear in many different contexts!")

except Exception as e:
    print(f"Error creating Kneser-Ney model: {e}")
    kn = None
```

Output:
```
Kneser-Ney model created successfully

Kneser-Ney probabilities:
('the', 'string')    (count=13): P = 0.021935
('the', 'function')  (count=0): P = 0.000850
('in', 'the')        (count=39): P = 0.444706
('of', 'module')     (count=1): P = 0.002457
('xyzabc', 'qwerty') (count=0): P = 0.000850

Continuation probability insight:
'the': appears 589 times in 79 different contexts
'string': appears 68 times in 13 different contexts
'module': appears 21 times in 9 different contexts

Kneser-Ney favors words that appear in many different contexts!
```

![Figure](docs/figures/figure_0.png)

## Modified Kneser-Ney Smoothing

Modified Kneser-Ney uses different discount values for different frequency counts.
```python

# Create Modified Kneser-Ney model
try:
    mkn = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=False)
    print("Modified Kneser-Ney model created successfully")

    # Compare KN vs MKN
    if kn is not None:
        print("\nComparison: Kneser-Ney vs Modified Kneser-Ney")
        print("-" * 55)

        for bigram in test_bigrams[:4]:  # Skip unseen for cleaner comparison
            count = bigram_freqdist.get(bigram, 0)
            kn_prob = kn(bigram)
            mkn_prob = mkn(bigram)
            print(f"{bigram!s:<20} (c={count}): KN={kn_prob:.6f}, MKN={mkn_prob:.6f}")

        # Visualize differences
        bigrams_to_plot = [bg for bg in test_bigrams[:4] if bg in bigram_freqdist]
        kn_probs_plot = [kn(bg) for bg in bigrams_to_plot]
        mkn_probs_plot = [mkn(bg) for bg in bigrams_to_plot]

        plt.figure(figsize=(12, 6))
        x = np.arange(len(bigrams_to_plot))
        width = 0.35

        plt.bar(x - width / 2, kn_probs_plot, width, label="Kneser-Ney", alpha=0.8)
        plt.bar(x + width / 2, mkn_probs_plot, width, label="Modified Kneser-Ney", alpha=0.8)

        plt.title("Kneser-Ney vs Modified Kneser-Ney")
        plt.xlabel("Bigrams")
        plt.ylabel("Probability")
        plt.xticks(x, [str(bg) for bg in bigrams_to_plot], rotation=45)
        plt.legend()
        plt.tight_layout()

        # Show discount values used by MKN
        print("\nModified Kneser-Ney uses count-dependent discounts:")
        print("- Different discount values for counts 1, 2, and 3+")
        print("- Automatically estimated from frequency-of-frequencies")

    else:
        print("Regular Kneser-Ney not available for comparison")

except Exception as e:
    print(f"Error creating Modified Kneser-Ney model: {e}")
    mkn = None
```

Output:
```
Modified Kneser-Ney model created successfully

Comparison: Kneser-Ney vs Modified Kneser-Ney
-------------------------------------------------------
('the', 'string')    (c=13): KN=0.021935, MKN=0.020620
('the', 'function')  (c=0): KN=0.000850, MKN=0.000850
('in', 'the')        (c=39): KN=0.444706, MKN=0.433860
('of', 'module')     (c=1): KN=0.002457, MKN=0.003802

Modified Kneser-Ney uses count-dependent discounts:
- Different discount values for counts 1, 2, and 3+
- Automatically estimated from frequency-of-frequencies
```

![Figure](docs/figures/figure_1.png)

## Interpolated Smoothing

Interpolated smoothing combines multiple models (e.g., trigram with bigram fallback).
The method automatically detects n-gram interpolation mode when distributions have
different tuple lengths, extracting lower-order context from higher-order n-grams.
```python

# Generate trigrams for interpolation example


def generate_trigrams(words):
    """Generate trigrams from a list of words, treating periods as sentence boundaries."""
    trigrams = []
    # Start with two sentence boundary markers
    context = ["<s>", "<s>"]

    for word in words:
        # Add the trigram
        trigrams.append((context[0], context[1], word))

        # Check if this word ends with sentence-ending punctuation
        if word.endswith((".", "!", "?", ":", ";")):
            # Reset context for new sentence
            context = ["<s>", "<s>"]
        else:
            # Shift context window
            context = [context[1], word]

    # Add final trigrams with sentence end markers
    trigrams.append((context[0], context[1], "</s>"))
    trigrams.append((context[1], "</s>", "</s>"))

    return trigrams


trigrams = generate_trigrams(all_words)
trigram_freqdist = Counter(trigrams)

print("Trigram statistics:")
print(f"Total trigrams: {len(trigrams)}")
print(f"Unique trigrams: {len(trigram_freqdist)}")

print("\nMost common trigrams:")
for trigram, count in trigram_freqdist.most_common(5):
    print(f"{trigram}: {count}")

# Create interpolated model (trigram + bigram)
try:
    # Lambda weight controls interpolation: λ * P_high + (1-λ) * P_low
    lambda_weight = 0.7  # Favor trigrams

    interpolated = freqprob.InterpolatedSmoothing(
        trigram_freqdist, bigram_freqdist, lambda_weight=lambda_weight, logprob=False
    )

    print(f"\nInterpolated model created (λ = {lambda_weight})")

    # Test on trigrams from programming documentation
    test_trigrams = [
        ("the", "string", "object"),
        ("in", "the", "module"),
        ("<s>", "the", "function"),
        ("pattern", "in", "the"),
    ]

    print("\nInterpolated smoothing probabilities:")
    print("(Using n-gram mode: extracts bigram context from trigrams)")
    for trigram in test_trigrams:
        count = trigram_freqdist.get(trigram, 0)
        prob = interpolated(trigram)
        # Extract bigram context for display
        bigram_context = (trigram[1], trigram[2])
        bigram_count = bigram_freqdist.get(bigram_context, 0)
        print(f"{trigram!s:<25} (count={count}, context {bigram_context} count={bigram_count}): P = {prob:.6f}")

    # Compare different lambda values
    lambda_values = [0.1, 0.3, 0.5, 0.7, 0.9]
    test_trigram = ("in", "the", "module")

    probs_by_lambda = []
    for lam in lambda_values:
        interp_model = freqprob.InterpolatedSmoothing(
            trigram_freqdist, bigram_freqdist, lambda_weight=lam, logprob=False
        )
        prob = interp_model(test_trigram)
        probs_by_lambda.append(prob)

    plt.figure(figsize=(10, 6))
    plt.plot(lambda_values, probs_by_lambda, "o-", linewidth=2, markersize=8)
    plt.title(f"Effect of lambda on P{test_trigram}")
    plt.xlabel("Lambda (lambda) - Weight for Higher-order Model")
    plt.ylabel("Probability")
    plt.grid(True, alpha=0.3)
    plt.axvline(
        x=lambda_weight,
        color="red",
        linestyle="--",
        alpha=0.7,
        label=f"Current lambda = {lambda_weight}",
    )
    plt.legend()
    plt.tight_layout()

    print("\nAs lambda increases, we rely more on trigram model (more specific context)")
    print("As lambda decreases, we rely more on bigram model (more general, better coverage)")

except Exception as e:
    print(f"Error creating interpolated model: {e}")
    interpolated = None
```

Output:
```
Trigram statistics:
Total trigrams: 7462
Unique trigrams: 3100

Most common trigrams:
('<s>', '<s>', 'if'): 91
('<s>', '<s>', 'the'): 58
('will', 'be', 'decoded'): 29
('<s>', '<s>', 'encoding'): 29
('<s>', '<s>', 'errors'): 29

Interpolated model created (λ = 0.7)

Interpolated smoothing probabilities:
(Using n-gram mode: extracts bigram context from trigrams)
('the', 'string', 'object') (count=0, context ('string', 'object') count=26): P = 0.001045
('in', 'the', 'module')   (count=0, context ('the', 'module') count=8): P = 0.000322
('<s>', 'the', 'function') (count=0, context ('the', 'function') count=0): P = 0.000000
('pattern', 'in', 'the')  (count=1, context ('in', 'the') count=39): P = 0.001662

As lambda increases, we rely more on trigram model (more specific context)
As lambda decreases, we rely more on bigram model (more general, better coverage)
```

![Figure](docs/figures/figure_2.png)

## Bayesian Smoothing

Bayesian smoothing uses a Dirichlet prior for theoretically principled probability estimates.
```python

# Create Bayesian smoothing models with different priors
alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0]
bayesian_models = {}

for alpha in alpha_values:
    bayesian_models[alpha] = freqprob.BayesianSmoothing(freqdist, alpha=alpha, logprob=False)

print("Bayesian Smoothing with different alpha (concentration parameters):")
print("=" * 65)

test_words = ["the", "string", "module", "xyzabc"]  # Last one unseen

for word in test_words:
    count = freqdist.get(word, 0)
    print(f"\nWord: '{word}' (count = {count})")
    print(f"{'alpha':<6} {'Probability':<12} {'Effect':<20}")
    print("-" * 40)

    for alpha in alpha_values:
        prob = bayesian_models[alpha](word)
        if alpha == 0.1:
            effect = "Minimal smoothing"
        elif alpha == 1.0:
            effect = "Uniform prior (Laplace)"
        elif alpha > 1.0:
            effect = "Strong uniform bias"
        else:
            effect = "Light smoothing"

        print(f"{alpha:<6.1f} {prob:<12.6f} {effect}")

# Visualize the effect of alpha
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.ravel()

for i, word in enumerate(test_words):
    if i >= 4:
        break

    probs = [bayesian_models[alpha](word) for alpha in alpha_values]

    axes[i].semilogx(alpha_values, probs, "o-", linewidth=2, markersize=8)
    axes[i].set_title(f'P("{word}") vs alpha (count = {freqdist.get(word, 0)})')
    axes[i].set_xlabel("Alpha (alpha)")
    axes[i].set_ylabel("Probability")
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()

# Compare Bayesian with other methods
print("\nComparison with other smoothing methods:")
print("=" * 45)

# Create comparison models
laplace_model = freqprob.Laplace(freqdist, bins=1000, logprob=False)
bayesian_model = bayesian_models[1.0]  # alpha = 1 is equivalent to Laplace
optimal_bayesian = bayesian_models[0.5]  # Often a good choice

comparison_models = {
    "Laplace": laplace_model,
    "Bayesian (alpha=1.0)": bayesian_model,
    "Bayesian (alpha=0.5)": optimal_bayesian,
}

for word in ["the", "string", "xyzabc"]:
    print(f"\nP('{word}'):")
    for name, model in comparison_models.items():
        prob = model(word)
        print(f"  {name:<18}: {prob:.6f}")

print("\nKey insight: Bayesian smoothing with alpha=1.0 is equivalent to Laplace!")
print("The alpha parameter controls the strength of the uniform prior.")
```

Output:
```
Bayesian Smoothing with different alpha (concentration parameters):
=================================================================

Word: 'the' (count = 589)
alpha  Probability  Effect              
----------------------------------------
0.1    0.077743     Minimal smoothing
0.5    0.073253     Light smoothing
1.0    0.068327     Uniform prior (Laplace)
2.0    0.060245     Strong uniform bias
5.0    0.044544     Strong uniform bias

Word: 'string' (count = 68)
alpha  Probability  Effect              
----------------------------------------
0.1    0.008987     Minimal smoothing
0.5    0.008512     Light smoothing
1.0    0.007991     Uniform prior (Laplace)
2.0    0.007136     Strong uniform bias
5.0    0.005474     Strong uniform bias

Word: 'module' (count = 21)
alpha  Probability  Effect              
----------------------------------------
0.1    0.002785     Minimal smoothing
0.5    0.002672     Light smoothing
1.0    0.002548     Uniform prior (Laplace)
2.0    0.002345     Strong uniform bias
5.0    0.001950     Strong uniform bias

Word: 'xyzabc' (count = 0)
alpha  Probability  Effect              
----------------------------------------
0.1    0.000013     Minimal smoothing
0.5    0.000062     Light smoothing
1.0    0.000116     Uniform prior (Laplace)
2.0    0.000204     Strong uniform bias
5.0    0.000375     Strong uniform bias

Comparison with other smoothing methods:
=============================================

P('the'):
  Laplace           : 0.069740
  Bayesian (alpha=1.0): 0.068327
  Bayesian (alpha=0.5): 0.073253

P('string'):
  Laplace           : 0.008156
  Bayesian (alpha=1.0): 0.007991
  Bayesian (alpha=0.5): 0.008512

P('xyzabc'):
  Laplace           : 0.000118
  Bayesian (alpha=1.0): 0.000116
  Bayesian (alpha=0.5): 0.000062

Key insight: Bayesian smoothing with alpha=1.0 is equivalent to Laplace!
The alpha parameter controls the strength of the uniform prior.
```

![Figure](docs/figures/figure_3.png)

# Create test set
test_corpus = [
    "the elephant walks slowly through the dense jungle",
    "wild animals search for food in the morning",
    "cats climb trees to escape from dangerous predators",
]

test_words = []
for sentence in test_corpus:
    test_words.extend(sentence.split())

print(f"Test set: {len(test_words)} words")
print(f"Words: {test_words}")

# Evaluate unigram models
print("\nUnigram Model Evaluation (Perplexity):")
print("=" * 40)

unigram_models = {
    "MLE": freqprob.MLE(freqdist, logprob=True),
    "Laplace": freqprob.Laplace(freqdist, bins=1000, logprob=True),
    "Bayesian (alpha=0.5)": freqprob.BayesianSmoothing(freqdist, alpha=0.5, logprob=True),
}

if sgt is not None:
    unigram_models["Simple Good-Turing"] = freqprob.SimpleGoodTuring(freqdist, logprob=True)

unigram_perplexities = {}
for name, model in unigram_models.items():
    try:
        pp = freqprob.perplexity(model, test_words)
        unigram_perplexities[name] = pp
        print(f"{name:<20}: {pp:.2f}")
    except Exception as e:
        print(f"{name:<20}: Error - {str(e)[:30]}...")

# Evaluate bigram models (on bigram test data)
test_bigrams = []
for sentence in test_corpus:
    words = ["<s>", *sentence.split(), "</s>"]
    for i in range(len(words) - 1):
        test_bigrams.append((words[i], words[i + 1]))

print("\nBigram Model Evaluation (Perplexity):")
print("=" * 40)

bigram_models = {"Bigram MLE": freqprob.MLE(bigram_freqdist, logprob=True)}

if kn is not None:
    bigram_models["Kneser-Ney"] = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=True)

if mkn is not None:
    bigram_models["Modified Kneser-Ney"] = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=True)

bigram_perplexities = {}
for name, model in bigram_models.items():
    try:
        pp = freqprob.perplexity(model, test_bigrams)
        bigram_perplexities[name] = pp
        print(f"{name:<20}: {pp:.2f}")
    except Exception as e:
        print(f"{name:<20}: Error - {str(e)[:30]}...")

# Visualize results
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Unigram models
if unigram_perplexities:
    methods = list(unigram_perplexities.keys())
    values = list(unigram_perplexities.values())

    bars1 = ax1.bar(methods, values, alpha=0.8, color=plt.cm.Set3(np.linspace(0, 1, len(methods))))
    ax1.set_title("Unigram Model Perplexity Comparison")
    ax1.set_ylabel("Perplexity (lower is better)")
    ax1.tick_params(axis="x", rotation=45)

    # Add value labels
    for bar, value in zip(bars1, values, strict=False):
        ax1.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.5,
            f"{value:.1f}",
            ha="center",
            va="bottom",
        )

# Bigram models
if bigram_perplexities:
    methods = list(bigram_perplexities.keys())
    values = list(bigram_perplexities.values())

    bars2 = ax2.bar(methods, values, alpha=0.8, color=plt.cm.Set2(np.linspace(0, 1, len(methods))))
    ax2.set_title("Bigram Model Perplexity Comparison")
    ax2.set_ylabel("Perplexity (lower is better)")
    ax2.tick_params(axis="x", rotation=45)

    # Add value labels
    for bar, value in zip(bars2, values, strict=False):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.5,
            f"{value:.1f}",
            ha="center",
            va="bottom",
        )

plt.tight_layout()

# Find best models
if unigram_perplexities:
    best_unigram = min(unigram_perplexities.items(), key=lambda x: x[1])
    print(f"\nBest unigram model: {best_unigram[0]} (PP = {best_unigram[1]:.2f})")

if bigram_perplexities:
    best_bigram = min(bigram_perplexities.items(), key=lambda x: x[1])
    print(f"Best bigram model: {best_bigram[0]} (PP = {best_bigram[1]:.2f})")
```python

# Create test set
test_corpus = [
    "the function returns a string object from the module",
    "pattern matching uses regular expressions for text processing",
    "the module provides methods for searching and replacing strings",
]

test_words = []
for sentence in test_corpus:
    test_words.extend(sentence.split())

print(f"Test set: {len(test_words)} words")
print(f"Words: {test_words}")

# Evaluate unigram models
print("\nUnigram Model Evaluation (Perplexity):")
print("=" * 40)

unigram_models = {
    "MLE": freqprob.MLE(freqdist, logprob=True),
    "Laplace": freqprob.Laplace(freqdist, bins=1000, logprob=True),
    "Bayesian (alpha=0.5)": freqprob.BayesianSmoothing(freqdist, alpha=0.5, logprob=True),
}

if sgt is not None:
    unigram_models["Simple Good-Turing"] = freqprob.SimpleGoodTuring(freqdist, logprob=True)

unigram_perplexities = {}
for name, model in unigram_models.items():
    try:
        pp = freqprob.perplexity(model, test_words)
        unigram_perplexities[name] = pp
        print(f"{name:<20}: {pp:.2f}")
    except Exception as e:
        print(f"{name:<20}: Error - {str(e)[:30]}...")

# Evaluate bigram models (on bigram test data)
test_bigrams = []
for sentence in test_corpus:
    words = ["<s>", *sentence.split(), "</s>"]
    for i in range(len(words) - 1):
        test_bigrams.append((words[i], words[i + 1]))

print("\nBigram Model Evaluation (Perplexity):")
print("=" * 40)

bigram_models = {"Bigram MLE": freqprob.MLE(bigram_freqdist, logprob=True)}

if kn is not None:
    bigram_models["Kneser-Ney"] = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=True)

if mkn is not None:
    bigram_models["Modified Kneser-Ney"] = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=True)

bigram_perplexities = {}
for name, model in bigram_models.items():
    try:
        pp = freqprob.perplexity(model, test_bigrams)
        bigram_perplexities[name] = pp
        print(f"{name:<20}: {pp:.2f}")
    except Exception as e:
        print(f"{name:<20}: Error - {str(e)[:30]}...")

# Visualize results
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Unigram models
if unigram_perplexities:
    methods = list(unigram_perplexities.keys())
    values = list(unigram_perplexities.values())

    bars1 = ax1.bar(methods, values, alpha=0.8, color=plt.cm.Set3(np.linspace(0, 1, len(methods))))
    ax1.set_title("Unigram Model Perplexity Comparison")
    ax1.set_ylabel("Perplexity (lower is better)")
    ax1.tick_params(axis="x", rotation=45)

    # Add value labels
    for bar, value in zip(bars1, values, strict=False):
        ax1.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.5,
            f"{value:.1f}",
            ha="center",
            va="bottom",
        )

# Bigram models
if bigram_perplexities:
    methods = list(bigram_perplexities.keys())
    values = list(bigram_perplexities.values())

    bars2 = ax2.bar(methods, values, alpha=0.8, color=plt.cm.Set2(np.linspace(0, 1, len(methods))))
    ax2.set_title("Bigram Model Perplexity Comparison")
    ax2.set_ylabel("Perplexity (lower is better)")
    ax2.tick_params(axis="x", rotation=45)

    # Add value labels
    for bar, value in zip(bars2, values, strict=False):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.5,
            f"{value:.1f}",
            ha="center",
            va="bottom",
        )

plt.tight_layout()

# Find best models
if unigram_perplexities:
    best_unigram = min(unigram_perplexities.items(), key=lambda x: x[1])
    print(f"\nBest unigram model: {best_unigram[0]} (PP = {best_unigram[1]:.2f})")

if bigram_perplexities:
    best_bigram = min(bigram_perplexities.items(), key=lambda x: x[1])
    print(f"Best bigram model: {best_bigram[0]} (PP = {best_bigram[1]:.2f})")

print("ADVANCED SMOOTHING METHODS: KEY INSIGHTS")
print("=" * 50)
print()

print("🔵 SIMPLE GOOD-TURING:")
print("   • Uses frequency-of-frequencies statistics")
print("   • Returns per-word probability (divides p₀ by estimated unseen types)")
print("   • bins parameter controls vocabulary size estimate (default: V + N₁)")
print("   • total_unseen_mass property provides access to p₀")
print("   • Works well when frequency patterns are reliable")
print("   • Can fail with sparse data or irregular patterns")
print()

print("🟢 KNESER-NEY:")
print("   • Gold standard for n-gram language models")
print("   • Uses absolute discounting (subtract fixed amount)")
print("   • Continuation probability: how likely is word in new contexts?")
print("   • Particularly effective for bigrams and trigrams")
print()

print("🟡 MODIFIED KNESER-NEY:")
print("   • Enhanced version of Kneser-Ney")
print("   • Different discount values for different frequency counts")
print("   • Automatically estimates discounts from data")
print("   • Generally performs better than standard Kneser-Ney")
print()

print("🔴 INTERPOLATED SMOOTHING:")
print("   • Combines multiple models (e.g., trigram + bigram)")
print("   • Automatic n-gram mode: extracts lower-order context from higher-order n-grams")
print("   • Linear interpolation: lambda*P_high(ngram) + (1-lambda)*P_low(context)")
print("   • Balances specificity with coverage")
print("   • Essential for practical n-gram systems")
print()

print("🟣 BAYESIAN SMOOTHING:")
print("   • Theoretically principled using Dirichlet prior")
print("   • alpha parameter controls prior strength")
print("   • alpha=1.0 equivalent to Laplace smoothing")
print("   • Good theoretical foundation")
print()

# Practical recommendations
print("🎯 PRACTICAL RECOMMENDATIONS:")
print("=" * 30)
print()

print("For Language Modeling:")
print("  1. Start with Modified Kneser-Ney for n-grams")
print("  2. Use interpolation for robustness")
print("  3. Consider neural models for large datasets")
print()

print("For General Frequency Estimation:")
print("  1. Try Simple Good-Turing first")
print("  2. Fall back to Bayesian smoothing if SGT fails")
print("  3. Tune alpha parameter using validation data")
print()

print("For Production Systems:")
print("  1. Use interpolated smoothing for robustness")
print("  2. Consider computational costs")
print("  3. Validate on domain-specific data")
print()

# Show computational complexity
print("⚡ COMPUTATIONAL COMPLEXITY:")
print("=" * 28)
methods_complexity = {
    "Laplace/Bayesian": "O(1) per query",
    "Simple Good-Turing": "O(V) preprocessing, O(1) query",
    "Kneser-Ney": "O(N) preprocessing, O(1) query",
    "Modified Kneser-Ney": "O(N) preprocessing, O(1) query",
    "Interpolated": "O(k) per query (k models)",
}

for method, complexity in methods_complexity.items():
    print(f"  {method:<18}: {complexity}")

print("\n  V = vocabulary size, N = total n-grams, k = number of models")
```

Output:
```
Test set: 26 words
Words: ['the', 'function', 'returns', 'a', 'string', 'object', 'from', 'the', 'module', 'pattern', 'matching', 'uses', 'regular', 'expressions', 'for', 'text', 'processing', 'the', 'module', 'provides', 'methods', 'for', 'searching', 'and', 'replacing', 'strings']

Unigram Model Evaluation (Perplexity):
========================================
MLE                 : 2262.13
Laplace             : 443.30
Bayesian (alpha=0.5): 483.76
Simple Good-Turing  : 540.13

Bigram Model Evaluation (Perplexity):
========================================
Bigram MLE          : 19279110.93
Kneser-Ney          : 211.94
Modified Kneser-Ney : 247.39

Best unigram model: Laplace (PP = 443.30)
Best bigram model: Kneser-Ney (PP = 211.94)
ADVANCED SMOOTHING METHODS: KEY INSIGHTS
==================================================

🔵 SIMPLE GOOD-TURING:
   • Uses frequency-of-frequencies statistics
   • Returns per-word probability (divides p₀ by estimated unseen types)
   • bins parameter controls vocabulary size estimate (default: V + N₁)
   • total_unseen_mass property provides access to p₀
   • Works well when frequency patterns are reliable
   • Can fail with sparse data or irregular patterns

🟢 KNESER-NEY:
   • Gold standard for n-gram language models
   • Uses absolute discounting (subtract fixed amount)
   • Continuation probability: how likely is word in new contexts?
   • Particularly effective for bigrams and trigrams

🟡 MODIFIED KNESER-NEY:
   • Enhanced version of Kneser-Ney
   • Different discount values for different frequency counts
   • Automatically estimates discounts from data
   • Generally performs better than standard Kneser-Ney

🔴 INTERPOLATED SMOOTHING:
   • Combines multiple models (e.g., trigram + bigram)
   • Automatic n-gram mode: extracts lower-order context from higher-order n-grams
   • Linear interpolation: lambda*P_high(ngram) + (1-lambda)*P_low(context)
   • Balances specificity with coverage
   • Essential for practical n-gram systems

🟣 BAYESIAN SMOOTHING:
   • Theoretically principled using Dirichlet prior
   • alpha parameter controls prior strength
   • alpha=1.0 equivalent to Laplace smoothing
   • Good theoretical foundation

🎯 PRACTICAL RECOMMENDATIONS:
==============================

For Language Modeling:
  1. Start with Modified Kneser-Ney for n-grams
  2. Use interpolation for robustness
  3. Consider neural models for large datasets

For General Frequency Estimation:
  1. Try Simple Good-Turing first
  2. Fall back to Bayesian smoothing if SGT fails
  3. Tune alpha parameter using validation data

For Production Systems:
  1. Use interpolated smoothing for robustness
  2. Consider computational costs
  3. Validate on domain-specific data

⚡ COMPUTATIONAL COMPLEXITY:
============================
  Laplace/Bayesian  : O(1) per query
  Simple Good-Turing: O(V) preprocessing, O(1) query
  Kneser-Ney        : O(N) preprocessing, O(1) query
  Modified Kneser-Ney: O(N) preprocessing, O(1) query
  Interpolated      : O(k) per query (k models)

  V = vocabulary size, N = total n-grams, k = number of models
```

![Figure](docs/figures/figure_4.png)

## Exercise: Advanced Method Selection

Practice choosing the right advanced method for different scenarios.
```python

```

## Summary

In this tutorial, you learned about advanced smoothing methods that are essential for modern NLP:

### Methods Covered:
1. **Simple Good-Turing** - Uses frequency-of-frequencies for principled probability estimation
2. **Kneser-Ney** - The gold standard for n-gram language models with continuation probabilities
3. **Modified Kneser-Ney** - Enhanced version with count-dependent discounting
4. **Interpolated Smoothing** - Combines multiple models for robustness
5. **Bayesian Smoothing** - Theoretically principled approach with Dirichlet priors

### Key Insights:
- **Kneser-Ney** dominates for n-gram language modeling
- **Good-Turing** provides theoretical foundation for unseen event estimation
- **Interpolation** is crucial for practical systems
- **Bayesian methods** offer principled parameter control
- **Context matters** - different methods excel in different scenarios

### Next Steps:
- **Tutorial 3**: Computational Efficiency and Memory Management
- **Tutorial 4**: Real-world Applications and Case Studies
- Practice implementing these methods on your own datasets
- Experiment with hyperparameter tuning

**Remember**: The best method depends on your specific use case, data characteristics, and computational constraints!
```python

```
