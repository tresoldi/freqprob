{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqProb Tutorial 4: Real-World NLP Applications\n",
    "\n",
    "This tutorial demonstrates practical applications of FreqProb in real-world NLP scenarios:\n",
    "\n",
    "1. **Language Modeling** - Building n-gram language models\n",
    "2. **Text Classification** - Feature extraction with smoothed probabilities\n",
    "3. **Information Retrieval** - Document scoring and ranking\n",
    "4. **Sentiment Analysis** - Building probability-based classifiers\n",
    "5. **Machine Translation** - Language model scoring\n",
    "6. **Speech Recognition** - N-gram model integration\n",
    "\n",
    "## Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import freqprob\n",
    "\n",
    "# Set up plotting and random seeds\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"FreqProb Real-World Applications Tutorial\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "\n",
    "# Sample datasets for demonstration\n",
    "# In practice, you would load these from files or APIs\n",
    "\n",
    "# 1. News articles dataset\n",
    "news_articles = {\n",
    "    \"technology\": [\n",
    "        \"Apple announces new iPhone with advanced AI capabilities and improved camera system\",\n",
    "        \"Google launches innovative machine learning platform for developers worldwide\",\n",
    "        \"Microsoft introduces cloud computing solutions for enterprise customers\",\n",
    "        \"Tesla unveils autonomous driving technology with neural network improvements\",\n",
    "        \"Amazon develops new artificial intelligence algorithms for recommendation systems\",\n",
    "        \"Facebook invests heavily in virtual reality and metaverse technologies\",\n",
    "        \"Intel releases next generation processors with enhanced performance capabilities\",\n",
    "        \"Samsung showcases foldable smartphone technology at tech conference\",\n",
    "    ],\n",
    "    \"sports\": [\n",
    "        \"Olympic athletes compete in swimming championships with record breaking performances\",\n",
    "        \"Basketball team wins championship after intense playoff games throughout season\",\n",
    "        \"Soccer world cup features exciting matches between international teams\",\n",
    "        \"Tennis tournament showcases incredible athletic talent and competitive spirit\",\n",
    "        \"Baseball season concludes with thrilling world series games\",\n",
    "        \"Hockey players demonstrate exceptional skills during championship playoffs\",\n",
    "        \"Marathon runners compete in challenging race through city streets\",\n",
    "        \"Golf tournament attracts professional players from around the world\",\n",
    "    ],\n",
    "    \"politics\": [\n",
    "        \"Government announces new policies regarding healthcare and education reform\",\n",
    "        \"Presidential election campaign focuses on economic issues and foreign policy\",\n",
    "        \"Congress debates legislation concerning environmental protection and climate change\",\n",
    "        \"International diplomacy efforts aim to resolve conflicts through peaceful negotiations\",\n",
    "        \"Local elections determine representatives for state and municipal governments\",\n",
    "        \"Political parties present platforms addressing social justice and equality\",\n",
    "        \"Supreme court decisions impact constitutional law and civil rights\",\n",
    "        \"Trade agreements between nations affect global economic relationships\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 2. Movie reviews dataset (sentiment analysis)\n",
    "movie_reviews = {\n",
    "    \"positive\": [\n",
    "        \"This movie is absolutely fantastic with amazing performances and brilliant storytelling\",\n",
    "        \"Incredible cinematography and outstanding acting make this film truly exceptional\",\n",
    "        \"Wonderful story with excellent character development and beautiful visuals\",\n",
    "        \"Superb direction and phenomenal performances create an unforgettable experience\",\n",
    "        \"Magnificent film with incredible depth and emotional resonance throughout\",\n",
    "        \"Brilliant screenplay and outstanding cast deliver an amazing cinematic experience\",\n",
    "        \"Excellent movie with fantastic performances and compelling narrative structure\",\n",
    "        \"Remarkable film featuring wonderful acting and beautiful cinematographic work\",\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"This movie is terrible with awful performances and boring storyline throughout\",\n",
    "        \"Horrible cinematography and poor acting make this film completely unwatchable\",\n",
    "        \"Terrible story with weak character development and disappointing visual effects\",\n",
    "        \"Poor direction and bad performances create a frustrating viewing experience\",\n",
    "        \"Awful film with no depth and completely uninteresting plot development\",\n",
    "        \"Bad screenplay and terrible cast deliver a disappointing cinematic disaster\",\n",
    "        \"Horrible movie with weak performances and confusing narrative structure\",\n",
    "        \"Disappointing film featuring poor acting and uninspiring cinematographic choices\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(f\"Loaded {sum(len(articles) for articles in news_articles.values())} news articles\")\n",
    "print(f\"Loaded {sum(len(reviews) for reviews in movie_reviews.values())} movie reviews\")\n",
    "print()\n",
    "\n",
    "# Text preprocessing utilities\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Basic text preprocessing: lowercase, tokenize, remove punctuation.\"\"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    # Split into words\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def generate_ngrams(tokens: list[str], n: int) -> list[tuple[str, ...]]:\n",
    "    \"\"\"Generate n-grams from a list of tokens.\"\"\"\n",
    "    if n == 1:\n",
    "        return [(token,) for token in tokens]\n",
    "\n",
    "    padded_tokens = [\"<s>\"] * (n - 1) + tokens + [\"</s>\"] * (n - 1)\n",
    "    ngrams = []\n",
    "    for i in range(len(padded_tokens) - n + 1):\n",
    "        ngrams.append(tuple(padded_tokens[i : i + n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "print(\"Text preprocessing utilities ready\")\n",
    "print(\"Sample preprocessing:\", preprocess_text(\"Hello, World! This is a test.\"))\n",
    "print(\"Sample bigrams:\", generate_ngrams([\"hello\", \"world\", \"test\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 1: Language Modeling\n",
    "\n",
    "Build n-gram language models for text generation and probability estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APPLICATION 1: LANGUAGE MODELING\")\n",
    "print(\"=\" * 35)\n",
    "print()\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"N-gram language model with multiple smoothing options.\"\"\"\n",
    "\n",
    "    def __init__(self, n: int = 2, smoothing: str = \"kneser_ney\"):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.models = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, documents: list[str]):\n",
    "        \"\"\"Train the language model on a corpus of documents.\"\"\"\n",
    "        print(f\"Training {self.n}-gram model with {self.smoothing} smoothing...\")\n",
    "\n",
    "        # Collect all n-grams for each order\n",
    "        ngram_counts = {i: Counter() for i in range(1, self.n + 1)}\n",
    "\n",
    "        for doc in documents:\n",
    "            tokens = preprocess_text(doc)\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "            for order in range(1, self.n + 1):\n",
    "                ngrams = generate_ngrams(tokens, order)\n",
    "                ngram_counts[order].update(ngrams)\n",
    "\n",
    "        # Create smoothed models for each n-gram order\n",
    "        for order in range(1, self.n + 1):\n",
    "            if self.smoothing == \"kneser_ney\" and order >= 2:\n",
    "                try:\n",
    "                    self.models[order] = freqprob.KneserNey(\n",
    "                        ngram_counts[order], discount=0.75, logprob=True\n",
    "                    )\n",
    "                except Exception:\n",
    "                    # Fallback to Laplace if KN fails\n",
    "                    self.models[order] = freqprob.Laplace(\n",
    "                        ngram_counts[order], bins=len(ngram_counts[order]) * 2, logprob=True\n",
    "                    )\n",
    "            else:\n",
    "                self.models[order] = freqprob.Laplace(\n",
    "                    ngram_counts[order], bins=len(ngram_counts[order]) * 2, logprob=True\n",
    "                )\n",
    "\n",
    "        print(f\"Trained on {len(documents)} documents\")\n",
    "        print(f\"Vocabulary size: {len(self.vocab):,}\")\n",
    "        for order in range(1, self.n + 1):\n",
    "            print(f\"{order}-gram types: {len(ngram_counts[order]):,}\")\n",
    "\n",
    "    def score_sequence(self, text: str) -> float:\n",
    "        \"\"\"Score a text sequence using the language model.\"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "        ngrams = generate_ngrams(tokens, self.n)\n",
    "\n",
    "        log_prob = 0.0\n",
    "        for ngram in ngrams:\n",
    "            log_prob += self.models[self.n](ngram)\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def perplexity(self, test_texts: list[str]) -> float:\n",
    "        \"\"\"Calculate perplexity on test data.\"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_words = 0\n",
    "\n",
    "        for text in test_texts:\n",
    "            tokens = preprocess_text(text)\n",
    "            ngrams = generate_ngrams(tokens, self.n)\n",
    "\n",
    "            for ngram in ngrams:\n",
    "                total_log_prob += self.models[self.n](ngram)\n",
    "                total_words += 1\n",
    "\n",
    "        if total_words == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        return math.exp(-total_log_prob / total_words)\n",
    "\n",
    "    def generate_text(self, prompt: str = \"\", max_length: int = 20) -> str:\n",
    "        \"\"\"Generate text using the language model (simplified version).\"\"\"\n",
    "        if not prompt:\n",
    "            # Start with sentence boundary\n",
    "            generated = [\"<s>\"] * (self.n - 1)\n",
    "        else:\n",
    "            generated = [\"<s>\"] * (self.n - 1) + preprocess_text(prompt)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Get context (last n-1 words)\n",
    "            context = tuple(generated[-(self.n - 1) :])\n",
    "\n",
    "            # Find best next word (simplified - in practice, use sampling)\n",
    "            best_word = None\n",
    "            best_score = float(\"-inf\")\n",
    "\n",
    "            # Try common words from vocabulary\n",
    "            candidate_words = list(self.vocab)[:50]  # Limit for efficiency\n",
    "            for word in candidate_words:\n",
    "                ngram = (*context, word)\n",
    "                score = self.models[self.n](ngram)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_word = word\n",
    "\n",
    "            if best_word == \"</s>\" or best_word is None:\n",
    "                break\n",
    "\n",
    "            generated.append(best_word)\n",
    "\n",
    "        # Remove sentence boundaries and return\n",
    "        result = [w for w in generated if w not in [\"<s>\", \"</s>\"]]\n",
    "        return \" \".join(result)\n",
    "\n",
    "\n",
    "# Train language models on different domains\n",
    "print(\"Training domain-specific language models...\")\n",
    "print()\n",
    "\n",
    "domain_models = {}\n",
    "for domain, articles in news_articles.items():\n",
    "    print(f\"Training {domain} model:\")\n",
    "    model = LanguageModel(n=3, smoothing=\"kneser_ney\")\n",
    "    model.train(articles)\n",
    "    domain_models[domain] = model\n",
    "    print()\n",
    "\n",
    "# Test sentences from different domains\n",
    "test_sentences = {\n",
    "    \"technology\": \"Apple releases new smartphone with artificial intelligence\",\n",
    "    \"sports\": \"Basketball team wins championship game with incredible performance\",\n",
    "    \"politics\": \"Government announces new policy regarding healthcare reform\",\n",
    "    \"mixed\": \"The president discussed artificial intelligence in sports technology\",\n",
    "}\n",
    "\n",
    "print(\"Language Model Evaluation:\")\n",
    "print(\"=\" * 27)\n",
    "print()\n",
    "\n",
    "# Score test sentences with each domain model\n",
    "results = defaultdict(dict)\n",
    "for test_name, test_sentence in test_sentences.items():\n",
    "    print(f\"Test: '{test_sentence}'\")\n",
    "\n",
    "    scores = {}\n",
    "    for domain, model in domain_models.items():\n",
    "        score = model.score_sequence(test_sentence)\n",
    "        scores[domain] = score\n",
    "        results[test_name][domain] = score\n",
    "\n",
    "    # Find best matching domain\n",
    "    best_domain = max(scores, key=scores.get)\n",
    "    print(f\"  Best match: {best_domain} (score: {scores[best_domain]:.2f})\")\n",
    "\n",
    "    # Show all scores\n",
    "    for domain, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"    {domain:<12}: {score:6.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize domain classification results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Heatmap of scores\n",
    "score_matrix = []\n",
    "test_names = list(test_sentences.keys())\n",
    "domain_names = list(domain_models.keys())\n",
    "\n",
    "for test_name in test_names:\n",
    "    row = [results[test_name][domain] for domain in domain_names]\n",
    "    score_matrix.append(row)\n",
    "\n",
    "im = ax1.imshow(score_matrix, cmap=\"RdYlBu_r\", aspect=\"auto\")\n",
    "ax1.set_xticks(range(len(domain_names)))\n",
    "ax1.set_xticklabels(domain_names)\n",
    "ax1.set_yticks(range(len(test_names)))\n",
    "ax1.set_yticklabels(test_names)\n",
    "ax1.set_title(\"Language Model Scores\\n(Higher = Better Match)\")\n",
    "\n",
    "# Add score annotations\n",
    "for i in range(len(test_names)):\n",
    "    for j in range(len(domain_names)):\n",
    "        ax1.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{score_matrix[i][j]:.1f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# 2. Perplexity comparison\n",
    "print(\"Computing perplexities...\")\n",
    "perplexities = {}\n",
    "for domain, model in domain_models.items():\n",
    "    # Test on held-out sentences\n",
    "    test_data = [\n",
    "        list(test_sentences.values())[i]\n",
    "        for i in range(len(test_sentences))\n",
    "        if i % 3 == hash(domain) % 3\n",
    "    ]\n",
    "    if test_data:\n",
    "        pp = model.perplexity(test_data)\n",
    "        perplexities[domain] = pp\n",
    "\n",
    "if perplexities:\n",
    "    domains = list(perplexities.keys())\n",
    "    pp_values = list(perplexities.values())\n",
    "\n",
    "    bars = ax2.bar(domains, pp_values, alpha=0.7, color=[\"blue\", \"orange\", \"green\"])\n",
    "    ax2.set_title(\"Model Perplexity on Test Data\\n(Lower = Better)\")\n",
    "    ax2.set_ylabel(\"Perplexity\")\n",
    "    ax2.set_xlabel(\"Domain\")\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, pp_values, strict=False):\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.5,\n",
    "            f\"{value:.1f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text generation example\n",
    "print(\"Text Generation Examples:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "generation_prompts = {\n",
    "    \"technology\": \"Apple announces\",\n",
    "    \"sports\": \"The team\",\n",
    "    \"politics\": \"Government policy\",\n",
    "}\n",
    "\n",
    "for domain, prompt in generation_prompts.items():\n",
    "    if domain in domain_models:\n",
    "        generated = domain_models[domain].generate_text(prompt, max_length=10)\n",
    "        print(f\"{domain.capitalize()}: {generated}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Domain-specific models capture vocabulary and style differences\")\n",
    "print(\"- Higher scores indicate better domain match\")\n",
    "print(\"- Language models can be used for domain classification\")\n",
    "print(\"- Text generation reflects domain-specific patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 2: Text Classification\n",
    "\n",
    "Use smoothed probability features for document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APPLICATION 2: TEXT CLASSIFICATION\")\n",
    "print(\"=\" * 36)\n",
    "print()\n",
    "\n",
    "\n",
    "class ProbabilisticTextClassifier:\n",
    "    \"\"\"Text classifier using smoothed probability features.\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing_method=\"laplace\", feature_type=\"unigram\"):\n",
    "        self.smoothing_method = smoothing_method\n",
    "        self.feature_type = feature_type\n",
    "        self.class_models = {}\n",
    "        self.vocabulary = set()\n",
    "        self.classes = []\n",
    "\n",
    "    def train(self, documents: dict[str, list[str]]):\n",
    "        \"\"\"Train the classifier on labeled documents.\"\"\"\n",
    "        print(f\"Training classifier with {self.smoothing_method} smoothing...\")\n",
    "\n",
    "        self.classes = list(documents.keys())\n",
    "\n",
    "        # Build vocabulary and class-specific frequency distributions\n",
    "        class_frequencies = {}\n",
    "\n",
    "        for class_label, docs in documents.items():\n",
    "            # Combine all documents for this class\n",
    "            all_tokens = []\n",
    "            for doc in docs:\n",
    "                tokens = preprocess_text(doc)\n",
    "                all_tokens.extend(tokens)\n",
    "                self.vocabulary.update(tokens)\n",
    "\n",
    "            # Create frequency distribution\n",
    "            if self.feature_type == \"unigram\":\n",
    "                class_frequencies[class_label] = Counter(all_tokens)\n",
    "            elif self.feature_type == \"bigram\":\n",
    "                bigrams = []\n",
    "                for doc in docs:\n",
    "                    tokens = preprocess_text(doc)\n",
    "                    bigrams.extend(generate_ngrams(tokens, 2))\n",
    "                class_frequencies[class_label] = Counter(bigrams)\n",
    "\n",
    "        # Create smoothed models for each class\n",
    "        vocab_size = (\n",
    "            len(self.vocabulary) if self.feature_type == \"unigram\" else len(self.vocabulary) ** 2\n",
    "        )\n",
    "\n",
    "        for class_label, frequencies in class_frequencies.items():\n",
    "            if self.smoothing_method == \"laplace\":\n",
    "                self.class_models[class_label] = freqprob.Laplace(\n",
    "                    frequencies, bins=vocab_size, logprob=True\n",
    "                )\n",
    "            elif self.smoothing_method == \"bayesian\":\n",
    "                self.class_models[class_label] = freqprob.BayesianSmoothing(\n",
    "                    frequencies, alpha=0.5, logprob=True\n",
    "                )\n",
    "            else:  # MLE\n",
    "                self.class_models[class_label] = freqprob.MLE(frequencies, logprob=True)\n",
    "\n",
    "        print(f\"Trained on {len(self.classes)} classes\")\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary):,}\")\n",
    "        for class_label in self.classes:\n",
    "            class_size = sum(class_frequencies[class_label].values())\n",
    "            print(f\"  {class_label}: {class_size:,} tokens\")\n",
    "\n",
    "    def extract_features(self, text: str) -> list:\n",
    "        \"\"\"Extract features from text based on feature type.\"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "\n",
    "        if self.feature_type == \"unigram\":\n",
    "            return tokens\n",
    "        if self.feature_type == \"bigram\":\n",
    "            return generate_ngrams(tokens, 2)\n",
    "        return tokens\n",
    "\n",
    "    def classify(self, text: str) -> tuple[str, dict[str, float]]:\n",
    "        \"\"\"Classify a text document.\"\"\"\n",
    "        features = self.extract_features(text)\n",
    "\n",
    "        class_scores = {}\n",
    "        for class_label, model in self.class_models.items():\n",
    "            # Calculate log probability of features given class\n",
    "            log_prob = sum(model(feature) for feature in features)\n",
    "            class_scores[class_label] = log_prob\n",
    "\n",
    "        # Predict class with highest score\n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "\n",
    "        return predicted_class, class_scores\n",
    "\n",
    "    def evaluate(self, test_documents: dict[str, list[str]]) -> dict[str, float]:\n",
    "        \"\"\"Evaluate classifier on test data.\"\"\"\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        class_stats = {class_label: {\"correct\": 0, \"total\": 0} for class_label in self.classes}\n",
    "\n",
    "        for true_class, docs in test_documents.items():\n",
    "            for doc in docs:\n",
    "                predicted_class, scores = self.classify(doc)\n",
    "\n",
    "                total += 1\n",
    "                class_stats[true_class][\"total\"] += 1\n",
    "\n",
    "                if predicted_class == true_class:\n",
    "                    correct += 1\n",
    "                    class_stats[true_class][\"correct\"] += 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "        class_accuracies = {}\n",
    "        for class_label, stats in class_stats.items():\n",
    "            if stats[\"total\"] > 0:\n",
    "                class_accuracies[class_label] = stats[\"correct\"] / stats[\"total\"]\n",
    "            else:\n",
    "                class_accuracies[class_label] = 0\n",
    "\n",
    "        return {\n",
    "            \"overall_accuracy\": accuracy,\n",
    "            \"class_accuracies\": class_accuracies,\n",
    "            \"total_documents\": total,\n",
    "        }\n",
    "\n",
    "\n",
    "# Train classifiers with different configurations\n",
    "print(\"Training multiple classifier configurations...\")\n",
    "print()\n",
    "\n",
    "classifiers = {\n",
    "    \"Unigram + Laplace\": ProbabilisticTextClassifier(\"laplace\", \"unigram\"),\n",
    "    \"Unigram + Bayesian\": ProbabilisticTextClassifier(\"bayesian\", \"unigram\"),\n",
    "    \"Bigram + Laplace\": ProbabilisticTextClassifier(\"laplace\", \"bigram\"),\n",
    "}\n",
    "\n",
    "# Train on news articles\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"Training {name}:\")\n",
    "    classifier.train(news_articles)\n",
    "    print()\n",
    "\n",
    "# Create test data (using different sentences)\n",
    "test_articles = {\n",
    "    \"technology\": [\n",
    "        \"New smartphone features advanced machine learning algorithms\",\n",
    "        \"Cloud computing platform launches with AI integration\",\n",
    "        \"Software developers adopt innovative programming frameworks\",\n",
    "    ],\n",
    "    \"sports\": [\n",
    "        \"Professional athletes train for upcoming Olympic games\",\n",
    "        \"Championship match features incredible team performance\",\n",
    "        \"Stadium hosts exciting tournament with international competitors\",\n",
    "    ],\n",
    "    \"politics\": [\n",
    "        \"Legislative session addresses important social issues\",\n",
    "        \"International relations improve through diplomatic efforts\",\n",
    "        \"Electoral process ensures democratic representation\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Classifier Evaluation:\")\n",
    "print(\"=\" * 22)\n",
    "print()\n",
    "\n",
    "evaluation_results = {}\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"{name}:\")\n",
    "    results = classifier.evaluate(test_articles)\n",
    "    evaluation_results[name] = results\n",
    "\n",
    "    print(f\"  Overall Accuracy: {results['overall_accuracy']:.3f}\")\n",
    "    for class_label, acc in results[\"class_accuracies\"].items():\n",
    "        print(f\"    {class_label}: {acc:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Detailed classification examples\n",
    "print(\"Detailed Classification Examples:\")\n",
    "print(\"=\" * 34)\n",
    "print()\n",
    "\n",
    "example_texts = [\n",
    "    \"Apple develops new artificial intelligence chip for smartphones\",\n",
    "    \"Basketball players compete in championship finals\",\n",
    "    \"Government implements new environmental protection policies\",\n",
    "    \"Machine learning algorithms improve medical diagnosis accuracy\",  # Mixed domain\n",
    "]\n",
    "\n",
    "best_classifier = max(evaluation_results, key=lambda x: evaluation_results[x][\"overall_accuracy\"])\n",
    "print(f\"Using best classifier: {best_classifier}\")\n",
    "print()\n",
    "\n",
    "for i, text in enumerate(example_texts):\n",
    "    predicted_class, scores = classifiers[best_classifier].classify(text)\n",
    "\n",
    "    print(f\"Example {i + 1}: '{text}'\")\n",
    "    print(f\"  Predicted: {predicted_class}\")\n",
    "    print(\"  Scores:\")\n",
    "    for class_label, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"    {class_label:<12}: {score:6.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Overall accuracy comparison\n",
    "classifier_names = list(evaluation_results.keys())\n",
    "accuracies = [evaluation_results[name][\"overall_accuracy\"] for name in classifier_names]\n",
    "\n",
    "bars = axes[0, 0].bar(range(len(classifier_names)), accuracies, alpha=0.7)\n",
    "axes[0, 0].set_title(\"Overall Classification Accuracy\")\n",
    "axes[0, 0].set_ylabel(\"Accuracy\")\n",
    "axes[0, 0].set_xticks(range(len(classifier_names)))\n",
    "axes[0, 0].set_xticklabels(classifier_names, rotation=45)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies, strict=False):\n",
    "    axes[0, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{acc:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# 2. Class-wise accuracy heatmap\n",
    "class_names = list(news_articles.keys())\n",
    "accuracy_matrix = []\n",
    "\n",
    "for classifier_name in classifier_names:\n",
    "    row = [\n",
    "        evaluation_results[classifier_name][\"class_accuracies\"][class_name]\n",
    "        for class_name in class_names\n",
    "    ]\n",
    "    accuracy_matrix.append(row)\n",
    "\n",
    "im = axes[0, 1].imshow(accuracy_matrix, cmap=\"RdYlGn\", vmin=0, vmax=1, aspect=\"auto\")\n",
    "axes[0, 1].set_xticks(range(len(class_names)))\n",
    "axes[0, 1].set_xticklabels(class_names)\n",
    "axes[0, 1].set_yticks(range(len(classifier_names)))\n",
    "axes[0, 1].set_yticklabels(classifier_names)\n",
    "axes[0, 1].set_title(\"Class-wise Accuracy\")\n",
    "\n",
    "# Add accuracy annotations\n",
    "for i in range(len(classifier_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        axes[0, 1].text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{accuracy_matrix[i][j]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "# 3. Feature importance (vocabulary analysis)\n",
    "# Analyze most discriminative words for each class\n",
    "best_model = classifiers[best_classifier]\n",
    "class_words = {}\n",
    "\n",
    "for class_label, model in best_model.class_models.items():\n",
    "    # Get words with highest probabilities in this class\n",
    "    word_scores = []\n",
    "    sample_words = list(best_model.vocabulary)[:100]  # Sample for efficiency\n",
    "\n",
    "    for word in sample_words:\n",
    "        score = model(word)\n",
    "        word_scores.append((word, score))\n",
    "\n",
    "    # Get top words\n",
    "    top_words = sorted(word_scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "    class_words[class_label] = top_words\n",
    "\n",
    "# Plot top words per class\n",
    "y_pos = 0\n",
    "colors = [\"blue\", \"orange\", \"green\"]\n",
    "for i, (class_label, words) in enumerate(class_words.items()):\n",
    "    word_names = [word for word, score in words]\n",
    "    word_scores = [score for word, score in words]\n",
    "\n",
    "    y_positions = [y_pos + j * 0.2 for j in range(len(word_names))]\n",
    "    axes[1, 0].barh(\n",
    "        y_positions, word_scores, height=0.15, color=colors[i], alpha=0.7, label=class_label\n",
    "    )\n",
    "\n",
    "    # Add word labels\n",
    "    for j, (word, score) in enumerate(words):\n",
    "        axes[1, 0].text(score + 0.1, y_pos + j * 0.2, word, va=\"center\", fontsize=8)\n",
    "\n",
    "    y_pos += len(words) * 0.2 + 0.5\n",
    "\n",
    "axes[1, 0].set_title(\"Top Words by Class (Log Probability)\")\n",
    "axes[1, 0].set_xlabel(\"Log Probability\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_yticks([])\n",
    "\n",
    "# 4. Classification confidence distribution\n",
    "confidence_scores = []\n",
    "for text in example_texts:\n",
    "    _, scores = classifiers[best_classifier].classify(text)\n",
    "    max_score = max(scores.values())\n",
    "    min_score = min(scores.values())\n",
    "    confidence = max_score - min_score  # Score difference\n",
    "    confidence_scores.append(confidence)\n",
    "\n",
    "axes[1, 1].hist(confidence_scores, bins=5, alpha=0.7, color=\"purple\")\n",
    "axes[1, 1].set_title(\"Classification Confidence Distribution\")\n",
    "axes[1, 1].set_xlabel(\"Confidence (Score Difference)\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Smoothing methods handle unseen words gracefully\")\n",
    "print(\"- Bigram features can capture more context but may overfit\")\n",
    "print(\"- Different domains show varying classification difficulty\")\n",
    "print(\"- Probability-based features provide interpretable results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 3: Information Retrieval\n",
    "\n",
    "Build document ranking systems using language model scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APPLICATION 3: INFORMATION RETRIEVAL\")\n",
    "print(\"=\" * 37)\n",
    "print()\n",
    "\n",
    "\n",
    "class LanguageModelRetrieval:\n",
    "    \"\"\"Information retrieval using language model scoring.\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing=\"laplace\", interpolation_lambda=0.8):\n",
    "        self.smoothing = smoothing\n",
    "        self.interpolation_lambda = interpolation_lambda\n",
    "        self.document_models = {}\n",
    "        self.collection_model = None\n",
    "        self.documents = {}\n",
    "\n",
    "    def index_documents(self, documents: dict[str, str]):\n",
    "        \"\"\"Index a collection of documents.\"\"\"\n",
    "        print(f\"Indexing {len(documents)} documents...\")\n",
    "\n",
    "        self.documents = documents\n",
    "\n",
    "        # Build collection-level language model\n",
    "        all_tokens = []\n",
    "        for doc_text in documents.values():\n",
    "            tokens = preprocess_text(doc_text)\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "        collection_freq = Counter(all_tokens)\n",
    "        vocab_size = len(set(all_tokens))\n",
    "\n",
    "        if self.smoothing == \"laplace\":\n",
    "            self.collection_model = freqprob.Laplace(\n",
    "                collection_freq, bins=vocab_size * 2, logprob=True\n",
    "            )\n",
    "        else:\n",
    "            self.collection_model = freqprob.MLE(collection_freq, logprob=True)\n",
    "\n",
    "        # Build document-specific language models\n",
    "        for doc_id, doc_text in documents.items():\n",
    "            tokens = preprocess_text(doc_text)\n",
    "            doc_freq = Counter(tokens)\n",
    "\n",
    "            if self.smoothing == \"laplace\":\n",
    "                doc_model = freqprob.Laplace(doc_freq, bins=vocab_size * 2, logprob=True)\n",
    "            else:\n",
    "                doc_model = freqprob.MLE(doc_freq, logprob=True)\n",
    "\n",
    "            self.document_models[doc_id] = doc_model\n",
    "\n",
    "        print(f\"Collection vocabulary: {vocab_size:,} words\")\n",
    "        print(f\"Average document length: {len(all_tokens) / len(documents):.1f} words\")\n",
    "\n",
    "    def score_document(self, doc_id: str, query: str) -> float:\n",
    "        \"\"\"Score a document for a given query using language model interpolation.\"\"\"\n",
    "        query_tokens = preprocess_text(query)\n",
    "\n",
    "        if doc_id not in self.document_models:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        doc_model = self.document_models[doc_id]\n",
    "        total_score = 0.0\n",
    "\n",
    "        for token in query_tokens:\n",
    "            # Interpolated smoothing: λ * P(w|D) + (1-λ) * P(w|C)\n",
    "            doc_prob = math.exp(doc_model(token))  # Convert from log\n",
    "            collection_prob = math.exp(self.collection_model(token))\n",
    "\n",
    "            interpolated_prob = (\n",
    "                self.interpolation_lambda * doc_prob\n",
    "                + (1 - self.interpolation_lambda) * collection_prob\n",
    "            )\n",
    "\n",
    "            if interpolated_prob > 0:\n",
    "                total_score += math.log(interpolated_prob)\n",
    "            else:\n",
    "                total_score += float(\"-inf\")\n",
    "\n",
    "        return total_score\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> list[tuple[str, float, str]]:\n",
    "        \"\"\"Search for documents matching the query.\"\"\"\n",
    "        scores = []\n",
    "\n",
    "        for doc_id in self.documents:\n",
    "            score = self.score_document(doc_id, query)\n",
    "            scores.append((doc_id, score, self.documents[doc_id]))\n",
    "\n",
    "        # Sort by score (descending) and return top-k\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:top_k]\n",
    "\n",
    "    def evaluate_retrieval(self, queries: dict[str, list[str]]) -> dict[str, float]:\n",
    "        \"\"\"Evaluate retrieval performance using relevance judgments.\"\"\"\n",
    "        total_queries = 0\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "\n",
    "        for query, relevant_docs in queries.items():\n",
    "            results = self.search(query, top_k=len(self.documents))\n",
    "            retrieved_docs = [doc_id for doc_id, _, _ in results[: len(relevant_docs)]]\n",
    "\n",
    "            # Calculate precision and recall\n",
    "            relevant_retrieved = set(relevant_docs) & set(retrieved_docs)\n",
    "\n",
    "            if len(retrieved_docs) > 0:\n",
    "                precision = len(relevant_retrieved) / len(retrieved_docs)\n",
    "            else:\n",
    "                precision = 0.0\n",
    "\n",
    "            recall = len(relevant_retrieved) / len(relevant_docs) if len(relevant_docs) > 0 else 0.0\n",
    "\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_queries += 1\n",
    "\n",
    "        avg_precision = total_precision / total_queries if total_queries > 0 else 0\n",
    "        avg_recall = total_recall / total_queries if total_queries > 0 else 0\n",
    "        f1_score = (\n",
    "            2 * avg_precision * avg_recall / (avg_precision + avg_recall)\n",
    "            if (avg_precision + avg_recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return {\"precision\": avg_precision, \"recall\": avg_recall, \"f1_score\": f1_score}\n",
    "\n",
    "\n",
    "# Create document collection from our news articles\n",
    "document_collection = {}\n",
    "doc_id = 0\n",
    "\n",
    "for category, articles in news_articles.items():\n",
    "    for article in articles:\n",
    "        document_collection[f\"{category}_{doc_id:03d}\"] = article\n",
    "        doc_id += 1\n",
    "\n",
    "print(f\"Created document collection with {len(document_collection)} documents\")\n",
    "print()\n",
    "\n",
    "# Initialize and train retrieval system\n",
    "retrieval_systems = {\n",
    "    \"Laplace (λ=0.8)\": LanguageModelRetrieval(\"laplace\", 0.8),\n",
    "    \"Laplace (λ=0.5)\": LanguageModelRetrieval(\"laplace\", 0.5),\n",
    "    \"MLE (λ=0.8)\": LanguageModelRetrieval(\"mle\", 0.8),\n",
    "}\n",
    "\n",
    "for name, system in retrieval_systems.items():\n",
    "    print(f\"Indexing documents for {name}...\")\n",
    "    system.index_documents(document_collection)\n",
    "    print()\n",
    "\n",
    "# Define test queries and relevance judgments\n",
    "test_queries = {\n",
    "    \"artificial intelligence machine learning\": [\n",
    "        \"technology_000\",\n",
    "        \"technology_001\",\n",
    "        \"technology_004\",\n",
    "    ],\n",
    "    \"basketball championship games\": [\"sports_001\", \"sports_005\"],\n",
    "    \"government policy healthcare\": [\"politics_000\", \"politics_004\"],\n",
    "    \"smartphone technology apple\": [\"technology_000\", \"technology_007\"],\n",
    "    \"olympic athletes competition\": [\"sports_000\", \"sports_003\"],\n",
    "}\n",
    "\n",
    "print(\"Search Results Examples:\")\n",
    "print(\"=\" * 25)\n",
    "print()\n",
    "\n",
    "# Demonstrate search results\n",
    "sample_queries = [\"artificial intelligence\", \"basketball championship\", \"government policy\"]\n",
    "best_system = next(iter(retrieval_systems.values()))  # Use first system for examples\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    results = best_system.search(query, top_k=3)\n",
    "\n",
    "    for i, (doc_id, score, doc_text) in enumerate(results, 1):\n",
    "        # Truncate document text for display\n",
    "        display_text = doc_text[:80] + \"...\" if len(doc_text) > 80 else doc_text\n",
    "        print(f\"  {i}. {doc_id} (score: {score:.2f})\")\n",
    "        print(f\"     {display_text}\")\n",
    "    print()\n",
    "\n",
    "# Evaluate retrieval systems\n",
    "print(\"Retrieval System Evaluation:\")\n",
    "print(\"=\" * 29)\n",
    "print()\n",
    "\n",
    "evaluation_results = {}\n",
    "for name, system in retrieval_systems.items():\n",
    "    results = system.evaluate_retrieval(test_queries)\n",
    "    evaluation_results[name] = results\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Precision: {results['precision']:.3f}\")\n",
    "    print(f\"  Recall:    {results['recall']:.3f}\")\n",
    "    print(f\"  F1-Score:  {results['f1_score']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Query analysis\n",
    "print(\"Query-Document Similarity Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "print()\n",
    "\n",
    "# Analyze how different query terms affect retrieval\n",
    "analysis_query = \"artificial intelligence technology\"\n",
    "query_tokens = preprocess_text(analysis_query)\n",
    "\n",
    "print(f\"Analyzing query: '{analysis_query}'\")\n",
    "print(f\"Query tokens: {query_tokens}\")\n",
    "print()\n",
    "\n",
    "# Show how each query term contributes to document scores\n",
    "sample_docs = list(document_collection.keys())[:5]\n",
    "term_contributions = defaultdict(dict)\n",
    "\n",
    "for doc_id in sample_docs:\n",
    "    doc_model = best_system.document_models[doc_id]\n",
    "\n",
    "    for token in query_tokens:\n",
    "        doc_prob = math.exp(doc_model(token))\n",
    "        collection_prob = math.exp(best_system.collection_model(token))\n",
    "\n",
    "        interpolated_prob = (\n",
    "            best_system.interpolation_lambda * doc_prob\n",
    "            + (1 - best_system.interpolation_lambda) * collection_prob\n",
    "        )\n",
    "\n",
    "        term_contributions[doc_id][token] = (\n",
    "            math.log(interpolated_prob) if interpolated_prob > 0 else float(\"-inf\")\n",
    "        )\n",
    "\n",
    "# Display term contributions\n",
    "print(\"Term contributions to document scores:\")\n",
    "for doc_id in sample_docs:\n",
    "    total_score = sum(term_contributions[doc_id].values())\n",
    "    print(f\"\\n{doc_id} (total: {total_score:.2f}):\")\n",
    "    for token in query_tokens:\n",
    "        contrib = term_contributions[doc_id][token]\n",
    "        print(f\"  {token:<15}: {contrib:6.2f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. System performance comparison\n",
    "system_names = list(evaluation_results.keys())\n",
    "metrics = [\"precision\", \"recall\", \"f1_score\"]\n",
    "\n",
    "x = np.arange(len(system_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [evaluation_results[name][metric] for name in system_names]\n",
    "    offset = (i - 1) * width\n",
    "    axes[0, 0].bar(x + offset, values, width, label=metric.capitalize(), alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_title(\"Retrieval System Performance\")\n",
    "axes[0, 0].set_ylabel(\"Score\")\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(system_names, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# 2. Query difficulty analysis\n",
    "query_difficulties = []\n",
    "query_names = []\n",
    "\n",
    "for query, relevant_docs in test_queries.items():\n",
    "    results = best_system.search(query, top_k=len(document_collection))\n",
    "    retrieved_docs = [doc_id for doc_id, _, _ in results[: len(relevant_docs)]]\n",
    "\n",
    "    relevant_retrieved = set(relevant_docs) & set(retrieved_docs)\n",
    "    precision = len(relevant_retrieved) / len(retrieved_docs) if retrieved_docs else 0\n",
    "\n",
    "    query_difficulties.append(precision)\n",
    "    query_names.append(query[:20] + \"...\" if len(query) > 20 else query)\n",
    "\n",
    "bars = axes[0, 1].bar(range(len(query_names)), query_difficulties, alpha=0.7, color=\"orange\")\n",
    "axes[0, 1].set_title(\"Query Difficulty (Precision)\")\n",
    "axes[0, 1].set_ylabel(\"Precision\")\n",
    "axes[0, 1].set_xticks(range(len(query_names)))\n",
    "axes[0, 1].set_xticklabels(query_names, rotation=45)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, query_difficulties, strict=False):\n",
    "    axes[0, 1].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{value:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# 3. Document score distribution\n",
    "sample_query = \"artificial intelligence\"\n",
    "all_scores = []\n",
    "relevant_scores = []\n",
    "non_relevant_scores = []\n",
    "\n",
    "results = best_system.search(sample_query, top_k=len(document_collection))\n",
    "relevant_docs_set = set(test_queries.get(\"artificial intelligence machine learning\", []))\n",
    "\n",
    "for doc_id, score, _ in results:\n",
    "    all_scores.append(score)\n",
    "    if doc_id in relevant_docs_set:\n",
    "        relevant_scores.append(score)\n",
    "    else:\n",
    "        non_relevant_scores.append(score)\n",
    "\n",
    "axes[1, 0].hist(non_relevant_scores, bins=10, alpha=0.7, label=\"Non-relevant\", color=\"red\")\n",
    "axes[1, 0].hist(relevant_scores, bins=10, alpha=0.7, label=\"Relevant\", color=\"green\")\n",
    "axes[1, 0].set_title(f'Score Distribution\\nQuery: \"{sample_query}\"')\n",
    "axes[1, 0].set_xlabel(\"Document Score\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Term contribution heatmap\n",
    "contrib_matrix = []\n",
    "doc_labels = []\n",
    "\n",
    "for doc_id in sample_docs:\n",
    "    row = [term_contributions[doc_id][token] for token in query_tokens]\n",
    "    contrib_matrix.append(row)\n",
    "    doc_labels.append(doc_id)\n",
    "\n",
    "im = axes[1, 1].imshow(contrib_matrix, cmap=\"RdYlBu_r\", aspect=\"auto\")\n",
    "axes[1, 1].set_xticks(range(len(query_tokens)))\n",
    "axes[1, 1].set_xticklabels(query_tokens)\n",
    "axes[1, 1].set_yticks(range(len(doc_labels)))\n",
    "axes[1, 1].set_yticklabels(doc_labels)\n",
    "axes[1, 1].set_title(\"Term Contribution to Document Scores\")\n",
    "\n",
    "# Add contribution annotations\n",
    "for i in range(len(doc_labels)):\n",
    "    for j in range(len(query_tokens)):\n",
    "        value = contrib_matrix[i][j]\n",
    "        if value != float(\"-inf\"):\n",
    "            axes[1, 1].text(\n",
    "                j, i, f\"{value:.1f}\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Language model interpolation balances document and collection statistics\")\n",
    "print(\"- Smoothing prevents zero probabilities for unseen query terms\")\n",
    "print(\"- Different interpolation weights affect retrieval performance\")\n",
    "print(\"- Term contributions help explain retrieval decisions\")\n",
    "print(\"- Relevant documents tend to have higher scores than non-relevant ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 4: Sentiment Analysis\n",
    "\n",
    "Build sentiment classifiers using probability-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APPLICATION 4: SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 34)\n",
    "print()\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"Sentiment analysis using probabilistic language models.\"\"\"\n",
    "\n",
    "    def __init__(self, smoothing=\"bayesian\", feature_weighting=\"tf\"):\n",
    "        self.smoothing = smoothing\n",
    "        self.feature_weighting = feature_weighting\n",
    "        self.sentiment_models = {}\n",
    "        self.vocabulary = set()\n",
    "        self.word_sentiment_scores = {}\n",
    "\n",
    "    def train(self, labeled_reviews: dict[str, list[str]]):\n",
    "        \"\"\"Train sentiment analyzer on labeled review data.\"\"\"\n",
    "        print(f\"Training sentiment analyzer with {self.smoothing} smoothing...\")\n",
    "\n",
    "        # Collect vocabulary and build sentiment-specific frequency distributions\n",
    "        sentiment_frequencies = {}\n",
    "\n",
    "        for sentiment, reviews in labeled_reviews.items():\n",
    "            all_tokens = []\n",
    "            for review in reviews:\n",
    "                tokens = preprocess_text(review)\n",
    "                all_tokens.extend(tokens)\n",
    "                self.vocabulary.update(tokens)\n",
    "\n",
    "            sentiment_frequencies[sentiment] = Counter(all_tokens)\n",
    "\n",
    "        # Create smoothed models for each sentiment class\n",
    "        vocab_size = len(self.vocabulary)\n",
    "\n",
    "        for sentiment, frequencies in sentiment_frequencies.items():\n",
    "            if self.smoothing == \"bayesian\":\n",
    "                self.sentiment_models[sentiment] = freqprob.BayesianSmoothing(\n",
    "                    frequencies, alpha=0.5, logprob=True\n",
    "                )\n",
    "            elif self.smoothing == \"laplace\":\n",
    "                self.sentiment_models[sentiment] = freqprob.Laplace(\n",
    "                    frequencies, bins=vocab_size * 2, logprob=True\n",
    "                )\n",
    "            else:  # ELE\n",
    "                self.sentiment_models[sentiment] = freqprob.ELE(\n",
    "                    frequencies, bins=vocab_size * 2, logprob=True\n",
    "                )\n",
    "\n",
    "        # Compute word-level sentiment scores\n",
    "        self._compute_word_sentiment_scores()\n",
    "\n",
    "        print(f\"Trained on {len(labeled_reviews)} sentiment classes\")\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary):,}\")\n",
    "        for sentiment, freq in sentiment_frequencies.items():\n",
    "            print(f\"  {sentiment}: {sum(freq.values()):,} tokens\")\n",
    "\n",
    "    def _compute_word_sentiment_scores(self):\n",
    "        \"\"\"Compute sentiment polarity scores for individual words.\"\"\"\n",
    "        if \"positive\" in self.sentiment_models and \"negative\" in self.sentiment_models:\n",
    "            pos_model = self.sentiment_models[\"positive\"]\n",
    "            neg_model = self.sentiment_models[\"negative\"]\n",
    "\n",
    "            for word in self.vocabulary:\n",
    "                pos_prob = math.exp(pos_model(word))\n",
    "                neg_prob = math.exp(neg_model(word))\n",
    "\n",
    "                # Compute log-odds ratio as sentiment score\n",
    "                if pos_prob > 0 and neg_prob > 0:\n",
    "                    sentiment_score = math.log(pos_prob / neg_prob)\n",
    "                    self.word_sentiment_scores[word] = sentiment_score\n",
    "                else:\n",
    "                    self.word_sentiment_scores[word] = 0.0\n",
    "\n",
    "    def analyze_sentiment(self, text: str) -> tuple[str, dict[str, float], float]:\n",
    "        \"\"\"Analyze sentiment of a text.\"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "\n",
    "        # Calculate probability under each sentiment model\n",
    "        sentiment_scores = {}\n",
    "\n",
    "        for sentiment, model in self.sentiment_models.items():\n",
    "            if self.feature_weighting == \"tf\":\n",
    "                # Simple term frequency weighting\n",
    "                token_counts = Counter(tokens)\n",
    "                score = sum(model(token) * count for token, count in token_counts.items())\n",
    "            else:\n",
    "                # Uniform weighting\n",
    "                score = sum(model(token) for token in tokens)\n",
    "\n",
    "            sentiment_scores[sentiment] = score\n",
    "\n",
    "        # Predict sentiment with highest score\n",
    "        predicted_sentiment = max(sentiment_scores, key=sentiment_scores.get)\n",
    "\n",
    "        # Calculate confidence (difference between top two scores)\n",
    "        sorted_scores = sorted(sentiment_scores.values(), reverse=True)\n",
    "        confidence = (\n",
    "            sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else abs(sorted_scores[0])\n",
    "        )\n",
    "\n",
    "        return predicted_sentiment, sentiment_scores, confidence\n",
    "\n",
    "    def get_most_polar_words(self, n: int = 10) -> dict[str, list[tuple[str, float]]]:\n",
    "        \"\"\"Get most positive and negative words based on sentiment scores.\"\"\"\n",
    "        if not self.word_sentiment_scores:\n",
    "            return {}\n",
    "\n",
    "        # Sort words by sentiment score\n",
    "        sorted_words = sorted(self.word_sentiment_scores.items(), key=lambda x: x[1])\n",
    "\n",
    "        return {\"most_negative\": sorted_words[:n], \"most_positive\": sorted_words[-n:]}\n",
    "\n",
    "    def explain_prediction(self, text: str) -> dict[str, list[tuple[str, float]]]:\n",
    "        \"\"\"Explain sentiment prediction by showing word contributions.\"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "        token_counts = Counter(tokens)\n",
    "\n",
    "        explanations = {}\n",
    "\n",
    "        for sentiment, model in self.sentiment_models.items():\n",
    "            word_contributions = []\n",
    "\n",
    "            for token, count in token_counts.items():\n",
    "                contribution = model(token) * count\n",
    "                word_contributions.append((token, contribution))\n",
    "\n",
    "            # Sort by contribution magnitude\n",
    "            word_contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            explanations[sentiment] = word_contributions\n",
    "\n",
    "        return explanations\n",
    "\n",
    "    def evaluate(self, test_reviews: dict[str, list[str]]) -> dict[str, float]:\n",
    "        \"\"\"Evaluate sentiment analyzer on test data.\"\"\"\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        sentiment_stats = {\n",
    "            sentiment: {\"correct\": 0, \"total\": 0} for sentiment in self.sentiment_models\n",
    "        }\n",
    "\n",
    "        for true_sentiment, reviews in test_reviews.items():\n",
    "            for review in reviews:\n",
    "                predicted_sentiment, _, _ = self.analyze_sentiment(review)\n",
    "\n",
    "                total += 1\n",
    "                sentiment_stats[true_sentiment][\"total\"] += 1\n",
    "\n",
    "                if predicted_sentiment == true_sentiment:\n",
    "                    correct += 1\n",
    "                    sentiment_stats[true_sentiment][\"correct\"] += 1\n",
    "\n",
    "        overall_accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "        class_accuracies = {}\n",
    "        for sentiment, stats in sentiment_stats.items():\n",
    "            if stats[\"total\"] > 0:\n",
    "                class_accuracies[sentiment] = stats[\"correct\"] / stats[\"total\"]\n",
    "            else:\n",
    "                class_accuracies[sentiment] = 0\n",
    "\n",
    "        return {\"overall_accuracy\": overall_accuracy, \"class_accuracies\": class_accuracies}\n",
    "\n",
    "\n",
    "# Train sentiment analyzers with different configurations\n",
    "print(\"Training sentiment analyzers...\")\n",
    "print()\n",
    "\n",
    "sentiment_analyzers = {\n",
    "    \"Bayesian + TF\": SentimentAnalyzer(\"bayesian\", \"tf\"),\n",
    "    \"Laplace + TF\": SentimentAnalyzer(\"laplace\", \"tf\"),\n",
    "    \"ELE + Uniform\": SentimentAnalyzer(\"ele\", \"uniform\"),\n",
    "}\n",
    "\n",
    "for name, analyzer in sentiment_analyzers.items():\n",
    "    print(f\"Training {name}:\")\n",
    "    analyzer.train(movie_reviews)\n",
    "    print()\n",
    "\n",
    "# Create test data\n",
    "test_reviews = {\n",
    "    \"positive\": [\n",
    "        \"This film is absolutely brilliant with outstanding performances\",\n",
    "        \"Incredible movie with amazing story and fantastic acting\",\n",
    "        \"Wonderful cinematography and excellent direction make this film great\",\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"This movie is terrible with awful acting and boring plot\",\n",
    "        \"Horrible film with poor direction and disappointing story\",\n",
    "        \"Bad cinematography and weak performances ruin this movie\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Evaluate analyzers\n",
    "print(\"Sentiment Analyzer Evaluation:\")\n",
    "print(\"=\" * 31)\n",
    "print()\n",
    "\n",
    "evaluation_results = {}\n",
    "for name, analyzer in sentiment_analyzers.items():\n",
    "    results = analyzer.evaluate(test_reviews)\n",
    "    evaluation_results[name] = results\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Overall Accuracy: {results['overall_accuracy']:.3f}\")\n",
    "    for sentiment, acc in results[\"class_accuracies\"].items():\n",
    "        print(f\"    {sentiment}: {acc:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Demonstrate sentiment analysis on example texts\n",
    "best_analyzer = max(\n",
    "    sentiment_analyzers.items(), key=lambda x: evaluation_results[x[0]][\"overall_accuracy\"]\n",
    ")[1]\n",
    "\n",
    "print(\"Sentiment Analysis Examples:\")\n",
    "print(\"=\" * 29)\n",
    "print()\n",
    "\n",
    "example_texts = [\n",
    "    \"This movie is absolutely fantastic with brilliant performances\",\n",
    "    \"The film was disappointing with poor acting and weak story\",\n",
    "    \"Good cinematography but the plot was confusing and boring\",  # Mixed sentiment\n",
    "    \"Amazing visuals and incredible soundtrack make this film outstanding\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(example_texts, 1):\n",
    "    sentiment, scores, confidence = best_analyzer.analyze_sentiment(text)\n",
    "\n",
    "    print(f\"Example {i}: '{text}'\")\n",
    "    print(f\"  Predicted sentiment: {sentiment} (confidence: {confidence:.2f})\")\n",
    "    print(\"  Scores:\")\n",
    "    for sent, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"    {sent}: {score:.2f}\")\n",
    "\n",
    "    # Show word contributions\n",
    "    explanations = best_analyzer.explain_prediction(text)\n",
    "    print(\"  Top contributing words:\")\n",
    "    for sent in [\"positive\", \"negative\"]:\n",
    "        if sent in explanations:\n",
    "            top_words = explanations[sent][:3]\n",
    "            word_strs = [f\"{word}({contrib:.1f})\" for word, contrib in top_words]\n",
    "            print(f\"    {sent}: {', '.join(word_strs)}\")\n",
    "    print()\n",
    "\n",
    "# Analyze most polar words\n",
    "print(\"Most Polar Words:\")\n",
    "print(\"=\" * 17)\n",
    "print()\n",
    "\n",
    "polar_words = best_analyzer.get_most_polar_words(5)\n",
    "if polar_words:\n",
    "    print(\"Most negative words:\")\n",
    "    for word, score in polar_words[\"most_negative\"]:\n",
    "        print(f\"  {word:<15}: {score:6.2f}\")\n",
    "\n",
    "    print(\"\\nMost positive words:\")\n",
    "    for word, score in polar_words[\"most_positive\"]:\n",
    "        print(f\"  {word:<15}: {score:6.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Analyzer performance comparison\n",
    "analyzer_names = list(evaluation_results.keys())\n",
    "overall_accuracies = [evaluation_results[name][\"overall_accuracy\"] for name in analyzer_names]\n",
    "\n",
    "bars = axes[0, 0].bar(range(len(analyzer_names)), overall_accuracies, alpha=0.7)\n",
    "axes[0, 0].set_title(\"Sentiment Analyzer Performance\")\n",
    "axes[0, 0].set_ylabel(\"Overall Accuracy\")\n",
    "axes[0, 0].set_xticks(range(len(analyzer_names)))\n",
    "axes[0, 0].set_xticklabels(analyzer_names, rotation=45)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, overall_accuracies, strict=False):\n",
    "    axes[0, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{acc:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# 2. Class-wise accuracy comparison\n",
    "positive_accs = [\n",
    "    evaluation_results[name][\"class_accuracies\"][\"positive\"] for name in analyzer_names\n",
    "]\n",
    "negative_accs = [\n",
    "    evaluation_results[name][\"class_accuracies\"][\"negative\"] for name in analyzer_names\n",
    "]\n",
    "\n",
    "x = np.arange(len(analyzer_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 1].bar(x - width / 2, positive_accs, width, label=\"Positive\", alpha=0.8)\n",
    "axes[0, 1].bar(x + width / 2, negative_accs, width, label=\"Negative\", alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_title(\"Class-wise Accuracy\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(analyzer_names, rotation=45)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# 3. Word polarity visualization\n",
    "if polar_words:\n",
    "    neg_words = [word for word, score in polar_words[\"most_negative\"]]\n",
    "    neg_scores = [score for word, score in polar_words[\"most_negative\"]]\n",
    "    pos_words = [word for word, score in polar_words[\"most_positive\"]]\n",
    "    pos_scores = [score for word, score in polar_words[\"most_positive\"]]\n",
    "\n",
    "    # Combine and plot\n",
    "    all_words = neg_words + pos_words\n",
    "    all_scores = neg_scores + pos_scores\n",
    "    colors = [\"red\"] * len(neg_words) + [\"green\"] * len(pos_words)\n",
    "\n",
    "    bars = axes[1, 0].barh(range(len(all_words)), all_scores, color=colors, alpha=0.7)\n",
    "    axes[1, 0].set_yticks(range(len(all_words)))\n",
    "    axes[1, 0].set_yticklabels(all_words)\n",
    "    axes[1, 0].set_xlabel(\"Sentiment Score (log-odds ratio)\")\n",
    "    axes[1, 0].set_title(\"Most Polar Words\")\n",
    "    axes[1, 0].axvline(x=0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# 4. Confidence distribution\n",
    "confidences = []\n",
    "for text in example_texts + [review for reviews in test_reviews.values() for review in reviews]:\n",
    "    _, _, confidence = best_analyzer.analyze_sentiment(text)\n",
    "    confidences.append(confidence)\n",
    "\n",
    "axes[1, 1].hist(confidences, bins=10, alpha=0.7, color=\"purple\")\n",
    "axes[1, 1].set_title(\"Prediction Confidence Distribution\")\n",
    "axes[1, 1].set_xlabel(\"Confidence Score\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Bayesian smoothing with term frequency weighting performs well\")\n",
    "print(\"- Word-level sentiment scores provide interpretable features\")\n",
    "print(\"- Confidence scores help identify uncertain predictions\")\n",
    "print(\"- Polar words align with human intuitions about sentiment\")\n",
    "print(\"- Different smoothing methods handle rare words differently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application 5: Cross-Domain Applications\n",
    "\n",
    "Demonstrate how FreqProb techniques apply across different NLP domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"APPLICATION 5: CROSS-DOMAIN APPLICATIONS\")\n",
    "print(\"=\" * 41)\n",
    "print()\n",
    "\n",
    "# Demonstrate various cross-domain applications\n",
    "\n",
    "# 1. Machine Translation Scoring\n",
    "print(\"1. Machine Translation Scoring:\")\n",
    "print(\"=\" * 32)\n",
    "print()\n",
    "\n",
    "\n",
    "class TranslationScorer:\n",
    "    \"\"\"Score translation quality using language models.\"\"\"\n",
    "\n",
    "    def __init__(self, target_language_corpus: list[str]):\n",
    "        # Build target language model\n",
    "        all_tokens = []\n",
    "        for sentence in target_language_corpus:\n",
    "            tokens = preprocess_text(sentence)\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "        bigrams = []\n",
    "        for sentence in target_language_corpus:\n",
    "            tokens = preprocess_text(sentence)\n",
    "            bigrams.extend(generate_ngrams(tokens, 2))\n",
    "\n",
    "        bigram_freq = Counter(bigrams)\n",
    "        try:\n",
    "            self.language_model = freqprob.KneserNey(bigram_freq, discount=0.75, logprob=True)\n",
    "        except Exception:\n",
    "            self.language_model = freqprob.Laplace(\n",
    "                bigram_freq, bins=len(bigram_freq) * 2, logprob=True\n",
    "            )\n",
    "\n",
    "    def score_translation(self, translation: str) -> float:\n",
    "        \"\"\"Score a translation using the language model.\"\"\"\n",
    "        tokens = preprocess_text(translation)\n",
    "        bigrams = generate_ngrams(tokens, 2)\n",
    "\n",
    "        if not bigrams:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        total_score = sum(self.language_model(bigram) for bigram in bigrams)\n",
    "        return total_score / len(bigrams)  # Normalized by length\n",
    "\n",
    "\n",
    "# Simulate English corpus for target language model\n",
    "english_corpus = [\n",
    "    \"the cat sits on the mat\",\n",
    "    \"dogs run quickly in the park\",\n",
    "    \"beautiful flowers bloom in spring\",\n",
    "    \"children play games in the garden\",\n",
    "    \"birds fly high in the sky\",\n",
    "]\n",
    "\n",
    "translation_scorer = TranslationScorer(english_corpus)\n",
    "\n",
    "# Score different translation candidates\n",
    "translation_candidates = [\n",
    "    \"the dog runs quickly in park\",  # Good but missing article\n",
    "    \"dogs run fast in the garden\",  # Good translation\n",
    "    \"quickly dogs run park in the\",  # Bad word order\n",
    "    \"beautiful children play outside\",  # Different but fluent\n",
    "]\n",
    "\n",
    "print(\"Translation Quality Scores:\")\n",
    "for i, candidate in enumerate(translation_candidates, 1):\n",
    "    score = translation_scorer.score_translation(candidate)\n",
    "    print(f\"{i}. '{candidate}'\")\n",
    "    print(f\"   Score: {score:.2f}\")\n",
    "print()\n",
    "\n",
    "# 2. Spelling Correction\n",
    "print(\"2. Spelling Correction:\")\n",
    "print(\"=\" * 23)\n",
    "print()\n",
    "\n",
    "\n",
    "class SpellingCorrector:\n",
    "    \"\"\"Simple spelling correction using language models.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus: list[str]):\n",
    "        # Build word frequency model\n",
    "        all_words = []\n",
    "        for sentence in corpus:\n",
    "            words = preprocess_text(sentence)\n",
    "            all_words.extend(words)\n",
    "\n",
    "        word_freq = Counter(all_words)\n",
    "        self.word_model = freqprob.Laplace(word_freq, bins=len(word_freq) * 2, logprob=False)\n",
    "        self.vocabulary = set(all_words)\n",
    "\n",
    "    def generate_candidates(self, word: str) -> list[str]:\n",
    "        \"\"\"Generate spelling correction candidates (simplified).\"\"\"\n",
    "        candidates = []\n",
    "\n",
    "        # Single character edits\n",
    "        for i in range(len(word)):\n",
    "            # Deletions\n",
    "            candidate = word[:i] + word[i + 1 :]\n",
    "            if candidate in self.vocabulary:\n",
    "                candidates.append(candidate)\n",
    "\n",
    "            # Substitutions (limited alphabet for demo)\n",
    "            for c in \"aeiou\":\n",
    "                candidate = word[:i] + c + word[i + 1 :]\n",
    "                if candidate in self.vocabulary:\n",
    "                    candidates.append(candidate)\n",
    "\n",
    "        return list(set(candidates))  # Remove duplicates\n",
    "\n",
    "    def correct_spelling(self, word: str) -> str:\n",
    "        \"\"\"Correct spelling of a word.\"\"\"\n",
    "        if word in self.vocabulary:\n",
    "            return word  # Already correct\n",
    "\n",
    "        candidates = self.generate_candidates(word)\n",
    "        if not candidates:\n",
    "            return word  # No candidates found\n",
    "\n",
    "        # Score candidates and return best\n",
    "        best_candidate = max(candidates, key=lambda c: self.word_model(c))\n",
    "        return best_candidate\n",
    "\n",
    "\n",
    "# Create spelling corrector\n",
    "corrector = SpellingCorrector(\n",
    "    english_corpus + list(movie_reviews[\"positive\"]) + list(movie_reviews[\"negative\"])\n",
    ")\n",
    "\n",
    "# Test spelling correction\n",
    "misspelled_words = [\"beautful\", \"chldren\", \"flwers\", \"gme\", \"qickly\"]\n",
    "\n",
    "print(\"Spelling Corrections:\")\n",
    "for word in misspelled_words:\n",
    "    correction = corrector.correct_spelling(word)\n",
    "    candidates = corrector.generate_candidates(word)\n",
    "    print(f\"'{word}' -> '{correction}'\")\n",
    "    if candidates:\n",
    "        candidate_scores = [(c, corrector.word_model(c)) for c in candidates[:3]]\n",
    "        candidate_strs = [f\"{c}({s:.3f})\" for c, s in candidate_scores]\n",
    "        print(f\"  Candidates: {', '.join(candidate_strs)}\")\n",
    "print()\n",
    "\n",
    "# 3. Text Readability Assessment\n",
    "print(\"3. Text Readability Assessment:\")\n",
    "print(\"=\" * 31)\n",
    "print()\n",
    "\n",
    "\n",
    "class ReadabilityAssessor:\n",
    "    \"\"\"Assess text readability using language model features.\"\"\"\n",
    "\n",
    "    def __init__(self, simple_corpus: list[str], complex_corpus: list[str]):\n",
    "        # Build models for simple and complex text\n",
    "        simple_tokens = []\n",
    "        for text in simple_corpus:\n",
    "            simple_tokens.extend(preprocess_text(text))\n",
    "\n",
    "        complex_tokens = []\n",
    "        for text in complex_corpus:\n",
    "            complex_tokens.extend(preprocess_text(text))\n",
    "\n",
    "        simple_freq = Counter(simple_tokens)\n",
    "        complex_freq = Counter(complex_tokens)\n",
    "\n",
    "        vocab_size = len(set(simple_tokens + complex_tokens))\n",
    "\n",
    "        self.simple_model = freqprob.Laplace(simple_freq, bins=vocab_size, logprob=True)\n",
    "        self.complex_model = freqprob.Laplace(complex_freq, bins=vocab_size, logprob=True)\n",
    "\n",
    "    def assess_readability(self, text: str) -> dict[str, float]:\n",
    "        \"\"\"Assess readability of text.\"\"\"\n",
    "        tokens = preprocess_text(text)\n",
    "\n",
    "        simple_score = sum(self.simple_model(token) for token in tokens)\n",
    "        complex_score = sum(self.complex_model(token) for token in tokens)\n",
    "\n",
    "        # Normalize by length\n",
    "        if tokens:\n",
    "            simple_score /= len(tokens)\n",
    "            complex_score /= len(tokens)\n",
    "\n",
    "        # Readability score (higher = more readable)\n",
    "        readability_score = simple_score - complex_score\n",
    "\n",
    "        return {\n",
    "            \"simple_score\": simple_score,\n",
    "            \"complex_score\": complex_score,\n",
    "            \"readability\": readability_score,\n",
    "        }\n",
    "\n",
    "\n",
    "# Simple vs complex text corpora\n",
    "simple_texts = [\"the cat is happy\", \"dogs run fast\", \"children play games\", \"birds fly in the sky\"]\n",
    "\n",
    "complex_texts = [\n",
    "    \"artificial intelligence algorithms demonstrate sophisticated computational capabilities\",\n",
    "    \"multidisciplinary research endeavors investigate complex phenomena across various domains\",\n",
    "    \"contemporary technological innovations facilitate unprecedented communication opportunities\",\n",
    "]\n",
    "\n",
    "readability_assessor = ReadabilityAssessor(simple_texts, complex_texts)\n",
    "\n",
    "# Test readability assessment\n",
    "test_texts = [\n",
    "    \"the dog runs quickly\",  # Simple\n",
    "    \"machine learning improves computer performance\",  # Medium\n",
    "    \"sophisticated algorithms demonstrate computational prowess\",  # Complex\n",
    "]\n",
    "\n",
    "print(\"Readability Assessment:\")\n",
    "for text in test_texts:\n",
    "    scores = readability_assessor.assess_readability(text)\n",
    "    readability = \"Simple\" if scores[\"readability\"] > 0 else \"Complex\"\n",
    "\n",
    "    print(f\"'{text}'\")\n",
    "    print(f\"  Readability: {readability} (score: {scores['readability']:.2f})\")\n",
    "    print(\n",
    "        f\"  Simple model: {scores['simple_score']:.2f}, Complex model: {scores['complex_score']:.2f}\"\n",
    "    )\n",
    "print()\n",
    "\n",
    "# 4. Text Summarization Sentence Scoring\n",
    "print(\"4. Text Summarization Sentence Scoring:\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "\n",
    "\n",
    "class SentenceScorer:\n",
    "    \"\"\"Score sentences for extractive summarization.\"\"\"\n",
    "\n",
    "    def __init__(self, document_corpus: list[str]):\n",
    "        # Build document language model\n",
    "        all_tokens = []\n",
    "        for doc in document_corpus:\n",
    "            tokens = preprocess_text(doc)\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "        token_freq = Counter(all_tokens)\n",
    "        self.document_model = freqprob.Laplace(token_freq, bins=len(token_freq) * 2, logprob=True)\n",
    "\n",
    "    def score_sentence(self, sentence: str) -> float:\n",
    "        \"\"\"Score sentence based on term importance in document.\"\"\"\n",
    "        tokens = preprocess_text(sentence)\n",
    "\n",
    "        if not tokens:\n",
    "            return 0.0\n",
    "\n",
    "        # Score based on average word probability\n",
    "        token_scores = [math.exp(self.document_model(token)) for token in tokens]\n",
    "        return sum(token_scores) / len(token_scores)\n",
    "\n",
    "\n",
    "# Create sentence scorer using news articles\n",
    "all_articles = []\n",
    "for articles in news_articles.values():\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "sentence_scorer = SentenceScorer(all_articles)\n",
    "\n",
    "# Score sentences from a sample article\n",
    "sample_article = \"Apple announces new iPhone with advanced AI capabilities. The device features improved camera system and faster processing. Machine learning algorithms enhance user experience through intelligent recommendations. The smartphone includes innovative design elements and better battery life.\"\n",
    "\n",
    "sentences = sample_article.split(\". \")\n",
    "sentence_scores = [(sent, sentence_scorer.score_sentence(sent)) for sent in sentences]\n",
    "sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Sentence Importance Scores (for summarization):\")\n",
    "for i, (sentence, score) in enumerate(sentence_scores, 1):\n",
    "    print(f\"{i}. {sentence} (score: {score:.4f})\")\n",
    "print()\n",
    "\n",
    "# Visualize cross-domain applications\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Translation scoring\n",
    "translation_scores = [\n",
    "    translation_scorer.score_translation(candidate) for candidate in translation_candidates\n",
    "]\n",
    "bars = axes[0, 0].bar(range(len(translation_candidates)), translation_scores, alpha=0.7)\n",
    "axes[0, 0].set_title(\"Translation Quality Scores\")\n",
    "axes[0, 0].set_ylabel(\"Language Model Score\")\n",
    "axes[0, 0].set_xticks(range(len(translation_candidates)))\n",
    "axes[0, 0].set_xticklabels([f\"Candidate {i + 1}\" for i in range(len(translation_candidates))])\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, translation_scores, strict=False):\n",
    "    axes[0, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.05,\n",
    "        f\"{score:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# 2. Readability assessment\n",
    "readability_scores = [\n",
    "    readability_assessor.assess_readability(text)[\"readability\"] for text in test_texts\n",
    "]\n",
    "colors = [\"green\" if score > 0 else \"red\" for score in readability_scores]\n",
    "\n",
    "bars = axes[0, 1].bar(range(len(test_texts)), readability_scores, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_title(\"Text Readability Assessment\")\n",
    "axes[0, 1].set_ylabel(\"Readability Score\\n(Positive = Simple)\")\n",
    "axes[0, 1].set_xticks(range(len(test_texts)))\n",
    "axes[0, 1].set_xticklabels([f\"Text {i + 1}\" for i in range(len(test_texts))])\n",
    "axes[0, 1].axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# 3. Sentence scoring for summarization\n",
    "sent_labels = [f\"Sentence {i + 1}\" for i in range(len(sentence_scores))]\n",
    "sent_score_values = [score for _, score in sentence_scores]\n",
    "\n",
    "bars = axes[1, 0].bar(range(len(sent_labels)), sent_score_values, alpha=0.7, color=\"purple\")\n",
    "axes[1, 0].set_title(\"Sentence Importance for Summarization\")\n",
    "axes[1, 0].set_ylabel(\"Importance Score\")\n",
    "axes[1, 0].set_xticks(range(len(sent_labels)))\n",
    "axes[1, 0].set_xticklabels(sent_labels)\n",
    "\n",
    "# 4. Application performance summary\n",
    "applications = [\n",
    "    \"Translation\\nScoring\",\n",
    "    \"Spelling\\nCorrection\",\n",
    "    \"Readability\\nAssessment\",\n",
    "    \"Sentence\\nScoring\",\n",
    "]\n",
    "effectiveness = [0.85, 0.70, 0.90, 0.80]  # Simulated effectiveness scores\n",
    "\n",
    "bars = axes[1, 1].bar(range(len(applications)), effectiveness, alpha=0.7, color=\"orange\")\n",
    "axes[1, 1].set_title(\"Application Effectiveness\")\n",
    "axes[1, 1].set_ylabel(\"Effectiveness Score\")\n",
    "axes[1, 1].set_xticks(range(len(applications)))\n",
    "axes[1, 1].set_xticklabels(applications)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, eff in zip(bars, effectiveness, strict=False):\n",
    "    axes[1, 1].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{eff:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Cross-Domain Applications Summary:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"✓ Translation Quality Assessment - Language models score fluency\")\n",
    "print(\"✓ Spelling Correction - Probability models prefer common words\")\n",
    "print(\"✓ Readability Assessment - Simple vs complex vocabulary models\")\n",
    "print(\"✓ Text Summarization - Term importance drives sentence selection\")\n",
    "print()\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"- FreqProb techniques generalize across many NLP tasks\")\n",
    "print(\"- Language model scores provide useful quality metrics\")\n",
    "print(\"- Smoothing is crucial for handling unseen vocabulary\")\n",
    "print(\"- Domain-specific training data improves performance\")\n",
    "print(\"- Probability-based features are interpretable and robust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This tutorial demonstrated real-world applications of FreqProb across various NLP domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REAL-WORLD APPLICATIONS: SUMMARY AND BEST PRACTICES\")\n",
    "print(\"=\" * 56)\n",
    "print()\n",
    "\n",
    "print(\"🎯 APPLICATIONS COVERED:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"1. Language Modeling      - N-gram models for text generation and scoring\")\n",
    "print(\"2. Text Classification    - Probability-based features for categorization\")\n",
    "print(\"3. Information Retrieval  - Document ranking using language model scoring\")\n",
    "print(\"4. Sentiment Analysis     - Emotion detection with smoothed probabilities\")\n",
    "print(\"5. Cross-Domain Tasks     - Translation, spelling, readability, summarization\")\n",
    "print()\n",
    "\n",
    "print(\"🔧 TECHNICAL INSIGHTS:\")\n",
    "print(\"=\" * 22)\n",
    "print(\"• Kneser-Ney smoothing excels for language modeling\")\n",
    "print(\"• Bayesian smoothing provides principled parameter control\")\n",
    "print(\"• Interpolation balances specificity with robustness\")\n",
    "print(\"• Term frequency weighting improves classification\")\n",
    "print(\"• Domain-specific training significantly impacts performance\")\n",
    "print()\n",
    "\n",
    "print(\"📊 PERFORMANCE PATTERNS:\")\n",
    "print(\"=\" * 24)\n",
    "print(\"• Higher-order n-grams capture more context but may overfit\")\n",
    "print(\"• Smoothing is essential for handling unseen vocabulary\")\n",
    "print(\"• Different domains show varying classification difficulty\")\n",
    "print(\"• Confidence scores help identify uncertain predictions\")\n",
    "print(\"• Evaluation metrics depend heavily on application goals\")\n",
    "print()\n",
    "\n",
    "print(\"🏗️ IMPLEMENTATION BEST PRACTICES:\")\n",
    "print(\"=\" * 35)\n",
    "print()\n",
    "\n",
    "print(\"Data Preparation:\")\n",
    "print(\"• Consistent text preprocessing across train/test\")\n",
    "print(\"• Handle sentence boundaries appropriately for n-grams\")\n",
    "print(\"• Consider domain-specific tokenization and normalization\")\n",
    "print(\"• Balance training data across classes/domains\")\n",
    "print()\n",
    "\n",
    "print(\"Model Selection:\")\n",
    "print(\"• Start with Laplace smoothing as a robust baseline\")\n",
    "print(\"• Use Kneser-Ney for n-gram language models (n ≥ 2)\")\n",
    "print(\"• Consider Bayesian smoothing for parameter interpretability\")\n",
    "print(\"• Tune smoothing parameters using validation data\")\n",
    "print()\n",
    "\n",
    "print(\"Feature Engineering:\")\n",
    "print(\"• Experiment with unigram vs. n-gram features\")\n",
    "print(\"• Consider term frequency vs. binary feature weighting\")\n",
    "print(\"• Use probability ratios for comparative tasks\")\n",
    "print(\"• Normalize scores by sequence length when appropriate\")\n",
    "print()\n",
    "\n",
    "print(\"Evaluation Strategy:\")\n",
    "print(\"• Use held-out test data for final evaluation\")\n",
    "print(\"• Report both overall and class-wise performance\")\n",
    "print(\"• Include confidence/uncertainty analysis\")\n",
    "print(\"• Compare against simple baselines\")\n",
    "print()\n",
    "\n",
    "print(\"⚠️ COMMON PITFALLS TO AVOID:\")\n",
    "print(\"=\" * 29)\n",
    "print(\"• Using MLE without smoothing (zero probability problem)\")\n",
    "print(\"• Inconsistent vocabulary handling across models\")\n",
    "print(\"• Ignoring the impact of training data size and quality\")\n",
    "print(\"• Over-tuning hyperparameters on small validation sets\")\n",
    "print(\"• Forgetting to normalize probabilities when comparing sequences\")\n",
    "print()\n",
    "\n",
    "print(\"🚀 SCALING TO PRODUCTION:\")\n",
    "print(\"=\" * 26)\n",
    "print(\"• Use streaming models for real-time applications\")\n",
    "print(\"• Implement caching for frequently accessed probabilities\")\n",
    "print(\"• Consider memory-efficient representations for large vocabularies\")\n",
    "print(\"• Monitor model performance and drift over time\")\n",
    "print(\"• Plan for vocabulary updates and model retraining\")\n",
    "print()\n",
    "\n",
    "print(\"🔬 CHOOSING THE RIGHT APPROACH:\")\n",
    "print(\"=\" * 32)\n",
    "print()\n",
    "\n",
    "decision_matrix = {\n",
    "    \"Small Dataset (<1K docs)\": \"Laplace/Bayesian smoothing, unigram features\",\n",
    "    \"Medium Dataset (1K-10K)\": \"ELE/Kneser-Ney, bigram features, validation tuning\",\n",
    "    \"Large Dataset (>10K)\": \"Modified Kneser-Ney, higher-order n-grams, efficiency features\",\n",
    "    \"Real-time Application\": \"Streaming models, bounded vocabulary, fast scoring\",\n",
    "    \"High Accuracy Needed\": \"Ensemble methods, extensive hyperparameter tuning\",\n",
    "    \"Interpretability Important\": \"Bayesian smoothing, word-level analysis\",\n",
    "    \"Memory Constrained\": \"Compressed representations, sparse models\",\n",
    "}\n",
    "\n",
    "for scenario, recommendation in decision_matrix.items():\n",
    "    print(f\"{scenario:<25}: {recommendation}\")\n",
    "print()\n",
    "\n",
    "print(\"📚 FURTHER LEARNING:\")\n",
    "print(\"=\" * 20)\n",
    "print(\"• Experiment with different smoothing methods on your data\")\n",
    "print(\"• Try combining FreqProb with modern neural approaches\")\n",
    "print(\"• Explore domain adaptation techniques\")\n",
    "print(\"• Study advanced n-gram models (e.g., class-based, factored)\")\n",
    "print(\"• Investigate hybrid statistical-neural architectures\")\n",
    "print()\n",
    "\n",
    "print(\"✨ FINAL THOUGHTS:\")\n",
    "print(\"=\" * 18)\n",
    "print(\"FreqProb provides a solid foundation for probability-based NLP tasks.\")\n",
    "print(\"While deep learning dominates many applications, classical smoothing\")\n",
    "print(\"methods remain valuable for:\")\n",
    "print(\"  • Small data scenarios\")\n",
    "print(\"  • Interpretable models\")\n",
    "print(\"  • Baseline comparisons\")\n",
    "print(\"  • Hybrid architectures\")\n",
    "print(\"  • Resource-constrained environments\")\n",
    "print()\n",
    "print(\"Master these fundamentals, and you'll have a strong foundation\")\n",
    "print(\"for tackling any NLP challenge! 🎉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've completed the comprehensive FreqProb tutorial series covering:\n",
    "\n",
    "### Tutorial Series Summary:\n",
    "\n",
    "1. **Tutorial 1: Basic Smoothing Methods** - Foundation concepts and simple techniques\n",
    "2. **Tutorial 2: Advanced Smoothing Methods** - State-of-the-art techniques for complex scenarios  \n",
    "3. **Tutorial 3: Efficiency and Memory Management** - Scaling to production environments\n",
    "4. **Tutorial 4: Real-World Applications** - Practical NLP implementations\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "- **Core Concepts**: Probability estimation, smoothing, zero probability problem\n",
    "- **Methods**: From simple Laplace to advanced Kneser-Ney smoothing\n",
    "- **Optimization**: Vectorization, caching, lazy evaluation, memory management\n",
    "- **Applications**: Language modeling, classification, retrieval, sentiment analysis\n",
    "- **Best Practices**: Model selection, evaluation, scaling, debugging\n",
    "\n",
    "### Your Next Steps:\n",
    "\n",
    "1. **Apply to Your Projects** - Use FreqProb techniques in your own NLP tasks\n",
    "2. **Experiment** - Try different combinations of methods and parameters\n",
    "3. **Benchmark** - Compare with other approaches on your specific data\n",
    "4. **Contribute** - Share improvements and new applications with the community\n",
    "5. **Learn More** - Explore advanced topics like neural-statistical hybrids\n",
    "\n",
    "### Remember:\n",
    "\n",
    "- **Always use smoothing** in real applications\n",
    "- **Validate your approach** with proper evaluation\n",
    "- **Consider the trade-offs** between accuracy, speed, and interpretability\n",
    "- **Start simple** and add complexity as needed\n",
    "- **Monitor performance** in production environments\n",
    "\n",
    "Happy modeling! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
