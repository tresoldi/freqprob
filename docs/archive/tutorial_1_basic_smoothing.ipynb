{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqProb Tutorial 1: Basic Smoothing Methods\n",
    "\n",
    "This tutorial introduces the fundamental smoothing methods in FreqProb. You'll learn how to:\n",
    "\n",
    "1. Create frequency distributions from text data\n",
    "2. Apply different smoothing techniques\n",
    "3. Compare model performance\n",
    "4. Handle unseen elements\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import freqprob\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"FreqProb version: {freqprob.__version__ if hasattr(freqprob, '__version__') else 'dev'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Frequency Distribution\n",
    "\n",
    "Let's start with a simple text corpus and create a frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\",\n",
    "    \"a cat and a dog played in the garden\",\n",
    "    \"the cat likes the warm sun\",\n",
    "    \"dogs and cats are pets\",\n",
    "]\n",
    "\n",
    "# Create word frequency distribution\n",
    "all_words = []\n",
    "for sentence in corpus:\n",
    "    all_words.extend(sentence.split())\n",
    "\n",
    "freqdist = Counter(all_words)\n",
    "print(\"Frequency Distribution:\")\n",
    "for word, count in freqdist.most_common():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal words: {sum(freqdist.values())}\")\n",
    "print(f\"Unique words: {len(freqdist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Let's start with the simplest approach - Maximum Likelihood Estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLE model\n",
    "mle = freqprob.MLE(freqdist, logprob=False)\n",
    "\n",
    "print(\"MLE Probabilities:\")\n",
    "test_words = [\"the\", \"cat\", \"dog\", \"garden\", \"unknown_word\"]\n",
    "for word in test_words:\n",
    "    prob = mle(word)\n",
    "    print(f\"P({word}) = {prob:.4f}\")\n",
    "\n",
    "# Visualize the distribution\n",
    "words = list(freqdist.keys())\n",
    "probs = [mle(word) for word in words]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words, probs)\n",
    "plt.title(\"MLE Probability Distribution\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nProbability sum: {sum(probs):.4f}\")\n",
    "print(f\"Problem: P(unknown_word) = {mle('unknown_word'):.4f} (zero probability!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing (Add-One)\n",
    "\n",
    "Laplace smoothing solves the zero probability problem by adding 1 to all counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Laplace smoothing model\n",
    "# We need to specify the number of possible words (bins)\n",
    "vocabulary_size = 1000  # Assume a vocabulary of 1000 possible words\n",
    "laplace = freqprob.Laplace(freqdist, bins=vocabulary_size, logprob=False)\n",
    "\n",
    "print(\"Laplace Smoothing Probabilities:\")\n",
    "for word in test_words:\n",
    "    prob = laplace(word)\n",
    "    print(f\"P({word}) = {prob:.6f}\")\n",
    "\n",
    "# Compare MLE vs Laplace\n",
    "observed_words = list(freqdist.keys())\n",
    "mle_probs = [mle(word) for word in observed_words]\n",
    "laplace_probs = [laplace(word) for word in observed_words]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(observed_words))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width / 2, mle_probs, width, label=\"MLE\", alpha=0.8)\n",
    "plt.bar(x + width / 2, laplace_probs, width, label=\"Laplace\", alpha=0.8)\n",
    "\n",
    "plt.title(\"MLE vs Laplace Smoothing\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(x, observed_words, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUnseen word probability (Laplace): {laplace('unknown_word'):.6f}\")\n",
    "print(f\"Unseen word probability (MLE): {mle('unknown_word'):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidstone Smoothing (Add-k)\n",
    "\n",
    "Lidstone smoothing is a generalization of Laplace smoothing where we can adjust the smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different gamma values for Lidstone smoothing\n",
    "gamma_list = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "common_word = \"the\"\n",
    "unseen_word = \"unknown_word\"\n",
    "\n",
    "# Create Lidstone models with different gamma values\n",
    "lidstone_models = {}\n",
    "common_probs = []\n",
    "unseen_probs = []\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    model = freqprob.Lidstone(freqdist, gamma=gamma, bins=vocabulary_size, logprob=False)\n",
    "    lidstone_models[gamma] = model\n",
    "\n",
    "    # Track probabilities for plotting\n",
    "    common_probs.append(model(common_word))\n",
    "    unseen_probs.append(model(unseen_word))\n",
    "\n",
    "print(\"Lidstone Smoothing with Different Gamma Values:\")\n",
    "for gamma in gamma_list:\n",
    "    model = lidstone_models[gamma]\n",
    "    print(\n",
    "        f\"gamma = {gamma}: P({common_word}) = {model(common_word):.6f}, P({unseen_word}) = {model(unseen_word):.6f}\"\n",
    "    )\n",
    "\n",
    "# Plot the effect of gamma on probabilities\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(gamma_list, common_probs, \"o-\", linewidth=2, markersize=8)\n",
    "plt.title(f'Effect of gamma on P(\"{common_word}\")')\n",
    "plt.xlabel(\"Gamma (gamma)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(gamma_list, unseen_probs, \"o-\", linewidth=2, markersize=8, color=\"orange\")\n",
    "plt.title(f'Effect of gamma on P(\"{unseen_word}\")')\n",
    "plt.xlabel(\"Gamma (gamma)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: As gamma increases, probabilities become more uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Likelihood Estimation (ELE)\n",
    "\n",
    "ELE is a special case of Lidstone smoothing with Î³ = 0.5, which has theoretical justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ELE model\n",
    "ele = freqprob.ELE(freqdist, bins=vocabulary_size, logprob=False)\n",
    "\n",
    "print(\"Expected Likelihood Estimation (gamma = 0.5):\")\n",
    "for word in test_words:\n",
    "    prob = ele(word)\n",
    "    print(f\"P({word}) = {prob:.6f}\")\n",
    "\n",
    "# Compare all methods\n",
    "methods = {\"MLE\": mle, \"Laplace\": laplace, \"ELE\": ele, \"Lidstone (gamma=0.1)\": lidstone_models[0.1]}\n",
    "\n",
    "comparison_words = [\"the\", \"cat\", \"garden\", \"unknown_word\"]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "method_names = list(methods.keys())\n",
    "word_positions = np.arange(len(comparison_words))\n",
    "bar_width = 0.2\n",
    "\n",
    "for i, (method_name, model) in enumerate(methods.items()):\n",
    "    probs = [model(word) for word in comparison_words]\n",
    "    offset = (i - len(methods) / 2 + 0.5) * bar_width\n",
    "    plt.bar(word_positions + offset, probs, bar_width, label=method_name, alpha=0.8)\n",
    "\n",
    "plt.title(\"Comparison of Basic Smoothing Methods\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(word_positions, comparison_words)\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")  # Log scale to see differences better\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with Perplexity\n",
    "\n",
    "Let's evaluate our models using perplexity on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set\n",
    "test_corpus = [\"the cat sleeps on the sofa\", \"a dog barks in the yard\", \"cats and dogs are friends\"]\n",
    "\n",
    "test_words = []\n",
    "for sentence in test_corpus:\n",
    "    test_words.extend(sentence.split())\n",
    "\n",
    "print(f\"Test set: {test_words}\")\n",
    "print(f\"Test set size: {len(test_words)} words\")\n",
    "\n",
    "# Evaluate models using perplexity\n",
    "print(\"\\nModel Evaluation (Perplexity):\")\n",
    "print(\"Lower perplexity = better model\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "perplexities = {}\n",
    "for method_name, model in methods.items():\n",
    "    # Convert to log probabilities for perplexity calculation\n",
    "    if hasattr(model, \"logprob\") and not model.logprob:\n",
    "        # Create log version\n",
    "        if method_name == \"MLE\":\n",
    "            log_model = freqprob.MLE(freqdist, logprob=True)\n",
    "        elif method_name == \"Laplace\":\n",
    "            log_model = freqprob.Laplace(freqdist, bins=vocabulary_size, logprob=True)\n",
    "        elif method_name == \"ELE\":\n",
    "            log_model = freqprob.ELE(freqdist, bins=vocabulary_size, logprob=True)\n",
    "        elif \"Lidstone\" in method_name:\n",
    "            log_model = freqprob.Lidstone(freqdist, gamma=0.1, bins=vocabulary_size, logprob=True)\n",
    "\n",
    "        try:\n",
    "            pp = freqprob.perplexity(log_model, test_words)\n",
    "            perplexities[method_name] = pp\n",
    "            print(f\"{method_name:<15}: {pp:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{method_name:<15}: Error - {e!s}\")\n",
    "\n",
    "# Visualize perplexity comparison\n",
    "if perplexities:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    methods_list = list(perplexities.keys())\n",
    "    pp_values = list(perplexities.values())\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(methods_list)))\n",
    "    bars = plt.bar(methods_list, pp_values, color=colors, alpha=0.8)\n",
    "\n",
    "    plt.title(\"Model Comparison: Perplexity on Test Set\")\n",
    "    plt.xlabel(\"Smoothing Method\")\n",
    "    plt.ylabel(\"Perplexity (lower is better)\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, pp_values, strict=False):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.1,\n",
    "            f\"{value:.1f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_method = min(perplexities.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nBest performing method: {best_method[0]} (PP = {best_method[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Trade-offs\n",
    "\n",
    "Let's visualize how different smoothing methods handle the trade-off between fitting training data and generalizing to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the probability mass allocated to unseen events\n",
    "total_prob_mass = {}\n",
    "unseen_prob_mass = {}\n",
    "\n",
    "# Calculate total probability mass for observed words\n",
    "observed_words = list(freqdist.keys())\n",
    "\n",
    "for method_name, model in methods.items():\n",
    "    observed_mass = sum(model(word) for word in observed_words)\n",
    "    unseen_mass = model(\"unseen_word_example\")  # Probability for any unseen word\n",
    "\n",
    "    total_prob_mass[method_name] = observed_mass\n",
    "    unseen_prob_mass[method_name] = unseen_mass\n",
    "\n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  Probability mass for observed words: {observed_mass:.4f}\")\n",
    "    print(f\"  Probability for unseen words: {unseen_mass:.6f}\")\n",
    "    print(f\"  Reserved mass for unseen events: {1 - observed_mass:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize the trade-off\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Probability mass for observed vs unseen\n",
    "methods_list = list(total_prob_mass.keys())\n",
    "observed_masses = [total_prob_mass[m] for m in methods_list]\n",
    "unseen_masses = [1 - total_prob_mass[m] for m in methods_list]\n",
    "\n",
    "x = np.arange(len(methods_list))\n",
    "ax1.bar(x, observed_masses, label=\"Observed words\", alpha=0.8)\n",
    "ax1.bar(x, unseen_masses, bottom=observed_masses, label=\"Reserved for unseen\", alpha=0.8)\n",
    "\n",
    "ax1.set_title(\"Probability Mass Distribution\")\n",
    "ax1.set_xlabel(\"Smoothing Method\")\n",
    "ax1.set_ylabel(\"Probability Mass\")\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods_list, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Individual unseen word probabilities\n",
    "unseen_probs = [unseen_prob_mass[m] for m in methods_list]\n",
    "bars = ax2.bar(x, unseen_probs, alpha=0.8, color=\"orange\")\n",
    "\n",
    "ax2.set_title(\"Probability for Individual Unseen Words\")\n",
    "ax2.set_xlabel(\"Smoothing Method\")\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods_list, rotation=45)\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, unseen_probs, strict=False):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() * 1.5,\n",
    "        f\"{value:.2e}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        rotation=45,\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"- MLE gives zero probability to unseen words (problematic)\")\n",
    "print(\"- Smoothing methods allocate probability mass to unseen events\")\n",
    "print(\"- Higher smoothing â more probability reserved for unseen words\")\n",
    "print(\"- Trade-off: fitting training data vs. handling unseen data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips and Conclusions\n",
    "\n",
    "Let's summarize what we've learned and provide practical guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "print(\"1. NEVER use pure MLE for real applications\")\n",
    "print(\"   â Zero probabilities break many algorithms\")\n",
    "print()\n",
    "\n",
    "print(\"2. Laplace smoothing (add-one) is a good baseline\")\n",
    "print(\"   â Simple, robust, works well for small datasets\")\n",
    "print()\n",
    "\n",
    "print(\"3. Use ELE (gamma=0.5) for theoretically motivated smoothing\")\n",
    "print(\"   â Good balance between smoothing and data fidelity\")\n",
    "print()\n",
    "\n",
    "print(\"4. Tune Lidstone gamma parameter using validation data\")\n",
    "print(\"   â Cross-validation to find optimal smoothing strength\")\n",
    "print()\n",
    "\n",
    "print(\"5. Consider vocabulary size when setting 'bins' parameter\")\n",
    "print(\"   â Underestimating leads to over-smoothing\")\n",
    "print(\"   â Overestimating leads to under-smoothing\")\n",
    "print()\n",
    "\n",
    "# Demonstrate vocabulary size effect\n",
    "print(\"VOCABULARY SIZE EFFECT DEMONSTRATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "vocab_sizes = [100, 1000, 10000]\n",
    "for vocab_size in vocab_sizes:\n",
    "    laplace_model = freqprob.Laplace(freqdist, bins=vocab_size, logprob=False)\n",
    "    unseen_prob = laplace_model(\"unseen_word\")\n",
    "    print(f\"Vocabulary size {vocab_size:5d}: P(unseen) = {unseen_prob:.6f}\")\n",
    "\n",
    "print(\"\\nAs vocabulary size increases, unseen word probability decreases\")\n",
    "print(\"This affects the smoothing strength!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try It Yourself!\n",
    "\n",
    "Now it's your turn to experiment with the concepts we've covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 1: Create your own text corpus and compare smoothing methods\n",
    "# TODO: Replace this with your own text data\n",
    "your_corpus = [\n",
    "    \"machine learning is fascinating\",\n",
    "    \"deep learning models are powerful\",\n",
    "    \"natural language processing uses machine learning\",\n",
    "    \"artificial intelligence and machine learning overlap\",\n",
    "    \"learning algorithms improve with data\",\n",
    "]\n",
    "\n",
    "# Create frequency distribution\n",
    "your_words = []\n",
    "for sentence in your_corpus:\n",
    "    your_words.extend(sentence.split())\n",
    "\n",
    "your_freqdist = Counter(your_words)\n",
    "print(\"Your frequency distribution:\")\n",
    "print(your_freqdist)\n",
    "\n",
    "# TODO: Create different smoothing models and compare them\n",
    "# Hint: Use the code patterns from above\n",
    "\n",
    "# EXERCISE 2: Find the optimal gamma for Lidstone smoothing\n",
    "# TODO: Create a validation set and test different gamma values\n",
    "# Hint: Use perplexity to evaluate performance\n",
    "\n",
    "# EXERCISE 3: Analyze the effect of different vocabulary size estimates\n",
    "# TODO: Try bins=[100, 500, 1000, 5000] and see how it affects smoothing\n",
    "\n",
    "print(\"\\nComplete the exercises above to deepen your understanding!\")\n",
    "print(\"Experiment with different:\")\n",
    "print(\"- Text corpora (different domains, sizes)\")\n",
    "print(\"- Smoothing parameters (gamma values)\")\n",
    "print(\"- Vocabulary size estimates\")\n",
    "print(\"- Evaluation metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **The zero probability problem** and why pure MLE fails\n",
    "2. **Laplace smoothing** as a simple solution (add-one)\n",
    "3. **Lidstone smoothing** for tunable smoothing strength\n",
    "4. **Expected Likelihood Estimation** as a theoretically motivated choice\n",
    "5. **Model evaluation** using perplexity\n",
    "6. **Trade-offs** between fitting training data and generalizing to unseen data\n",
    "7. **Practical considerations** for real-world applications\n",
    "\n",
    "**Next Steps:**\n",
    "- Try Tutorial 2: Advanced Smoothing Methods (Kneser-Ney, Simple Good-Turing)\n",
    "- Try Tutorial 3: Computational Efficiency and Memory Management\n",
    "- Try Tutorial 4: Real-world NLP Applications\n",
    "\n",
    "**Key Takeaway:** Always use some form of smoothing in real applications. The choice of method depends on your data size, domain, and performance requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
