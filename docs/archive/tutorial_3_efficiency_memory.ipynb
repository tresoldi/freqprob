{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqProb Tutorial 3: Computational Efficiency and Memory Management\n",
    "\n",
    "This tutorial demonstrates the efficiency and memory management features of FreqProb for large-scale applications:\n",
    "\n",
    "1. **Vectorized Operations** - Batch processing for performance\n",
    "2. **Caching and Memoization** - Avoiding redundant computations\n",
    "3. **Lazy Evaluation** - Computing only what's needed\n",
    "4. **Streaming Updates** - Real-time incremental learning\n",
    "5. **Memory-Efficient Representations** - Handling large vocabularies\n",
    "6. **Memory Profiling** - Monitoring and optimization\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "\n",
    "import freqprob\n",
    "\n",
    "# Import efficiency features\n",
    "from freqprob import (\n",
    "    BatchScorer,\n",
    "    DistributionMemoryAnalyzer,\n",
    "    MemoryProfiler,\n",
    "    StreamingLaplace,\n",
    "    StreamingMLE,\n",
    "    VectorizedScorer,\n",
    "    create_compressed_distribution,\n",
    "    create_lazy_mle,\n",
    "    create_sparse_distribution,\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"FreqProb Efficiency and Memory Management Tutorial\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Current memory usage: {psutil.Process().memory_info().rss / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Large-Scale Data\n",
    "\n",
    "First, let's create realistic large-scale datasets to demonstrate efficiency features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realistic_frequency_distribution(vocab_size, total_count, zipf_exponent=1.2):\n",
    "    \"\"\"Create a realistic frequency distribution following Zipf's law.\"\"\"\n",
    "    # Generate Zipfian frequencies\n",
    "    frequencies = np.random.zipf(zipf_exponent, vocab_size)\n",
    "\n",
    "    # Normalize to desired total count\n",
    "    frequencies = (frequencies / frequencies.sum()) * total_count\n",
    "    frequencies = frequencies.astype(int)\n",
    "    frequencies[frequencies == 0] = 1  # Ensure no zero counts\n",
    "\n",
    "    # Create word-like keys\n",
    "    words = [f\"word_{i:06d}\" for i in range(vocab_size)]\n",
    "\n",
    "    return dict(zip(words, frequencies, strict=False))\n",
    "\n",
    "\n",
    "# Create datasets of different sizes\n",
    "print(\"Creating realistic datasets...\")\n",
    "datasets = {\n",
    "    \"small\": create_realistic_frequency_distribution(1000, 10000),\n",
    "    \"medium\": create_realistic_frequency_distribution(10000, 100000),\n",
    "    \"large\": create_realistic_frequency_distribution(50000, 500000),\n",
    "}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(\n",
    "        f\"{name.capitalize()} dataset: {len(dataset):,} words, {sum(dataset.values()):,} total count\"\n",
    "    )\n",
    "\n",
    "    # Show frequency distribution characteristics\n",
    "    counts = list(dataset.values())\n",
    "    print(f\"  Frequency range: {min(counts)} - {max(counts):,}\")\n",
    "    print(\n",
    "        f\"  Top 10 account for {sum(sorted(counts, reverse=True)[:10]) / sum(counts) * 100:.1f}% of data\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "# Visualize the Zipfian distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    counts = sorted(dataset.values(), reverse=True)\n",
    "    ranks = np.arange(1, len(counts) + 1)\n",
    "\n",
    "    plt.loglog(ranks, counts, \"o-\", alpha=0.7, markersize=2)\n",
    "    plt.title(f\"{name.capitalize()} Dataset\\n(Zipf Distribution)\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Operations for Batch Processing\n",
    "\n",
    "Vectorized operations allow efficient batch processing of multiple elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for comparison\n",
    "medium_dataset = datasets[\"medium\"]\n",
    "mle_model = freqprob.MLE(medium_dataset, logprob=False)\n",
    "laplace_model = freqprob.Laplace(medium_dataset, bins=20000, logprob=False)\n",
    "\n",
    "print(\"VECTORIZED OPERATIONS DEMONSTRATION\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "\n",
    "# Create vectorized scorer\n",
    "vectorized_mle = VectorizedScorer(mle_model)\n",
    "\n",
    "# Prepare test data\n",
    "test_elements = list(medium_dataset.keys())[:1000]  # First 1000 words\n",
    "print(f\"Testing with {len(test_elements)} elements...\")\n",
    "\n",
    "# Compare individual vs batch scoring\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Individual scoring (traditional approach)\n",
    "start_time = time.time()\n",
    "individual_scores = [mle_model(element) for element in test_elements]\n",
    "individual_time = time.time() - start_time\n",
    "\n",
    "# Batch scoring (vectorized approach)\n",
    "start_time = time.time()\n",
    "batch_scores = vectorized_mle.score_batch(test_elements)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"Individual scoring: {individual_time:.4f} seconds\")\n",
    "print(f\"Batch scoring:      {batch_time:.4f} seconds\")\n",
    "print(f\"Speedup:           {individual_time / batch_time:.1f}x\")\n",
    "\n",
    "# Verify results are equivalent\n",
    "max_diff = np.max(np.abs(np.array(individual_scores) - batch_scores))\n",
    "print(f\"Maximum difference: {max_diff:.2e} (should be ~0)\")\n",
    "\n",
    "# Demonstrate matrix operations\n",
    "print(\"\\nMatrix Operations:\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "# Create 2D array of elements\n",
    "elements_2d = [test_elements[:10], test_elements[10:20]]  # 2 rows, 10 columns\n",
    "score_matrix = vectorized_mle.score_matrix(elements_2d)\n",
    "\n",
    "print(f\"Score matrix shape: {score_matrix.shape}\")\n",
    "print(\"Sample scores:\")\n",
    "print(score_matrix[:2, :5])  # Show first 2 rows, 5 columns\n",
    "\n",
    "# Top-k most probable elements\n",
    "print(\"\\nTop-K Operations:\")\n",
    "print(\"-\" * 17)\n",
    "\n",
    "top_elements, top_scores = vectorized_mle.top_k_elements(10)\n",
    "print(\"Top 10 most probable elements:\")\n",
    "for i, (element, score) in enumerate(zip(top_elements, top_scores, strict=False)):\n",
    "    count = medium_dataset[element]\n",
    "    print(f\"{i + 1:2d}. {element} (count={count:,}, prob={score:.6f})\")\n",
    "\n",
    "# Score normalization - import the function from vectorized module\n",
    "print(\"\\nScore Normalization:\")\n",
    "print(\"-\" * 19)\n",
    "\n",
    "sample_scores = batch_scores[:20]\n",
    "# Import the normalize_scores function from vectorized module\n",
    "from freqprob.vectorized import normalize_scores\n",
    "\n",
    "normalized_scores = normalize_scores(sample_scores, method=\"softmax\")\n",
    "\n",
    "print(f\"Original scores range:    [{np.min(sample_scores):.6f}, {np.max(sample_scores):.6f}]\")\n",
    "print(\n",
    "    f\"Normalized scores range:  [{np.min(normalized_scores):.6f}, {np.max(normalized_scores):.6f}]\"\n",
    ")\n",
    "print(f\"Normalized scores sum:    {np.sum(normalized_scores):.6f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Scoring with Multiple Methods\n",
    "\n",
    "BatchScorer allows comparing multiple models efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BATCH SCORING WITH MULTIPLE METHODS\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "\n",
    "# Create multiple scorers\n",
    "scorers = {\n",
    "    \"mle\": freqprob.MLE(medium_dataset, logprob=False),\n",
    "    \"laplace\": freqprob.Laplace(medium_dataset, bins=20000, logprob=False),\n",
    "    \"ele\": freqprob.ELE(medium_dataset, bins=20000, logprob=False),\n",
    "}\n",
    "\n",
    "# Create batch scorer\n",
    "batch_scorer = BatchScorer(scorers)\n",
    "\n",
    "# Test elements (mix of frequent, rare, and unseen)\n",
    "test_mix = (\n",
    "    list(medium_dataset.keys())[:5]\n",
    "    + list(medium_dataset.keys())[-5:]  # Frequent words\n",
    "    + [\"unseen_word_1\", \"unseen_word_2\"]  # Rare words  # Unseen words\n",
    ")\n",
    "\n",
    "print(f\"Testing {len(test_mix)} elements with {len(scorers)} methods...\")\n",
    "\n",
    "# Batch score all methods\n",
    "start_time = time.time()\n",
    "batch_results = batch_scorer.score_batch(test_mix)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"Batch scoring completed in {batch_time:.4f} seconds\")\n",
    "print()\n",
    "\n",
    "# Display results\n",
    "print(\"Results by method:\")\n",
    "print(\"-\" * 18)\n",
    "for method_name, scores in batch_results.items():\n",
    "    print(f\"{method_name.upper()}:\")\n",
    "    for i, (element, score) in enumerate(zip(test_mix, scores, strict=False)):\n",
    "        count = medium_dataset.get(element, 0)\n",
    "        element_type = \"frequent\" if i < 5 else \"rare\" if i < 10 else \"unseen\"\n",
    "        print(f\"  {element:<15} ({element_type:<8}, c={count:>3}): {score:.6f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize method comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for plotting\n",
    "methods = list(batch_results.keys())\n",
    "element_types = [\"Frequent\"] * 5 + [\"Rare\"] * 5 + [\"Unseen\"] * 2\n",
    "colors = {\"Frequent\": \"green\", \"Rare\": \"orange\", \"Unseen\": \"red\"}\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    scores = batch_results[method]\n",
    "\n",
    "    # Color bars by element type\n",
    "    bar_colors = [colors[t] for t in element_types]\n",
    "    bars = plt.bar(range(len(test_mix)), scores, color=bar_colors, alpha=0.7)\n",
    "\n",
    "    plt.title(f\"{method.upper()} Scores\")\n",
    "    plt.xlabel(\"Test Elements\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(\n",
    "        range(len(test_mix)), [e[:8] + \"...\" if len(e) > 8 else e for e in test_mix], rotation=45\n",
    "    )\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "plt.subplot(2, 2, 4)\n",
    "legend_elements = [plt.Rectangle((0, 0), 1, 1, facecolor=colors[t], alpha=0.7) for t in colors]\n",
    "plt.legend(legend_elements, colors.keys(), loc=\"center\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance analysis\n",
    "print(\"Method Performance Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for method_name in methods:\n",
    "    scores = batch_results[method_name]\n",
    "    frequent_scores = scores[:5]\n",
    "    rare_scores = scores[5:10]\n",
    "    unseen_scores = scores[10:]\n",
    "\n",
    "    print(f\"{method_name.upper()}:\")\n",
    "    print(\n",
    "        f\"  Frequent words: avg = {np.mean(frequent_scores):.6f}, std = {np.std(frequent_scores):.6f}\"\n",
    "    )\n",
    "    print(f\"  Rare words:     avg = {np.mean(rare_scores):.6f}, std = {np.std(rare_scores):.6f}\")\n",
    "    print(\n",
    "        f\"  Unseen words:   avg = {np.mean(unseen_scores):.6f}, std = {np.std(unseen_scores):.6f}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and Memoization\n",
    "\n",
    "FreqProb automatically caches expensive computations like Simple Good-Turing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from functools import lru_cache\n",
    "\n",
    "print(\"CACHING AND MEMOIZATION\")\n",
    "print(\"=\" * 25)\n",
    "print()\n",
    "\n",
    "# Demonstrate SGT caching (if available)\n",
    "small_dataset = datasets[\"small\"]\n",
    "\n",
    "print(\"Simple Good-Turing Caching Demo:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "try:\n",
    "    # First creation (cold cache)\n",
    "    print(\"Creating first SGT model (cold cache)...\")\n",
    "    start_time = time.time()\n",
    "    sgt1 = freqprob.SimpleGoodTuring(small_dataset, logprob=False)\n",
    "    first_time = time.time() - start_time\n",
    "    print(f\"Time: {first_time:.4f} seconds\")\n",
    "\n",
    "    # Second creation (hot cache)\n",
    "    print(\"\\nCreating second SGT model (hot cache)...\")\n",
    "    start_time = time.time()\n",
    "    sgt2 = freqprob.SimpleGoodTuring(small_dataset, logprob=False)\n",
    "    second_time = time.time() - start_time\n",
    "    print(f\"Time: {second_time:.4f} seconds\")\n",
    "\n",
    "    print(f\"\\nSpeedup from caching: {first_time / second_time:.1f}x\")\n",
    "\n",
    "    # Verify cache is working by checking results are identical\n",
    "    test_words = list(small_dataset.keys())[:10]\n",
    "    scores1 = [sgt1(word) for word in test_words]\n",
    "    scores2 = [sgt2(word) for word in test_words]\n",
    "\n",
    "    max_diff = max(abs(s1 - s2) for s1, s2 in zip(scores1, scores2, strict=False))\n",
    "    print(f\"Maximum score difference: {max_diff:.2e} (should be 0)\")\n",
    "\n",
    "    # Cache statistics\n",
    "    cache_stats = freqprob.get_cache_stats()\n",
    "    print(\"\\nCache Statistics:\")\n",
    "    for key, value in cache_stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Clear cache and show effect\n",
    "    print(\"\\nClearing all caches...\")\n",
    "    freqprob.clear_all_caches()\n",
    "\n",
    "    print(\"Creating third SGT model (cache cleared)...\")\n",
    "    start_time = time.time()\n",
    "    sgt3 = freqprob.SimpleGoodTuring(small_dataset, logprob=False)\n",
    "    third_time = time.time() - start_time\n",
    "    print(f\"Time: {third_time:.4f} seconds (should be similar to first time)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SGT caching demo failed: {e}\")\n",
    "    print(\"This can happen with certain frequency distributions\")\n",
    "\n",
    "# Demonstrate custom caching for expensive operations\n",
    "print(\"\\n\\nCustom Caching Example:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Simulate expensive computation\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def expensive_computation(x):\n",
    "    \"\"\"Simulate expensive computation with caching.\"\"\"\n",
    "    time.sleep(0.001)  # Simulate work\n",
    "    return x**2 + np.sin(x) * 100\n",
    "\n",
    "\n",
    "# Test caching effect\n",
    "test_values = np.random.uniform(0, 10, 500)\n",
    "repeated_values = np.tile(test_values[:100], 5)  # Repeat first 100 values 5 times\n",
    "\n",
    "print(f\"Testing with {len(repeated_values)} values (many repeats)...\")\n",
    "\n",
    "# Without cache (clear cache first)\n",
    "expensive_computation.cache_clear()\n",
    "start_time = time.time()\n",
    "results_no_cache = [expensive_computation(x) for x in test_values[:100]]  # Fresh computation\n",
    "no_cache_time = time.time() - start_time\n",
    "\n",
    "# With cache (subsequent calls)\n",
    "start_time = time.time()\n",
    "results_with_cache = [expensive_computation(x) for x in repeated_values]  # Many cached hits\n",
    "with_cache_time = time.time() - start_time\n",
    "\n",
    "print(f\"Time without cache benefit: {no_cache_time:.4f} seconds\")\n",
    "print(f\"Time with cache benefit:    {with_cache_time:.4f} seconds\")\n",
    "print(f\"Speedup from caching:      {no_cache_time * 5 / with_cache_time:.1f}x\")\n",
    "\n",
    "# Cache info\n",
    "cache_info = expensive_computation.cache_info()\n",
    "print(\"\\nCache statistics:\")\n",
    "print(f\"  Hits: {cache_info.hits}\")\n",
    "print(f\"  Misses: {cache_info.misses}\")\n",
    "print(f\"  Hit rate: {cache_info.hits / (cache_info.hits + cache_info.misses) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "\n",
    "Lazy evaluation computes probabilities only when needed, saving time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freqprob import LazyBatchScorer\n",
    "\n",
    "print(\"LAZY EVALUATION\")\n",
    "print(\"=\" * 16)\n",
    "print()\n",
    "\n",
    "# Use large dataset for dramatic effect\n",
    "large_dataset = datasets[\"large\"]\n",
    "print(\n",
    "    f\"Working with dataset: {len(large_dataset):,} words, {sum(large_dataset.values()):,} total count\"\n",
    ")\n",
    "\n",
    "# Create lazy and eager models\n",
    "print(\"\\nCreating models...\")\n",
    "\n",
    "# Eager evaluation (traditional)\n",
    "start_time = time.time()\n",
    "eager_mle = freqprob.MLE(large_dataset, logprob=False)\n",
    "eager_creation_time = time.time() - start_time\n",
    "print(f\"Eager MLE creation: {eager_creation_time:.4f} seconds\")\n",
    "\n",
    "# Lazy evaluation\n",
    "start_time = time.time()\n",
    "lazy_mle = create_lazy_mle(large_dataset, logprob=False)\n",
    "lazy_creation_time = time.time() - start_time\n",
    "print(f\"Lazy MLE creation:  {lazy_creation_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Creation speedup: {eager_creation_time / lazy_creation_time:.1f}x\")\n",
    "\n",
    "# Test queries on subset of elements\n",
    "test_subset = list(large_dataset.keys())[::1000]  # Every 1000th element\n",
    "print(f\"\\nQuerying {len(test_subset)} elements...\")\n",
    "\n",
    "# Eager model queries\n",
    "start_time = time.time()\n",
    "eager_scores = [eager_mle(word) for word in test_subset]\n",
    "eager_query_time = time.time() - start_time\n",
    "\n",
    "# Lazy model queries (first time)\n",
    "start_time = time.time()\n",
    "lazy_scores = [lazy_mle(word) for word in test_subset]\n",
    "lazy_query_time = time.time() - start_time\n",
    "\n",
    "print(f\"Eager query time: {eager_query_time:.4f} seconds\")\n",
    "print(f\"Lazy query time:  {lazy_query_time:.4f} seconds\")\n",
    "\n",
    "# Verify results are identical\n",
    "max_diff = max(\n",
    "    abs(eager_score - lazy_score)\n",
    "    for eager_score, lazy_score in zip(eager_scores, lazy_scores, strict=False)\n",
    ")\n",
    "print(f\"Maximum difference: {max_diff:.2e}\")\n",
    "\n",
    "# Show lazy evaluation statistics\n",
    "computed_elements = lazy_mle.get_computed_elements()\n",
    "total_elements = len(large_dataset)\n",
    "computation_ratio = len(computed_elements) / total_elements\n",
    "\n",
    "print(\"\\nLazy Evaluation Statistics:\")\n",
    "print(f\"Total elements:    {total_elements:,}\")\n",
    "print(f\"Computed elements: {len(computed_elements):,}\")\n",
    "print(f\"Computation ratio: {computation_ratio:.1%}\")\n",
    "print(f\"Memory savings:    ~{(1 - computation_ratio) * 100:.1f}%\")\n",
    "\n",
    "# Demonstrate repeated queries (cached)\n",
    "print(\"\\nRepeated queries (should be cached):\")\n",
    "start_time = time.time()\n",
    "lazy_scores_repeat = [lazy_mle(word) for word in test_subset]\n",
    "lazy_repeat_time = time.time() - start_time\n",
    "\n",
    "print(f\"Lazy repeat time: {lazy_repeat_time:.4f} seconds\")\n",
    "print(f\"Speedup vs first: {lazy_query_time / lazy_repeat_time:.1f}x\")\n",
    "\n",
    "# Batch lazy evaluation\n",
    "print(\"\\nBatch Lazy Evaluation:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Create lazy batch scorer - use the lazy_mle we already created\n",
    "lazy_batch = LazyBatchScorer(lazy_mle)\n",
    "\n",
    "# Test batch processing\n",
    "batch_test_elements = list(large_dataset.keys())[::2000]  # Every 2000th element\n",
    "print(f\"Batch processing {len(batch_test_elements)} elements...\")\n",
    "\n",
    "start_time = time.time()\n",
    "batch_scores = lazy_batch.score_batch(batch_test_elements)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"Batch processing time: {batch_time:.4f} seconds\")\n",
    "print(f\"Average per element:   {batch_time / len(batch_test_elements) * 1000:.2f} ms\")\n",
    "\n",
    "# Demonstrate streaming evaluation\n",
    "print(\"\\nStreaming Evaluation:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Create a stream of elements (some repeated)\n",
    "stream_elements = batch_test_elements[:5] * 3  # Repeat first 5 elements 3 times\n",
    "print(f\"Processing stream of {len(stream_elements)} elements (with repetitions)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "streaming_scores = list(lazy_batch.score_streaming(stream_elements))\n",
    "streaming_time = time.time() - start_time\n",
    "\n",
    "print(f\"Streaming processing time: {streaming_time:.4f} seconds\")\n",
    "print(f\"Average per element:       {streaming_time / len(stream_elements) * 1000:.2f} ms\")\n",
    "\n",
    "# Memory usage comparison (simplified approach)\n",
    "print(\"\\nMemory Usage Comparison:\")\n",
    "print(\"-\" * 27)\n",
    "\n",
    "# Get current process memory for comparison\n",
    "import psutil\n",
    "\n",
    "current_process = psutil.Process()\n",
    "current_memory = current_process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Estimate memory usage based on computed elements\n",
    "eager_total_elements = len(large_dataset)\n",
    "lazy_computed_elements = len(computed_elements)\n",
    "\n",
    "# Estimated memory usage (simplified calculation)\n",
    "estimated_eager_memory = eager_total_elements * 0.1  # Assume ~0.1KB per element\n",
    "estimated_lazy_memory = lazy_computed_elements * 0.1  # Only computed elements\n",
    "\n",
    "print(f\"Current process memory:     {current_memory:.1f} MB\")\n",
    "print(\n",
    "    f\"Estimated eager model:      {estimated_eager_memory / 1024:.1f} MB (all {eager_total_elements:,} elements)\"\n",
    ")\n",
    "print(\n",
    "    f\"Estimated lazy model:       {estimated_lazy_memory / 1024:.1f} MB (only {lazy_computed_elements:,} computed)\"\n",
    ")\n",
    "print(f\"Computational efficiency:   {computation_ratio:.1%} of full computation\")\n",
    "print(f\"Memory efficiency:          ~{(1 - computation_ratio) * 100:.1f}% savings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Updates and Real-time Learning\n",
    "\n",
    "Streaming models allow incremental updates for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STREAMING UPDATES AND REAL-TIME LEARNING\")\n",
    "print(\"=\" * 42)\n",
    "print()\n",
    "\n",
    "# Initialize streaming models\n",
    "initial_data = {f\"word_{i}\": max(1, 100 - i) for i in range(50)}  # Small initial vocabulary\n",
    "\n",
    "streaming_mle = StreamingMLE(initial_data, max_vocabulary_size=100, logprob=False)\n",
    "streaming_laplace = StreamingLaplace(initial_data, max_vocabulary_size=100, logprob=False)\n",
    "\n",
    "print(f\"Initial vocabulary size: {len(initial_data)}\")\n",
    "print(\"Maximum vocabulary size: 100\")\n",
    "print()\n",
    "\n",
    "# Simulate streaming data\n",
    "\n",
    "\n",
    "def generate_streaming_data(n_updates=500):\n",
    "    \"\"\"Generate streaming data with evolving vocabulary.\"\"\"\n",
    "    stream = []\n",
    "\n",
    "    # Mix of existing and new words\n",
    "    existing_words = list(initial_data.keys())\n",
    "\n",
    "    for i in range(n_updates):\n",
    "        word = random.choice(existing_words) if random.random() < 0.7 else f\"new_word_{i}\"\n",
    "        count = random.randint(1, 5)\n",
    "        stream.append((word, count))\n",
    "\n",
    "    return stream\n",
    "\n",
    "\n",
    "streaming_data = generate_streaming_data(500)\n",
    "print(f\"Generated {len(streaming_data)} streaming updates\")\n",
    "\n",
    "# Process streaming data and track statistics\n",
    "print(\"\\nProcessing streaming data...\")\n",
    "\n",
    "vocab_sizes = []\n",
    "update_times = []\n",
    "probabilities_tracked = {\"word_0\": [], \"word_10\": [], \"new_word_100\": []}\n",
    "\n",
    "for i, (word, count) in enumerate(streaming_data):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Update both models\n",
    "    streaming_mle.update_single(word, count)\n",
    "    streaming_laplace.update_single(word, count)\n",
    "\n",
    "    update_time = time.time() - start_time\n",
    "    update_times.append(update_time * 1000)  # Convert to milliseconds\n",
    "\n",
    "    # Track statistics every 50 updates\n",
    "    if i % 50 == 0:\n",
    "        stats = streaming_mle.get_streaming_statistics()\n",
    "        vocab_sizes.append(stats[\"vocabulary_size\"])\n",
    "\n",
    "        # Track specific word probabilities\n",
    "        for tracked_word in probabilities_tracked:\n",
    "            prob = streaming_mle(tracked_word)\n",
    "            probabilities_tracked[tracked_word].append(prob)\n",
    "\n",
    "print(f\"Average update time: {np.mean(update_times):.3f} ms\")\n",
    "print(f\"Maximum update time: {np.max(update_times):.3f} ms\")\n",
    "\n",
    "# Final statistics\n",
    "final_stats = streaming_mle.get_streaming_statistics()\n",
    "print(\"\\nFinal Statistics:\")\n",
    "print(f\"  Vocabulary size: {final_stats['vocabulary_size']}\")\n",
    "print(f\"  Total count: {final_stats['total_count']:,}\")\n",
    "print(f\"  Update count: {final_stats['update_count']}\")\n",
    "print(f\"  Most frequent: {final_stats['most_frequent']}\")\n",
    "\n",
    "# Visualize streaming behavior\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Vocabulary size over time\n",
    "axes[0, 0].plot(range(0, len(streaming_data), 50), vocab_sizes, \"b-\", linewidth=2)\n",
    "axes[0, 0].axhline(y=100, color=\"r\", linestyle=\"--\", alpha=0.7, label=\"Max vocabulary\")\n",
    "axes[0, 0].set_title(\"Vocabulary Size Over Time\")\n",
    "axes[0, 0].set_xlabel(\"Updates\")\n",
    "axes[0, 0].set_ylabel(\"Vocabulary Size\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Update time distribution\n",
    "axes[0, 1].hist(update_times, bins=30, alpha=0.7, color=\"green\")\n",
    "axes[0, 1].axvline(\n",
    "    x=np.mean(update_times),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {np.mean(update_times):.2f} ms\",\n",
    ")\n",
    "axes[0, 1].set_title(\"Update Time Distribution\")\n",
    "axes[0, 1].set_xlabel(\"Update Time (ms)\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Probability evolution\n",
    "colors = [\"blue\", \"orange\", \"green\"]\n",
    "for i, (word, probs) in enumerate(probabilities_tracked.items()):\n",
    "    if probs:  # Only plot if we have data\n",
    "        axes[1, 0].plot(\n",
    "            range(0, len(probs) * 50, 50),\n",
    "            probs,\n",
    "            color=colors[i],\n",
    "            linewidth=2,\n",
    "            label=word,\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "        )\n",
    "\n",
    "axes[1, 0].set_title(\"Probability Evolution\")\n",
    "axes[1, 0].set_xlabel(\"Updates\")\n",
    "axes[1, 0].set_ylabel(\"Probability\")\n",
    "axes[1, 0].set_yscale(\"log\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model comparison on test words\n",
    "test_words = [\"word_0\", \"word_25\", \"new_word_100\", \"unseen_word\"]\n",
    "mle_probs = [streaming_mle(word) for word in test_words]\n",
    "laplace_probs = [streaming_laplace(word) for word in test_words]\n",
    "\n",
    "x = np.arange(len(test_words))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width / 2, mle_probs, width, label=\"Streaming MLE\", alpha=0.8)\n",
    "axes[1, 1].bar(x + width / 2, laplace_probs, width, label=\"Streaming Laplace\", alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_title(\"Model Comparison After Streaming\")\n",
    "axes[1, 1].set_xlabel(\"Test Words\")\n",
    "axes[1, 1].set_ylabel(\"Probability\")\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(test_words, rotation=45)\n",
    "axes[1, 1].set_yscale(\"log\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Vocabulary size stabilizes at the maximum limit\")\n",
    "print(\"- Update times remain consistently low\")\n",
    "print(\"- Probabilities adapt to new data in real-time\")\n",
    "print(\"- Streaming Laplace provides smoother probability estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Representations\n",
    "\n",
    "For large vocabularies, compressed and sparse representations can significantly reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MEMORY-EFFICIENT REPRESENTATIONS\")\n",
    "print(\"=\" * 33)\n",
    "print()\n",
    "\n",
    "# Create large frequency distribution for demonstration\n",
    "large_freq_dist = create_realistic_frequency_distribution(25000, 250000)\n",
    "print(\n",
    "    f\"Original dataset: {len(large_freq_dist):,} words, {sum(large_freq_dist.values()):,} total count\"\n",
    ")\n",
    "\n",
    "# Memory usage of original dictionary\n",
    "original_size = sys.getsizeof(large_freq_dist)\n",
    "for k, v in large_freq_dist.items():\n",
    "    original_size += sys.getsizeof(k) + sys.getsizeof(v)\n",
    "\n",
    "print(f\"Original memory usage: {original_size / 1024 / 1024:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# Create different representations\n",
    "print(\"Creating memory-efficient representations...\")\n",
    "\n",
    "# 1. Compressed representation\n",
    "start_time = time.time()\n",
    "compressed_dist = create_compressed_distribution(large_freq_dist, quantization_levels=1024)\n",
    "compressed_time = time.time() - start_time\n",
    "compressed_memory = compressed_dist.get_memory_usage()\n",
    "\n",
    "# 2. Sparse representation\n",
    "start_time = time.time()\n",
    "sparse_dist = create_sparse_distribution(large_freq_dist)\n",
    "sparse_time = time.time() - start_time\n",
    "sparse_memory = sparse_dist.get_memory_usage()\n",
    "\n",
    "# 3. Highly compressed representation\n",
    "start_time = time.time()\n",
    "highly_compressed = create_compressed_distribution(\n",
    "    large_freq_dist, quantization_levels=256, use_compression=True\n",
    ")\n",
    "highly_compressed_time = time.time() - start_time\n",
    "highly_compressed_memory = highly_compressed.get_memory_usage()\n",
    "\n",
    "print(f\"Compressed creation time:        {compressed_time:.3f} seconds\")\n",
    "print(f\"Sparse creation time:            {sparse_time:.3f} seconds\")\n",
    "print(f\"Highly compressed creation time: {highly_compressed_time:.3f} seconds\")\n",
    "print()\n",
    "\n",
    "# Memory comparison\n",
    "print(\"Memory Usage Comparison:\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "representations = [\n",
    "    (\"Original Dictionary\", original_size),\n",
    "    (\"Compressed\", compressed_memory[\"total\"]),\n",
    "    (\"Sparse\", sparse_memory[\"total\"]),\n",
    "    (\"Highly Compressed\", highly_compressed_memory[\"total\"]),\n",
    "]\n",
    "\n",
    "for name, size in representations:\n",
    "    size_mb = size / 1024 / 1024\n",
    "    savings = (1 - size / original_size) * 100\n",
    "    print(f\"{name:<20}: {size_mb:6.1f} MB ({savings:+5.1f}% vs original)\")\n",
    "\n",
    "# Accuracy comparison (for compressed representations)\n",
    "print(\"\\nAccuracy Comparison:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "test_sample = list(large_freq_dist.items())[:100]  # Test on first 100 words\n",
    "\n",
    "# Calculate errors for compressed representations\n",
    "compressed_errors = []\n",
    "highly_compressed_errors = []\n",
    "\n",
    "for word, original_count in test_sample:\n",
    "    compressed_count = compressed_dist.get_count(word)\n",
    "    highly_compressed_count = highly_compressed.get_count(word)\n",
    "\n",
    "    compressed_error = abs(original_count - compressed_count) / original_count\n",
    "    highly_compressed_error = abs(original_count - highly_compressed_count) / original_count\n",
    "\n",
    "    compressed_errors.append(compressed_error)\n",
    "    highly_compressed_errors.append(highly_compressed_error)\n",
    "\n",
    "print(\"Compressed (1024 levels):\")\n",
    "print(f\"  Mean relative error: {np.mean(compressed_errors) * 100:.2f}%\")\n",
    "print(f\"  Max relative error:  {np.max(compressed_errors) * 100:.2f}%\")\n",
    "\n",
    "print(\"Highly compressed (256 levels):\")\n",
    "print(f\"  Mean relative error: {np.mean(highly_compressed_errors) * 100:.2f}%\")\n",
    "print(f\"  Max relative error:  {np.max(highly_compressed_errors) * 100:.2f}%\")\n",
    "\n",
    "# Sparse distribution benefits\n",
    "print(\"\\nSparse distribution benefits:\")\n",
    "print(f\"  Stored elements: {sparse_dist.get_vocabulary_size():,}\")\n",
    "print(\n",
    "    f\"  Storage efficiency: {sparse_dist.get_vocabulary_size() / len(large_freq_dist) * 100:.1f}% of original\"\n",
    ")\n",
    "\n",
    "# Demonstrate sparse operations\n",
    "top_10 = sparse_dist.get_top_k(10)\n",
    "print(f\"  Top 10 elements: {top_10[:3]}...\")  # Show first 3\n",
    "\n",
    "mid_freq_elements = sparse_dist.get_elements_with_count_range(100, 1000)\n",
    "print(f\"  Elements with count 100-1000: {len(mid_freq_elements)}\")\n",
    "\n",
    "# Visualize memory usage breakdown\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Memory usage comparison\n",
    "names = [name for name, _ in representations]\n",
    "sizes = [size / 1024 / 1024 for _, size in representations]\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\"]\n",
    "\n",
    "bars = ax1.bar(names, sizes, color=colors, alpha=0.7)\n",
    "ax1.set_title(\"Memory Usage Comparison\")\n",
    "ax1.set_ylabel(\"Memory Usage (MB)\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes, strict=False):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.5,\n",
    "        f\"{size:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# Accuracy vs compression trade-off\n",
    "compression_levels = [1024, 256]\n",
    "mean_errors = [np.mean(compressed_errors) * 100, np.mean(highly_compressed_errors) * 100]\n",
    "memory_savings = [\n",
    "    (1 - compressed_memory[\"total\"] / original_size) * 100,\n",
    "    (1 - highly_compressed_memory[\"total\"] / original_size) * 100,\n",
    "]\n",
    "\n",
    "ax2.scatter(memory_savings, mean_errors, s=100, c=[\"blue\", \"orange\"], alpha=0.7)\n",
    "for i, level in enumerate(compression_levels):\n",
    "    ax2.annotate(\n",
    "        f\"{level} levels\",\n",
    "        (memory_savings[i], mean_errors[i]),\n",
    "        xytext=(5, 5),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "ax2.set_title(\"Accuracy vs Memory Savings Trade-off\")\n",
    "ax2.set_xlabel(\"Memory Savings (%)\")\n",
    "ax2.set_ylabel(\"Mean Relative Error (%)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nQuery Performance Comparison:\")\n",
    "print(\"-\" * 31)\n",
    "\n",
    "query_words = list(large_freq_dist.keys())[:1000]\n",
    "\n",
    "# Time each representation\n",
    "query_times = {}\n",
    "\n",
    "# Original dictionary\n",
    "start_time = time.time()\n",
    "_ = [large_freq_dist.get(word, 0) for word in query_words]\n",
    "query_times[\"Original\"] = time.time() - start_time\n",
    "\n",
    "# Compressed\n",
    "start_time = time.time()\n",
    "_ = [compressed_dist.get_count(word) for word in query_words]\n",
    "query_times[\"Compressed\"] = time.time() - start_time\n",
    "\n",
    "# Sparse\n",
    "start_time = time.time()\n",
    "_ = [sparse_dist.get_count(word) for word in query_words]\n",
    "query_times[\"Sparse\"] = time.time() - start_time\n",
    "\n",
    "for name, time_taken in query_times.items():\n",
    "    queries_per_sec = len(query_words) / time_taken\n",
    "    print(f\"{name:<12}: {time_taken:.4f}s ({queries_per_sec:,.0f} queries/sec)\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Compressed representations can save 50-80% memory\")\n",
    "print(\"- Sparse representations excel for distributions with many zeros\")\n",
    "print(\"- Trade-off between compression ratio and accuracy\")\n",
    "print(\"- Query performance generally comparable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling and Monitoring\n",
    "\n",
    "FreqProb provides tools for monitoring memory usage and optimizing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MEMORY PROFILING AND MONITORING\")\n",
    "print(\"=\" * 33)\n",
    "print()\n",
    "\n",
    "# Initialize memory profiler\n",
    "profiler = MemoryProfiler()\n",
    "\n",
    "print(\"Memory Profiling Demo:\")\n",
    "print(\"-\" * 22)\n",
    "\n",
    "# Profile different operations\n",
    "operations = [\n",
    "    (\"Create MLE Model\", lambda: freqprob.MLE(datasets[\"medium\"], logprob=False)),\n",
    "    (\n",
    "        \"Create Laplace Model\",\n",
    "        lambda: freqprob.Laplace(datasets[\"medium\"], bins=20000, logprob=False),\n",
    "    ),\n",
    "    (\"Create Compressed Dist\", lambda: create_compressed_distribution(datasets[\"medium\"])),\n",
    "    (\n",
    "        \"Vectorized Scoring\",\n",
    "        lambda: VectorizedScorer(freqprob.MLE(datasets[\"small\"])).score_batch(\n",
    "            list(datasets[\"small\"].keys())[:100]\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "profiling_results = []\n",
    "\n",
    "for op_name, operation in operations:\n",
    "    print(f\"Profiling: {op_name}...\")\n",
    "\n",
    "    with profiler.profile_operation(op_name):\n",
    "        result = operation()\n",
    "        # Force garbage collection to clean measurement\n",
    "        gc.collect()\n",
    "\n",
    "    metrics = profiler.get_latest_metrics()\n",
    "    profiling_results.append((op_name, metrics))\n",
    "\n",
    "    print(f\"  Memory delta: {metrics.memory_delta_mb:+6.2f} MB\")\n",
    "    print(f\"  Execution time: {metrics.execution_time:.4f} seconds\")\n",
    "    print()\n",
    "\n",
    "# Display profiling summary\n",
    "print(\"Profiling Summary:\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "summary = profiler.get_memory_summary()\n",
    "print(f\"Total snapshots taken: {summary['total_snapshots']}\")\n",
    "\n",
    "# Fix: Use the correct keys from the actual memory summary structure\n",
    "if \"rss_memory\" in summary:\n",
    "    rss_memory = summary[\"rss_memory\"]\n",
    "    print(f\"Peak memory usage: {rss_memory['max_mb']:.1f} MB\")\n",
    "    print(f\"Memory range: {rss_memory['min_mb']:.1f} - {rss_memory['max_mb']:.1f} MB\")\n",
    "else:\n",
    "    print(\"Memory summary not available (no snapshots taken)\")\n",
    "print()\n",
    "\n",
    "# Analyze memory efficiency of different representations\n",
    "print(\"Distribution Memory Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "analyzer = DistributionMemoryAnalyzer()\n",
    "\n",
    "# Analyze medium dataset\n",
    "analysis_dataset = datasets[\"medium\"]\n",
    "print(f\"Analyzing dataset with {len(analysis_dataset):,} elements...\")\n",
    "\n",
    "# Measure original distribution\n",
    "original_memory = analyzer.measure_distribution_memory(analysis_dataset)\n",
    "print(\"\\nOriginal distribution:\")\n",
    "print(f\"  Total memory: {original_memory['total_mb']:.1f} MB\")\n",
    "print(f\"  Elements: {original_memory['num_elements']:,}\")\n",
    "print(\n",
    "    f\"  Memory per element: {original_memory['total_mb'] * 1024 / original_memory['num_elements']:.1f} KB\"\n",
    ")\n",
    "\n",
    "# Compare all representations\n",
    "comparison = analyzer.compare_representations(analysis_dataset)\n",
    "\n",
    "print(\"\\nRepresentation Comparison:\")\n",
    "for repr_name, memory_info in comparison.items():\n",
    "    if repr_name != \"memory_savings\" and repr_name != \"profiling_metrics\":\n",
    "        print(f\"  {repr_name.capitalize()}:\")\n",
    "\n",
    "        # Fix: Handle different key structures\n",
    "        if \"total_mb\" in memory_info:\n",
    "            # Original distribution uses 'total_mb'\n",
    "            memory_mb = memory_info[\"total_mb\"]\n",
    "        elif \"total\" in memory_info:\n",
    "            # Other representations use 'total' in bytes\n",
    "            memory_mb = memory_info[\"total\"] / 1024 / 1024\n",
    "        else:\n",
    "            memory_mb = 0.0\n",
    "\n",
    "        print(f\"    Memory: {memory_mb:.1f} MB\")\n",
    "        if \"creation_time\" in memory_info:\n",
    "            print(f\"    Creation time: {memory_info['creation_time']:.3f} seconds\")\n",
    "\n",
    "print(\"\\nMemory Savings:\")\n",
    "if \"memory_savings\" in comparison:\n",
    "    savings = comparison[\"memory_savings\"]\n",
    "    for method, saving_info in savings.items():\n",
    "        print(f\"  {method.capitalize()}: {saving_info['percentage_savings']:.1f}% savings\")\n",
    "        print(f\"    Absolute: {saving_info['absolute_savings_mb']:.1f} MB\")\n",
    "\n",
    "# Memory trend monitoring\n",
    "print(\"\\nMemory Trend Analysis:\")\n",
    "print(\"-\" * 23)\n",
    "\n",
    "# Simulate memory usage over time\n",
    "memory_monitor = freqprob.MemoryMonitor(memory_threshold_mb=100.0, monitoring_interval=0.1)\n",
    "memory_monitor.start_monitoring()\n",
    "\n",
    "# Simulate workload\n",
    "workload_memory = []\n",
    "for _i in range(10):\n",
    "    # Create and destroy objects to simulate memory usage\n",
    "    temp_data = {f\"temp_{j}\": j for j in range(1000)}\n",
    "    temp_model = freqprob.MLE(temp_data, logprob=False)\n",
    "\n",
    "    # Check memory\n",
    "    alert = memory_monitor.check_memory()\n",
    "    current_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "    workload_memory.append(current_memory)\n",
    "\n",
    "    time.sleep(0.05)  # Brief pause\n",
    "    del temp_data, temp_model\n",
    "    gc.collect()\n",
    "\n",
    "memory_monitor.stop_monitoring()\n",
    "\n",
    "# Get monitoring report\n",
    "monitoring_report = memory_monitor.get_monitoring_report()\n",
    "print(f\"Monitoring duration: {monitoring_report['monitoring_duration']:.2f} seconds\")\n",
    "print(\"Memory statistics:\")\n",
    "for stat_name, value in monitoring_report[\"memory_statistics\"].items():\n",
    "    print(f\"  {stat_name}: {value:.1f} MB\")\n",
    "\n",
    "# Visualize profiling results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Memory usage by operation\n",
    "op_names = [name for name, _ in profiling_results]\n",
    "memory_deltas = [metrics.memory_delta_mb for _, metrics in profiling_results]\n",
    "execution_times = [metrics.execution_time for _, metrics in profiling_results]\n",
    "\n",
    "bars = axes[0, 0].bar(range(len(op_names)), memory_deltas, alpha=0.7)\n",
    "axes[0, 0].set_title(\"Memory Usage by Operation\")\n",
    "axes[0, 0].set_xlabel(\"Operations\")\n",
    "axes[0, 0].set_ylabel(\"Memory Delta (MB)\")\n",
    "axes[0, 0].set_xticks(range(len(op_names)))\n",
    "axes[0, 0].set_xticklabels([name.replace(\" \", \"\\n\") for name in op_names], rotation=0)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, memory_deltas, strict=False):\n",
    "    axes[0, 0].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.1,\n",
    "        f\"{value:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "# 2. Execution time by operation\n",
    "axes[0, 1].bar(range(len(op_names)), execution_times, alpha=0.7, color=\"orange\")\n",
    "axes[0, 1].set_title(\"Execution Time by Operation\")\n",
    "axes[0, 1].set_xlabel(\"Operations\")\n",
    "axes[0, 1].set_ylabel(\"Execution Time (seconds)\")\n",
    "axes[0, 1].set_xticks(range(len(op_names)))\n",
    "axes[0, 1].set_xticklabels([name.replace(\" \", \"\\n\") for name in op_names], rotation=0)\n",
    "\n",
    "# 3. Memory efficiency comparison - fix to handle different key structures\n",
    "repr_names = [\"Original\", \"Compressed\", \"Sparse\"]\n",
    "memory_usage = []\n",
    "\n",
    "# Extract memory usage with proper key handling\n",
    "for repr_name in [\"original\", \"compressed\", \"sparse\"]:\n",
    "    if repr_name in comparison:\n",
    "        memory_info = comparison[repr_name]\n",
    "        if \"total_mb\" in memory_info:\n",
    "            memory_usage.append(memory_info[\"total_mb\"])\n",
    "        elif \"total\" in memory_info:\n",
    "            memory_usage.append(memory_info[\"total\"] / 1024 / 1024)\n",
    "        else:\n",
    "            memory_usage.append(0.0)\n",
    "    else:\n",
    "        memory_usage.append(0.0)\n",
    "\n",
    "if memory_usage and memory_usage[0] > 0:  # Only plot if we have valid data\n",
    "    bars = axes[1, 0].bar(repr_names, memory_usage, alpha=0.7, color=[\"red\", \"blue\", \"green\"])\n",
    "    axes[1, 0].set_title(\"Memory Usage by Representation\")\n",
    "    axes[1, 0].set_ylabel(\"Memory Usage (MB)\")\n",
    "\n",
    "    # Add percentage savings\n",
    "    for i, (bar, usage) in enumerate(zip(bars, memory_usage, strict=False)):\n",
    "        if i > 0 and memory_usage[0] > 0:  # Skip original\n",
    "            savings_pct = (1 - usage / memory_usage[0]) * 100\n",
    "            axes[1, 0].text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                bar.get_height() + 0.5,\n",
    "                f\"-{savings_pct:.0f}%\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                color=\"green\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "else:\n",
    "    axes[1, 0].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"Memory comparison data not available\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[1, 0].transAxes,\n",
    "    )\n",
    "    axes[1, 0].set_title(\"Memory Usage by Representation\")\n",
    "\n",
    "# 4. Memory trend during workload\n",
    "axes[1, 1].plot(range(len(workload_memory)), workload_memory, \"b-\", linewidth=2, marker=\"o\")\n",
    "axes[1, 1].set_title(\"Memory Usage During Workload\")\n",
    "axes[1, 1].set_xlabel(\"Time Steps\")\n",
    "axes[1, 1].set_ylabel(\"Memory Usage (MB)\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add threshold line if available\n",
    "axes[1, 1].axhline(y=100, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Threshold\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nMemory Optimization Recommendations:\")\n",
    "print(\"=\" * 38)\n",
    "print(\"1. Use compressed representations for large vocabularies (50%+ savings)\")\n",
    "print(\"2. Consider sparse representations for distributions with many zeros\")\n",
    "print(\"3. Monitor memory usage during long-running processes\")\n",
    "print(\"4. Use streaming models for real-time applications with bounded memory\")\n",
    "print(\"5. Profile operations to identify memory bottlenecks\")\n",
    "print(\"6. Implement garbage collection strategies for batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarks and Best Practices\n",
    "\n",
    "Let's conclude with comprehensive benchmarks and practical recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PERFORMANCE BENCHMARKS AND BEST PRACTICES\")\n",
    "print(\"=\" * 44)\n",
    "print()\n",
    "\n",
    "# Comprehensive benchmark suite\n",
    "\n",
    "\n",
    "def run_comprehensive_benchmark():\n",
    "    \"\"\"Run comprehensive performance benchmark.\"\"\"\n",
    "\n",
    "    benchmark_results = {}\n",
    "\n",
    "    # Test datasets of different sizes\n",
    "    test_datasets = {\n",
    "        \"Small (1K)\": datasets[\"small\"],\n",
    "        \"Medium (10K)\": datasets[\"medium\"],\n",
    "        \"Large (50K)\": datasets[\"large\"],\n",
    "    }\n",
    "\n",
    "    for dataset_name, dataset in test_datasets.items():\n",
    "        print(f\"Benchmarking {dataset_name}...\")\n",
    "\n",
    "        dataset_results = {}\n",
    "        test_elements = list(dataset.keys())[: min(1000, len(dataset))]\n",
    "\n",
    "        # 1. Model creation time\n",
    "        def create_mle(ds):\n",
    "            return freqprob.MLE(ds, logprob=False)\n",
    "\n",
    "        def create_laplace(ds):\n",
    "            return freqprob.Laplace(ds, bins=len(ds) * 2, logprob=False)\n",
    "\n",
    "        def create_lazy_mle_func(ds):\n",
    "            return create_lazy_mle(ds, logprob=False)\n",
    "\n",
    "        models_to_test = [\n",
    "            (\"MLE\", create_mle),\n",
    "            (\"Laplace\", create_laplace),\n",
    "            (\"Lazy MLE\", create_lazy_mle_func),\n",
    "        ]\n",
    "\n",
    "        for model_name, model_creator in models_to_test:\n",
    "            # Creation time\n",
    "            start_time = time.time()\n",
    "            model = model_creator(dataset)\n",
    "            creation_time = time.time() - start_time\n",
    "\n",
    "            # Query time\n",
    "            start_time = time.time()\n",
    "            _ = [model(element) for element in test_elements[:100]]\n",
    "            query_time = time.time() - start_time\n",
    "\n",
    "            dataset_results[model_name] = {\n",
    "                \"creation_time\": creation_time,\n",
    "                \"query_time\": query_time,\n",
    "                \"queries_per_sec\": 100 / query_time,\n",
    "            }\n",
    "\n",
    "        # 2. Vectorized vs individual scoring\n",
    "        mle_model = freqprob.MLE(dataset, logprob=False)\n",
    "        vectorized_model = VectorizedScorer(mle_model)\n",
    "\n",
    "        # Individual scoring\n",
    "        start_time = time.time()\n",
    "        _ = [mle_model(element) for element in test_elements]\n",
    "        individual_time = time.time() - start_time\n",
    "\n",
    "        # Batch scoring\n",
    "        start_time = time.time()\n",
    "        _ = vectorized_model.score_batch(test_elements)\n",
    "        batch_time = time.time() - start_time\n",
    "\n",
    "        dataset_results[\"Vectorization\"] = {\n",
    "            \"individual_time\": individual_time,\n",
    "            \"batch_time\": batch_time,\n",
    "            \"speedup\": individual_time / batch_time,\n",
    "        }\n",
    "\n",
    "        # 3. Memory representations\n",
    "        if len(dataset) >= 5000:  # Only for larger datasets\n",
    "            # Compressed\n",
    "            start_time = time.time()\n",
    "            compressed = create_compressed_distribution(dataset)\n",
    "            compressed_creation_time = time.time() - start_time\n",
    "\n",
    "            # Sparse\n",
    "            start_time = time.time()\n",
    "            sparse = create_sparse_distribution(dataset)\n",
    "            sparse_creation_time = time.time() - start_time\n",
    "\n",
    "            dataset_results[\"Memory Representations\"] = {\n",
    "                \"compressed_creation\": compressed_creation_time,\n",
    "                \"sparse_creation\": sparse_creation_time,\n",
    "                \"compressed_memory\": compressed.get_memory_usage()[\"total\"] / 1024 / 1024,\n",
    "                \"sparse_memory\": sparse.get_memory_usage()[\"total\"] / 1024 / 1024,\n",
    "            }\n",
    "\n",
    "        benchmark_results[dataset_name] = dataset_results\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Running comprehensive benchmarks...\")\n",
    "benchmark_results = run_comprehensive_benchmark()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(\"=\" * 18)\n",
    "\n",
    "for dataset_name, results in benchmark_results.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(\"-\" * len(dataset_name))\n",
    "\n",
    "    # Model performance\n",
    "    print(\"Model Performance:\")\n",
    "    for model_name in [\"MLE\", \"Laplace\", \"Lazy MLE\"]:\n",
    "        if model_name in results:\n",
    "            r = results[model_name]\n",
    "            print(\n",
    "                f\"  {model_name:<10}: {r['creation_time']:.4f}s creation, {r['queries_per_sec']:,.0f} queries/sec\"\n",
    "            )\n",
    "\n",
    "    # Vectorization performance\n",
    "    if \"Vectorization\" in results:\n",
    "        v = results[\"Vectorization\"]\n",
    "        print(f\"  Vectorization: {v['speedup']:.1f}x speedup\")\n",
    "\n",
    "    # Memory representations\n",
    "    if \"Memory Representations\" in results:\n",
    "        m = results[\"Memory Representations\"]\n",
    "        print(f\"  Memory (Compressed): {m['compressed_memory']:.1f} MB\")\n",
    "        print(f\"  Memory (Sparse): {m['sparse_memory']:.1f} MB\")\n",
    "\n",
    "# Performance scaling analysis\n",
    "print(\"\\n\\nPerformance Scaling Analysis:\")\n",
    "print(\"=\" * 31)\n",
    "\n",
    "dataset_sizes = [len(datasets[\"small\"]), len(datasets[\"medium\"]), len(datasets[\"large\"])]\n",
    "mle_creation_times = []\n",
    "lazy_creation_times = []\n",
    "vectorization_speedups = []\n",
    "\n",
    "for dataset_name in [\"Small (1K)\", \"Medium (10K)\", \"Large (50K)\"]:\n",
    "    results = benchmark_results[dataset_name]\n",
    "    mle_creation_times.append(results[\"MLE\"][\"creation_time\"])\n",
    "    lazy_creation_times.append(results[\"Lazy MLE\"][\"creation_time\"])\n",
    "    vectorization_speedups.append(results[\"Vectorization\"][\"speedup\"])\n",
    "\n",
    "# Visualize scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Creation time scaling\n",
    "axes[0, 0].loglog(dataset_sizes, mle_creation_times, \"o-\", label=\"MLE\", linewidth=2, markersize=8)\n",
    "axes[0, 0].loglog(\n",
    "    dataset_sizes, lazy_creation_times, \"s-\", label=\"Lazy MLE\", linewidth=2, markersize=8\n",
    ")\n",
    "axes[0, 0].set_title(\"Model Creation Time Scaling\")\n",
    "axes[0, 0].set_xlabel(\"Dataset Size\")\n",
    "axes[0, 0].set_ylabel(\"Creation Time (seconds)\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Vectorization speedup\n",
    "axes[0, 1].semilogx(\n",
    "    dataset_sizes, vectorization_speedups, \"o-\", color=\"green\", linewidth=2, markersize=8\n",
    ")\n",
    "axes[0, 1].set_title(\"Vectorization Speedup by Dataset Size\")\n",
    "axes[0, 1].set_xlabel(\"Dataset Size\")\n",
    "axes[0, 1].set_ylabel(\"Speedup Factor\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Memory usage comparison (for larger datasets)\n",
    "memory_datasets = [\"Medium (10K)\", \"Large (50K)\"]\n",
    "compressed_memory = []\n",
    "sparse_memory = []\n",
    "\n",
    "for dataset_name in memory_datasets:\n",
    "    if \"Memory Representations\" in benchmark_results[dataset_name]:\n",
    "        m = benchmark_results[dataset_name][\"Memory Representations\"]\n",
    "        compressed_memory.append(m[\"compressed_memory\"])\n",
    "        sparse_memory.append(m[\"sparse_memory\"])\n",
    "\n",
    "if compressed_memory and sparse_memory:\n",
    "    x = np.arange(len(memory_datasets))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[1, 0].bar(x - width / 2, compressed_memory, width, label=\"Compressed\", alpha=0.8)\n",
    "    axes[1, 0].bar(x + width / 2, sparse_memory, width, label=\"Sparse\", alpha=0.8)\n",
    "\n",
    "    axes[1, 0].set_title(\"Memory Usage by Representation\")\n",
    "    axes[1, 0].set_xlabel(\"Dataset\")\n",
    "    axes[1, 0].set_ylabel(\"Memory Usage (MB)\")\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(memory_datasets)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "# 4. Queries per second comparison\n",
    "model_names = [\"MLE\", \"Laplace\", \"Lazy MLE\"]\n",
    "queries_per_sec = {model: [] for model in model_names}\n",
    "\n",
    "for dataset_name in [\"Small (1K)\", \"Medium (10K)\", \"Large (50K)\"]:\n",
    "    results = benchmark_results[dataset_name]\n",
    "    for model_name in model_names:\n",
    "        if model_name in results:\n",
    "            queries_per_sec[model_name].append(results[model_name][\"queries_per_sec\"])\n",
    "\n",
    "x = np.arange(len([\"Small\", \"Medium\", \"Large\"]))\n",
    "width = 0.25\n",
    "\n",
    "for i, (model_name, qps_values) in enumerate(queries_per_sec.items()):\n",
    "    if qps_values:\n",
    "        offset = (i - 1) * width\n",
    "        axes[1, 1].bar(x + offset, qps_values, width, label=model_name, alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_title(\"Query Performance by Dataset Size\")\n",
    "axes[1, 1].set_xlabel(\"Dataset Size\")\n",
    "axes[1, 1].set_ylabel(\"Queries per Second\")\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([\"Small\", \"Medium\", \"Large\"])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\nBEST PRACTICES SUMMARY\")\n",
    "print(\"=\" * 23)\n",
    "print()\n",
    "\n",
    "print(\"🚀 PERFORMANCE OPTIMIZATION:\")\n",
    "print(\"  • Use vectorized operations for batch processing (2-10x speedup)\")\n",
    "print(\"  • Choose lazy evaluation for sparse access patterns\")\n",
    "print(\"  • Implement caching for expensive computations (SGT, etc.)\")\n",
    "print(\"  • Profile operations to identify bottlenecks\")\n",
    "print()\n",
    "\n",
    "print(\"💾 MEMORY MANAGEMENT:\")\n",
    "print(\"  • Use compressed representations for large vocabularies (50%+ savings)\")\n",
    "print(\"  • Consider sparse representations for distributions with many zeros\")\n",
    "print(\"  • Implement streaming for real-time applications\")\n",
    "print(\"  • Monitor memory usage and set appropriate limits\")\n",
    "print()\n",
    "\n",
    "print(\"⚖️ TRADE-OFFS TO CONSIDER:\")\n",
    "print(\"  • Accuracy vs. Memory: Higher compression = lower accuracy\")\n",
    "print(\"  • Speed vs. Memory: Eager computation = faster queries, more memory\")\n",
    "print(\"  • Complexity vs. Performance: Advanced methods may not always be better\")\n",
    "print(\"  • Real-time vs. Batch: Streaming good for real-time, batch good for throughput\")\n",
    "print()\n",
    "\n",
    "print(\"🎯 CHOOSING THE RIGHT APPROACH:\")\n",
    "print(\"  • Small datasets (<10K): Standard implementations sufficient\")\n",
    "print(\"  • Medium datasets (10K-100K): Consider vectorization and compression\")\n",
    "print(\"  • Large datasets (>100K): Use streaming, compression, and lazy evaluation\")\n",
    "print(\"  • Real-time applications: Streaming models with bounded memory\")\n",
    "print(\"  • Batch processing: Vectorized operations with memory monitoring\")\n",
    "print()\n",
    "\n",
    "print(\"🔧 IMPLEMENTATION CHECKLIST:\")\n",
    "print(\"  ☐ Profile your specific use case\")\n",
    "print(\"  ☐ Choose appropriate data representations\")\n",
    "print(\"  ☐ Implement memory monitoring\")\n",
    "print(\"  ☐ Test with realistic data sizes\")\n",
    "print(\"  ☐ Validate accuracy vs. efficiency trade-offs\")\n",
    "print(\"  ☐ Plan for scaling and growth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this tutorial, you learned how to optimize FreqProb for large-scale, production applications:\n",
    "\n",
    "### Key Techniques Covered:\n",
    "\n",
    "1. **Vectorized Operations** - 2-10x speedup for batch processing\n",
    "2. **Caching and Memoization** - Automatic optimization for expensive computations\n",
    "3. **Lazy Evaluation** - Compute only what's needed, save memory\n",
    "4. **Streaming Updates** - Real-time learning with bounded memory\n",
    "5. **Memory-Efficient Representations** - 50-80% memory savings for large vocabularies\n",
    "6. **Memory Profiling** - Monitor and optimize memory usage\n",
    "\n",
    "### Performance Insights:\n",
    "\n",
    "- **Vectorization** provides consistent speedups across dataset sizes\n",
    "- **Lazy evaluation** is most beneficial for sparse access patterns\n",
    "- **Compressed representations** offer significant memory savings with minimal accuracy loss\n",
    "- **Streaming models** enable real-time applications with predictable memory usage\n",
    "\n",
    "### When to Use What:\n",
    "\n",
    "- **Small datasets** (<10K): Standard implementations are sufficient\n",
    "- **Medium datasets** (10K-100K): Add vectorization and consider compression\n",
    "- **Large datasets** (>100K): Use all efficiency features\n",
    "- **Real-time applications**: Streaming models with memory monitoring\n",
    "- **Batch processing**: Vectorized operations with profiling\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Apply these techniques** to your specific use case\n",
    "2. **Profile your applications** to identify bottlenecks\n",
    "3. **Try Tutorial 4**: Real-world NLP Applications\n",
    "4. **Experiment** with different combinations of techniques\n",
    "5. **Scale gradually** and monitor performance metrics\n",
    "\n",
    "Remember: **optimization is application-specific**. Always profile your specific use case and validate that optimizations actually improve performance for your data and usage patterns!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
