{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqProb Tutorial 2: Advanced Smoothing Methods\n",
    "\n",
    "This tutorial covers advanced smoothing techniques that are essential for modern NLP applications:\n",
    "\n",
    "1. **Simple Good-Turing smoothing** - Using frequency-of-frequencies\n",
    "2. **Kneser-Ney smoothing** - The gold standard for n-gram models\n",
    "3. **Modified Kneser-Ney** - Enhanced version with count-dependent discounting\n",
    "4. **Interpolated smoothing** - Combining multiple models\n",
    "5. **Bayesian smoothing** - Principled probabilistic approach\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import freqprob\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Advanced Smoothing Methods Tutorial\")\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We'll use a larger, more realistic dataset to demonstrate advanced smoothing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more substantial corpus for realistic frequency patterns\n",
    "corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"a dog runs fast in the park every morning\",\n",
    "    \"the cat sits quietly on the warm windowsill\",\n",
    "    \"brown bears roam freely in the dense forest\",\n",
    "    \"quick movements help animals escape from predators\",\n",
    "    \"lazy cats sleep most of the day in sunny spots\",\n",
    "    \"the forest contains many different species of animals\",\n",
    "    \"fast cars drive on the highway during rush hour\",\n",
    "    \"morning light filters through the trees in the forest\",\n",
    "    \"animals in the wild must find food and shelter\",\n",
    "    \"the dog barks loudly when strangers approach the house\",\n",
    "    \"cats and dogs are popular pets in many households\",\n",
    "    \"sunny weather brings people outdoors to enjoy nature\",\n",
    "    \"dense fog covers the mountains in the early morning\",\n",
    "    \"species diversity is important for ecosystem health\",\n",
    "]\n",
    "\n",
    "# Create word frequency distribution\n",
    "all_words = []\n",
    "for sentence in corpus:\n",
    "    all_words.extend(sentence.split())\n",
    "\n",
    "freqdist = Counter(all_words)\n",
    "print(\"Corpus statistics:\")\n",
    "print(f\"Total tokens: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(freqdist)}\")\n",
    "print(f\"Average frequency: {len(all_words) / len(freqdist):.2f}\")\n",
    "\n",
    "# Show frequency distribution\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in freqdist.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Analyze frequency-of-frequencies (crucial for Good-Turing)\n",
    "freq_of_freqs = Counter(freqdist.values())\n",
    "print(\"\\nFrequency-of-frequencies (r -> Nr):\")\n",
    "for r in sorted(freq_of_freqs.keys()):\n",
    "    print(f\"r={r}: {freq_of_freqs[r]} words appear {r} time(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Good-Turing Smoothing\n",
    "\n",
    "Good-Turing uses the frequency-of-frequencies to estimate probabilities for unseen events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Simple Good-Turing model\n",
    "try:\n",
    "    sgt = freqprob.SimpleGoodTuring(freqdist, logprob=False)\n",
    "    print(\"Simple Good-Turing model created successfully\")\n",
    "\n",
    "    # Test on sample words\n",
    "    test_words = [\"the\", \"cat\", \"forest\", \"elephant\"]  # Last one is unseen\n",
    "\n",
    "    print(\"Simple Good-Turing probabilities:\")\n",
    "    for word in test_words:\n",
    "        original_count = freqdist.get(word, 0)\n",
    "        sgt_prob = sgt(word)\n",
    "        print(f\"P({word}) = {sgt_prob:.6f} (original count: {original_count})\")\n",
    "\n",
    "    # Compare with MLE\n",
    "    mle_comparison = freqprob.MLE(freqdist, logprob=False)\n",
    "\n",
    "    print(\"\\nMLE vs Simple Good-Turing comparison:\")\n",
    "    print(f\"{'Word':<10} {'MLE':<10} {'SGT':<10} {'Difference':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for word in test_words:\n",
    "        mle_prob = mle_comparison(word)\n",
    "        sgt_prob = sgt(word)\n",
    "        diff = sgt_prob - mle_prob\n",
    "        print(f\"{word:<10} {mle_prob:<10.6f} {sgt_prob:<10.6f} {diff:<10.6f}\")\n",
    "\n",
    "    # Show probability mass for unseen events\n",
    "    unseen_prob = sgt(\"unseen_word\")\n",
    "    total_observed_mass = sum(sgt(word) for word in freqdist)\n",
    "    print(f\"\\nProbability for unseen words: {unseen_prob:.6f}\")\n",
    "    print(f\"Total probability mass for observed words: {total_observed_mass:.4f}\")\n",
    "    print(f\"Reserved mass for unseen events: {1 - total_observed_mass:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Simple Good-Turing model: {e}\")\n",
    "    print(\"This can happen with small datasets or irregular frequency patterns\")\n",
    "    sgt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Models Setup\n",
    "\n",
    "For Kneser-Ney smoothing, we need to work with n-grams. Let's create bigram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigrams from our corpus\n",
    "\n",
    "\n",
    "def generate_bigrams(text_corpus):\n",
    "    \"\"\"Generate bigrams with sentence boundaries.\"\"\"\n",
    "    bigrams = []\n",
    "    for sentence in text_corpus:\n",
    "        words = [\"<s>\", *sentence.split(), \"</s>\"]  # Add sentence boundaries\n",
    "        for i in range(len(words) - 1):\n",
    "            bigrams.append((words[i], words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "bigrams = generate_bigrams(corpus)\n",
    "bigram_freqdist = Counter(bigrams)\n",
    "\n",
    "print(\"Bigram statistics:\")\n",
    "print(f\"Total bigrams: {len(bigrams)}\")\n",
    "print(f\"Unique bigrams: {len(bigram_freqdist)}\")\n",
    "\n",
    "print(\"\\nMost common bigrams:\")\n",
    "for bigram, count in bigram_freqdist.most_common(10):\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "# Also create context counts for Kneser-Ney\n",
    "context_counts = Counter()\n",
    "word_contexts = defaultdict(set)\n",
    "\n",
    "for (context, word), count in bigram_freqdist.items():\n",
    "    context_counts[context] += count\n",
    "    word_contexts[word].add(context)\n",
    "\n",
    "print(f\"\\nNumber of unique contexts: {len(context_counts)}\")\n",
    "print(\"Most frequent contexts:\")\n",
    "for context, count in context_counts.most_common(5):\n",
    "    print(f\"'{context}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney Smoothing\n",
    "\n",
    "Kneser-Ney is the gold standard for n-gram language models. It uses absolute discounting and continuation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kneser-Ney model\n",
    "try:\n",
    "    # Default discount of 0.75 is commonly used\n",
    "    kn = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=False)\n",
    "    print(\"Kneser-Ney model created successfully\")\n",
    "\n",
    "    # Test on various bigrams\n",
    "    test_bigrams = [\n",
    "        (\"the\", \"cat\"),\n",
    "        (\"the\", \"dog\"),\n",
    "        (\"in\", \"the\"),\n",
    "        (\"of\", \"animals\"),\n",
    "        (\"elephant\", \"runs\"),  # Unseen bigram\n",
    "    ]\n",
    "\n",
    "    print(\"\\nKneser-Ney probabilities:\")\n",
    "    for bigram in test_bigrams:\n",
    "        original_count = bigram_freqdist.get(bigram, 0)\n",
    "        kn_prob = kn(bigram)\n",
    "        print(f\"{bigram!s:<20} (count={original_count}): P = {kn_prob:.6f}\")\n",
    "\n",
    "    # Compare with bigram MLE\n",
    "    bigram_mle = freqprob.MLE(bigram_freqdist, logprob=False)\n",
    "\n",
    "    observed_bigrams = [bg for bg in test_bigrams if bg in bigram_freqdist]\n",
    "    kn_probs = [kn(bg) for bg in observed_bigrams]\n",
    "    mle_probs = [bigram_mle(bg) for bg in observed_bigrams]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(observed_bigrams))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width / 2, mle_probs, width, label=\"Bigram MLE\", alpha=0.8)\n",
    "    plt.bar(x + width / 2, kn_probs, width, label=\"Kneser-Ney\", alpha=0.8)\n",
    "\n",
    "    plt.title(\"Bigram MLE vs Kneser-Ney\")\n",
    "    plt.xlabel(\"Bigrams\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(x, [str(bg) for bg in observed_bigrams], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Demonstrate continuation probability concept\n",
    "    print(\"\\nContinuation probability insight:\")\n",
    "    words_to_analyze = [\"the\", \"cat\", \"forest\"]\n",
    "\n",
    "    for word in words_to_analyze:\n",
    "        # Count how many different contexts this word appears in\n",
    "        contexts = len(word_contexts[word])\n",
    "        total_count = sum(count for (ctx, w), count in bigram_freqdist.items() if w == word)\n",
    "        print(f\"'{word}': appears {total_count} times in {contexts} different contexts\")\n",
    "\n",
    "    print(\"\\nKneser-Ney favors words that appear in many different contexts!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Kneser-Ney model: {e}\")\n",
    "    kn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Kneser-Ney Smoothing\n",
    "\n",
    "Modified Kneser-Ney uses different discount values for different frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modified Kneser-Ney model\n",
    "try:\n",
    "    mkn = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=False)\n",
    "    print(\"Modified Kneser-Ney model created successfully\")\n",
    "\n",
    "    # Compare KN vs MKN\n",
    "    if kn is not None:\n",
    "        print(\"\\nComparison: Kneser-Ney vs Modified Kneser-Ney\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "        for bigram in test_bigrams[:4]:  # Skip unseen for cleaner comparison\n",
    "            count = bigram_freqdist.get(bigram, 0)\n",
    "            kn_prob = kn(bigram)\n",
    "            mkn_prob = mkn(bigram)\n",
    "            print(f\"{bigram!s:<20} (c={count}): KN={kn_prob:.6f}, MKN={mkn_prob:.6f}\")\n",
    "\n",
    "        # Visualize differences\n",
    "        bigrams_to_plot = [bg for bg in test_bigrams[:4] if bg in bigram_freqdist]\n",
    "        kn_probs_plot = [kn(bg) for bg in bigrams_to_plot]\n",
    "        mkn_probs_plot = [mkn(bg) for bg in bigrams_to_plot]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x = np.arange(len(bigrams_to_plot))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.bar(x - width / 2, kn_probs_plot, width, label=\"Kneser-Ney\", alpha=0.8)\n",
    "        plt.bar(x + width / 2, mkn_probs_plot, width, label=\"Modified Kneser-Ney\", alpha=0.8)\n",
    "\n",
    "        plt.title(\"Kneser-Ney vs Modified Kneser-Ney\")\n",
    "        plt.xlabel(\"Bigrams\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.xticks(x, [str(bg) for bg in bigrams_to_plot], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Show discount values used by MKN\n",
    "        print(\"\\nModified Kneser-Ney uses count-dependent discounts:\")\n",
    "        print(\"- Different discount values for counts 1, 2, and 3+\")\n",
    "        print(\"- Automatically estimated from frequency-of-frequencies\")\n",
    "\n",
    "    else:\n",
    "        print(\"Regular Kneser-Ney not available for comparison\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Modified Kneser-Ney model: {e}\")\n",
    "    mkn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolated Smoothing\n",
    "\n",
    "Interpolated smoothing combines multiple models (e.g., trigram with bigram fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trigrams for interpolation example\n",
    "\n",
    "\n",
    "def generate_trigrams(text_corpus):\n",
    "    \"\"\"Generate trigrams with sentence boundaries.\"\"\"\n",
    "    trigrams = []\n",
    "    for sentence in text_corpus:\n",
    "        words = [\"<s>\", \"<s>\", *sentence.split(), \"</s>\", \"</s>\"]\n",
    "        for i in range(len(words) - 2):\n",
    "            trigrams.append((words[i], words[i + 1], words[i + 2]))\n",
    "    return trigrams\n",
    "\n",
    "\n",
    "trigrams = generate_trigrams(corpus)\n",
    "trigram_freqdist = Counter(trigrams)\n",
    "\n",
    "print(\"Trigram statistics:\")\n",
    "print(f\"Total trigrams: {len(trigrams)}\")\n",
    "print(f\"Unique trigrams: {len(trigram_freqdist)}\")\n",
    "\n",
    "print(\"\\nMost common trigrams:\")\n",
    "for trigram, count in trigram_freqdist.most_common(5):\n",
    "    print(f\"{trigram}: {count}\")\n",
    "\n",
    "# Create interpolated model (trigram + bigram)\n",
    "try:\n",
    "    # Lambda weight controls interpolation: λ * P_high + (1-λ) * P_low\n",
    "    lambda_weight = 0.7  # Favor trigrams\n",
    "\n",
    "    interpolated = freqprob.InterpolatedSmoothing(\n",
    "        trigram_freqdist, bigram_freqdist, lambda_weight=lambda_weight, logprob=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInterpolated model created (λ = {lambda_weight})\")\n",
    "\n",
    "    # Test on trigrams\n",
    "    test_trigrams = [\n",
    "        (\"the\", \"cat\", \"sits\"),\n",
    "        (\"in\", \"the\", \"forest\"),\n",
    "        (\"<s>\", \"the\", \"quick\"),\n",
    "        (\"animals\", \"in\", \"the\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\nInterpolated smoothing probabilities:\")\n",
    "    for trigram in test_trigrams:\n",
    "        count = trigram_freqdist.get(trigram, 0)\n",
    "        prob = interpolated(trigram)\n",
    "        print(f\"{trigram!s:<25} (count={count}): P = {prob:.6f}\")\n",
    "\n",
    "    # Compare different lambda values\n",
    "    lambda_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    test_trigram = (\"in\", \"the\", \"forest\")\n",
    "\n",
    "    probs_by_lambda = []\n",
    "    for lam in lambda_values:\n",
    "        interp_model = freqprob.InterpolatedSmoothing(\n",
    "            trigram_freqdist, bigram_freqdist, lambda_weight=lam, logprob=False\n",
    "        )\n",
    "        prob = interp_model(test_trigram)\n",
    "        probs_by_lambda.append(prob)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lambda_values, probs_by_lambda, \"o-\", linewidth=2, markersize=8)\n",
    "    plt.title(f\"Effect of lambda on P{test_trigram}\")\n",
    "    plt.xlabel(\"Lambda (lambda) - Weight for Higher-order Model\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axvline(\n",
    "        x=lambda_weight,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Current lambda = {lambda_weight}\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAs lambda increases, we rely more on trigram model (more specific context)\")\n",
    "    print(\"As lambda decreases, we rely more on bigram model (more general, better coverage)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating interpolated model: {e}\")\n",
    "    interpolated = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Smoothing\n",
    "\n",
    "Bayesian smoothing uses a Dirichlet prior for theoretically principled probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bayesian smoothing models with different priors\n",
    "alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "bayesian_models = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    bayesian_models[alpha] = freqprob.BayesianSmoothing(freqdist, alpha=alpha, logprob=False)\n",
    "\n",
    "print(\"Bayesian Smoothing with different alpha (concentration parameters):\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "test_words = [\"the\", \"cat\", \"forest\", \"elephant\"]  # Last one unseen\n",
    "\n",
    "for word in test_words:\n",
    "    count = freqdist.get(word, 0)\n",
    "    print(f\"\\nWord: '{word}' (count = {count})\")\n",
    "    print(f\"{'alpha':<6} {'Probability':<12} {'Effect':<20}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        prob = bayesian_models[alpha](word)\n",
    "        if alpha == 0.1:\n",
    "            effect = \"Minimal smoothing\"\n",
    "        elif alpha == 1.0:\n",
    "            effect = \"Uniform prior (Laplace)\"\n",
    "        elif alpha > 1.0:\n",
    "            effect = \"Strong uniform bias\"\n",
    "        else:\n",
    "            effect = \"Light smoothing\"\n",
    "\n",
    "        print(f\"{alpha:<6.1f} {prob:<12.6f} {effect}\")\n",
    "\n",
    "# Visualize the effect of alpha\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, word in enumerate(test_words):\n",
    "    if i >= 4:\n",
    "        break\n",
    "\n",
    "    probs = [bayesian_models[alpha](word) for alpha in alpha_values]\n",
    "\n",
    "    axes[i].semilogx(alpha_values, probs, \"o-\", linewidth=2, markersize=8)\n",
    "    axes[i].set_title(f'P(\"{word}\") vs alpha (count = {freqdist.get(word, 0)})')\n",
    "    axes[i].set_xlabel(\"Alpha (alpha)\")\n",
    "    axes[i].set_ylabel(\"Probability\")\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare Bayesian with other methods\n",
    "print(\"\\nComparison with other smoothing methods:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create comparison models\n",
    "laplace_model = freqprob.Laplace(freqdist, bins=1000, logprob=False)\n",
    "bayesian_model = bayesian_models[1.0]  # alpha = 1 is equivalent to Laplace\n",
    "optimal_bayesian = bayesian_models[0.5]  # Often a good choice\n",
    "\n",
    "comparison_models = {\n",
    "    \"Laplace\": laplace_model,\n",
    "    \"Bayesian (alpha=1.0)\": bayesian_model,\n",
    "    \"Bayesian (alpha=0.5)\": optimal_bayesian,\n",
    "}\n",
    "\n",
    "for word in [\"the\", \"cat\", \"elephant\"]:\n",
    "    print(f\"\\nP('{word}'):\")\n",
    "    for name, model in comparison_models.items():\n",
    "        prob = model(word)\n",
    "        print(f\"  {name:<18}: {prob:.6f}\")\n",
    "\n",
    "print(\"\\nKey insight: Bayesian smoothing with alpha=1.0 is equivalent to Laplace!\")\n",
    "print(\"The alpha parameter controls the strength of the uniform prior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create test set\ntest_corpus = [\n    \"the elephant walks slowly through the dense jungle\",\n    \"wild animals search for food in the morning\",\n    \"cats climb trees to escape from dangerous predators\",\n]\n\ntest_words = []\nfor sentence in test_corpus:\n    test_words.extend(sentence.split())\n\nprint(f\"Test set: {len(test_words)} words\")\nprint(f\"Words: {test_words}\")\n\n# Evaluate unigram models\nprint(\"\\nUnigram Model Evaluation (Perplexity):\")\nprint(\"=\" * 40)\n\nunigram_models = {\n    \"MLE\": freqprob.MLE(freqdist, logprob=True),\n    \"Laplace\": freqprob.Laplace(freqdist, bins=1000, logprob=True),\n    \"Bayesian (alpha=0.5)\": freqprob.BayesianSmoothing(freqdist, alpha=0.5, logprob=True),\n}\n\nif sgt is not None:\n    unigram_models[\"Simple Good-Turing\"] = freqprob.SimpleGoodTuring(freqdist, logprob=True)\n\nunigram_perplexities = {}\nfor name, model in unigram_models.items():\n    try:\n        pp = freqprob.perplexity(model, test_words)\n        unigram_perplexities[name] = pp\n        print(f\"{name:<20}: {pp:.2f}\")\n    except Exception as e:\n        print(f\"{name:<20}: Error - {str(e)[:30]}...\")\n\n# Evaluate bigram models (on bigram test data)\ntest_bigrams = []\nfor sentence in test_corpus:\n    words = [\"<s>\", *sentence.split(), \"</s>\"]\n    for i in range(len(words) - 1):\n        test_bigrams.append((words[i], words[i + 1]))\n\nprint(\"\\nBigram Model Evaluation (Perplexity):\")\nprint(\"=\" * 40)\n\nbigram_models = {\"Bigram MLE\": freqprob.MLE(bigram_freqdist, logprob=True)}\n\nif kn is not None:\n    bigram_models[\"Kneser-Ney\"] = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=True)\n\nif mkn is not None:\n    bigram_models[\"Modified Kneser-Ney\"] = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=True)\n\nbigram_perplexities = {}\nfor name, model in bigram_models.items():\n    try:\n        pp = freqprob.perplexity(model, test_bigrams)\n        bigram_perplexities[name] = pp\n        print(f\"{name:<20}: {pp:.2f}\")\n    except Exception as e:\n        print(f\"{name:<20}: Error - {str(e)[:30]}...\")\n\n# Visualize results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Unigram models\nif unigram_perplexities:\n    methods = list(unigram_perplexities.keys())\n    values = list(unigram_perplexities.values())\n\n    bars1 = ax1.bar(methods, values, alpha=0.8, color=plt.cm.Set3(np.linspace(0, 1, len(methods))))\n    ax1.set_title(\"Unigram Model Perplexity Comparison\")\n    ax1.set_ylabel(\"Perplexity (lower is better)\")\n    ax1.tick_params(axis=\"x\", rotation=45)\n\n    # Add value labels\n    for bar, value in zip(bars1, values, strict=False):\n        ax1.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.5,\n            f\"{value:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n\n# Bigram models\nif bigram_perplexities:\n    methods = list(bigram_perplexities.keys())\n    values = list(bigram_perplexities.values())\n\n    bars2 = ax2.bar(methods, values, alpha=0.8, color=plt.cm.Set2(np.linspace(0, 1, len(methods))))\n    ax2.set_title(\"Bigram Model Perplexity Comparison\")\n    ax2.set_ylabel(\"Perplexity (lower is better)\")\n    ax2.tick_params(axis=\"x\", rotation=45)\n\n    # Add value labels\n    for bar, value in zip(bars2, values, strict=False):\n        ax2.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.5,\n            f\"{value:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n\nplt.tight_layout()\nplt.show()\n\n# Find best models\nif unigram_perplexities:\n    best_unigram = min(unigram_perplexities.items(), key=lambda x: x[1])\n    print(f\"\\nBest unigram model: {best_unigram[0]} (PP = {best_unigram[1]:.2f})\")\n\nif bigram_perplexities:\n    best_bigram = min(bigram_perplexities.items(), key=lambda x: x[1])\n    print(f\"Best bigram model: {best_bigram[0]} (PP = {best_bigram[1]:.2f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create test set\ntest_corpus = [\n    \"the elephant walks slowly through the dense jungle\",\n    \"wild animals search for food in the morning\",\n    \"cats climb trees to escape from dangerous predators\",\n]\n\ntest_words = []\nfor sentence in test_corpus:\n    test_words.extend(sentence.split())\n\nprint(f\"Test set: {len(test_words)} words\")\nprint(f\"Words: {test_words}\")\n\n# Evaluate unigram models\nprint(\"\\nUnigram Model Evaluation (Perplexity):\")\nprint(\"=\" * 40)\n\nunigram_models = {\n    \"MLE\": freqprob.MLE(freqdist, logprob=True),\n    \"Laplace\": freqprob.Laplace(freqdist, bins=1000, logprob=True),\n    \"Bayesian (alpha=0.5)\": freqprob.BayesianSmoothing(freqdist, alpha=0.5, logprob=True),\n}\n\nif sgt is not None:\n    unigram_models[\"Simple Good-Turing\"] = freqprob.SimpleGoodTuring(freqdist, logprob=True)\n\nunigram_perplexities = {}\nfor name, model in unigram_models.items():\n    try:\n        pp = freqprob.perplexity(model, test_words)\n        unigram_perplexities[name] = pp\n        print(f\"{name:<20}: {pp:.2f}\")\n    except Exception as e:\n        print(f\"{name:<20}: Error - {str(e)[:30]}...\")\n\n# Evaluate bigram models (on bigram test data)\ntest_bigrams = []\nfor sentence in test_corpus:\n    words = [\"<s>\", *sentence.split(), \"</s>\"]\n    for i in range(len(words) - 1):\n        test_bigrams.append((words[i], words[i + 1]))\n\nprint(\"\\nBigram Model Evaluation (Perplexity):\")\nprint(\"=\" * 40)\n\nbigram_models = {\"Bigram MLE\": freqprob.MLE(bigram_freqdist, logprob=True)}\n\nif kn is not None:\n    bigram_models[\"Kneser-Ney\"] = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=True)\n\nif mkn is not None:\n    bigram_models[\"Modified Kneser-Ney\"] = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=True)\n\nbigram_perplexities = {}\nfor name, model in bigram_models.items():\n    try:\n        pp = freqprob.perplexity(model, test_bigrams)\n        bigram_perplexities[name] = pp\n        print(f\"{name:<20}: {pp:.2f}\")\n    except Exception as e:\n        print(f\"{name:<20}: Error - {str(e)[:30]}...\")\n\n# Visualize results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Unigram models\nif unigram_perplexities:\n    methods = list(unigram_perplexities.keys())\n    values = list(unigram_perplexities.values())\n\n    bars1 = ax1.bar(methods, values, alpha=0.8, color=plt.cm.Set3(np.linspace(0, 1, len(methods))))\n    ax1.set_title(\"Unigram Model Perplexity Comparison\")\n    ax1.set_ylabel(\"Perplexity (lower is better)\")\n    ax1.tick_params(axis=\"x\", rotation=45)\n\n    # Add value labels\n    for bar, value in zip(bars1, values, strict=False):\n        ax1.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.5,\n            f\"{value:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n\n# Bigram models\nif bigram_perplexities:\n    methods = list(bigram_perplexities.keys())\n    values = list(bigram_perplexities.values())\n\n    bars2 = ax2.bar(methods, values, alpha=0.8, color=plt.cm.Set2(np.linspace(0, 1, len(methods))))\n    ax2.set_title(\"Bigram Model Perplexity Comparison\")\n    ax2.set_ylabel(\"Perplexity (lower is better)\")\n    ax2.tick_params(axis=\"x\", rotation=45)\n\n    # Add value labels\n    for bar, value in zip(bars2, values, strict=False):\n        ax2.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.5,\n            f\"{value:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n\nplt.tight_layout()\nplt.show()\n\n# Find best models\nif unigram_perplexities:\n    best_unigram = min(unigram_perplexities.items(), key=lambda x: x[1])\n    print(f\"\\nBest unigram model: {best_unigram[0]} (PP = {best_unigram[1]:.2f})\")\n\nif bigram_perplexities:\n    best_bigram = min(bigram_perplexities.items(), key=lambda x: x[1])\n    print(f\"Best bigram model: {best_bigram[0]} (PP = {best_bigram[1]:.2f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"ADVANCED SMOOTHING METHODS: KEY INSIGHTS\")\nprint(\"=\" * 50)\nprint()\n\nprint(\"🔵 SIMPLE GOOD-TURING:\")\nprint(\"   • Uses frequency-of-frequencies statistics\")\nprint(\"   • Estimates probability of unseen events from singletons\")\nprint(\"   • Works well when frequency patterns are reliable\")\nprint(\"   • Can fail with sparse data or irregular patterns\")\nprint()\n\nprint(\"🟢 KNESER-NEY:\")\nprint(\"   • Gold standard for n-gram language models\")\nprint(\"   • Uses absolute discounting (subtract fixed amount)\")\nprint(\"   • Continuation probability: how likely is word in new contexts?\")\nprint(\"   • Particularly effective for bigrams and trigrams\")\nprint()\n\nprint(\"🟡 MODIFIED KNESER-NEY:\")\nprint(\"   • Enhanced version of Kneser-Ney\")\nprint(\"   • Different discount values for different frequency counts\")\nprint(\"   • Automatically estimates discounts from data\")\nprint(\"   • Generally performs better than standard Kneser-Ney\")\nprint()\n\nprint(\"🔴 INTERPOLATED SMOOTHING:\")\nprint(\"   • Combines multiple models (e.g., trigram + bigram)\")\nprint(\"   • Linear interpolation: lambda*P_high + (1-lambda)*P_low\")\nprint(\"   • Balances specificity with coverage\")\nprint(\"   • Essential for practical n-gram systems\")\nprint()\n\nprint(\"🟣 BAYESIAN SMOOTHING:\")\nprint(\"   • Theoretically principled using Dirichlet prior\")\nprint(\"   • alpha parameter controls prior strength\")\nprint(\"   • alpha=1.0 equivalent to Laplace smoothing\")\nprint(\"   • Good theoretical foundation\")\nprint()\n\n# Practical recommendations\nprint(\"🎯 PRACTICAL RECOMMENDATIONS:\")\nprint(\"=\" * 30)\nprint()\n\nprint(\"For Language Modeling:\")\nprint(\"  1. Start with Modified Kneser-Ney for n-grams\")\nprint(\"  2. Use interpolation for robustness\")\nprint(\"  3. Consider neural models for large datasets\")\nprint()\n\nprint(\"For General Frequency Estimation:\")\nprint(\"  1. Try Simple Good-Turing first\")\nprint(\"  2. Fall back to Bayesian smoothing if SGT fails\")\nprint(\"  3. Tune alpha parameter using validation data\")\nprint()\n\nprint(\"For Production Systems:\")\nprint(\"  1. Use interpolated smoothing for robustness\")\nprint(\"  2. Consider computational costs\")\nprint(\"  3. Validate on domain-specific data\")\nprint()\n\n# Show computational complexity\nprint(\"⚡ COMPUTATIONAL COMPLEXITY:\")\nprint(\"=\" * 28)\nmethods_complexity = {\n    \"Laplace/Bayesian\": \"O(1) per query\",\n    \"Simple Good-Turing\": \"O(V) preprocessing, O(1) query\",\n    \"Kneser-Ney\": \"O(N) preprocessing, O(1) query\",\n    \"Modified Kneser-Ney\": \"O(N) preprocessing, O(1) query\",\n    \"Interpolated\": \"O(k) per query (k models)\",\n}\n\nfor method, complexity in methods_complexity.items():\n    print(f\"  {method:<18}: {complexity}\")\n\nprint(\"\\n  V = vocabulary size, N = total n-grams, k = number of models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Advanced Method Selection\n",
    "\n",
    "Practice choosing the right advanced method for different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about advanced smoothing methods that are essential for modern NLP:\n",
    "\n",
    "### Methods Covered:\n",
    "1. **Simple Good-Turing** - Uses frequency-of-frequencies for principled probability estimation\n",
    "2. **Kneser-Ney** - The gold standard for n-gram language models with continuation probabilities\n",
    "3. **Modified Kneser-Ney** - Enhanced version with count-dependent discounting\n",
    "4. **Interpolated Smoothing** - Combines multiple models for robustness\n",
    "5. **Bayesian Smoothing** - Theoretically principled approach with Dirichlet priors\n",
    "\n",
    "### Key Insights:\n",
    "- **Kneser-Ney** dominates for n-gram language modeling\n",
    "- **Good-Turing** provides theoretical foundation for unseen event estimation\n",
    "- **Interpolation** is crucial for practical systems\n",
    "- **Bayesian methods** offer principled parameter control\n",
    "- **Context matters** - different methods excel in different scenarios\n",
    "\n",
    "### Next Steps:\n",
    "- **Tutorial 3**: Computational Efficiency and Memory Management\n",
    "- **Tutorial 4**: Real-world Applications and Case Studies\n",
    "- Practice implementing these methods on your own datasets\n",
    "- Experiment with hyperparameter tuning\n",
    "\n",
    "**Remember**: The best method depends on your specific use case, data characteristics, and computational constraints!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
