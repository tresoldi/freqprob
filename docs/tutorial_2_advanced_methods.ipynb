{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreqProb Tutorial 2: Advanced Smoothing Methods\n",
    "\n",
    "This tutorial covers advanced smoothing techniques that are essential for modern NLP applications:\n",
    "\n",
    "1. **Simple Good-Turing smoothing** - Using frequency-of-frequencies\n",
    "2. **Kneser-Ney smoothing** - The gold standard for n-gram models\n",
    "3. **Modified Kneser-Ney** - Enhanced version with count-dependent discounting\n",
    "4. **Interpolated smoothing** - Combining multiple models\n",
    "5. **Bayesian smoothing** - Principled probabilistic approach\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import freqprob\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Advanced Smoothing Methods Tutorial\")\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We'll use a larger, more realistic dataset to demonstrate advanced smoothing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more substantial corpus for realistic frequency patterns\n",
    "corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"a dog runs fast in the park every morning\",\n",
    "    \"the cat sits quietly on the warm windowsill\",\n",
    "    \"brown bears roam freely in the dense forest\",\n",
    "    \"quick movements help animals escape from predators\",\n",
    "    \"lazy cats sleep most of the day in sunny spots\",\n",
    "    \"the forest contains many different species of animals\",\n",
    "    \"fast cars drive on the highway during rush hour\",\n",
    "    \"morning light filters through the trees in the forest\",\n",
    "    \"animals in the wild must find food and shelter\",\n",
    "    \"the dog barks loudly when strangers approach the house\",\n",
    "    \"cats and dogs are popular pets in many households\",\n",
    "    \"sunny weather brings people outdoors to enjoy nature\",\n",
    "    \"dense fog covers the mountains in the early morning\",\n",
    "    \"species diversity is important for ecosystem health\",\n",
    "]\n",
    "\n",
    "# Create word frequency distribution\n",
    "all_words = []\n",
    "for sentence in corpus:\n",
    "    all_words.extend(sentence.split())\n",
    "\n",
    "freqdist = Counter(all_words)\n",
    "print(\"Corpus statistics:\")\n",
    "print(f\"Total tokens: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(freqdist)}\")\n",
    "print(f\"Average frequency: {len(all_words) / len(freqdist):.2f}\")\n",
    "\n",
    "# Show frequency distribution\n",
    "print(\"\\nMost common words:\")\n",
    "for word, count in freqdist.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Analyze frequency-of-frequencies (crucial for Good-Turing)\n",
    "freq_of_freqs = Counter(freqdist.values())\n",
    "print(\"\\nFrequency-of-frequencies (r -> Nr):\")\n",
    "for r in sorted(freq_of_freqs.keys()):\n",
    "    print(f\"r={r}: {freq_of_freqs[r]} words appear {r} time(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Good-Turing Smoothing\n",
    "\n",
    "Good-Turing uses the frequency-of-frequencies to estimate probabilities for unseen events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Simple Good-Turing model\n",
    "try:\n",
    "    sgt = freqprob.SimpleGoodTuring(freqdist, logprob=False)\n",
    "    print(\"Simple Good-Turing model created successfully\")\n",
    "\n",
    "    # Test on various words\n",
    "    test_words = [\"the\", \"cat\", \"forest\", \"species\", \"elephant\"]  # Last one is unseen\n",
    "\n",
    "    print(\"\\nSimple Good-Turing probabilities:\")\n",
    "    for word in test_words:\n",
    "        original_count = freqdist.get(word, 0)\n",
    "        sgt_prob = sgt(word)\n",
    "        print(f\"{word:<10} (count={original_count}): P = {sgt_prob:.6f}\")\n",
    "\n",
    "    # Compare with MLE\n",
    "    mle = freqprob.MLE(freqdist, logprob=False)\n",
    "\n",
    "    observed_words = [w for w in test_words if w in freqdist]\n",
    "    sgt_probs = [sgt(word) for word in observed_words]\n",
    "    mle_probs = [mle(word) for word in observed_words]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(observed_words))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width / 2, mle_probs, width, label=\"MLE\", alpha=0.8)\n",
    "    plt.bar(x + width / 2, sgt_probs, width, label=\"Simple Good-Turing\", alpha=0.8)\n",
    "\n",
    "    plt.title(\"MLE vs Simple Good-Turing\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(x, observed_words)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show probability mass for unseen events\n",
    "    unseen_prob = sgt(\"unseen_word\")\n",
    "    total_observed_mass = sum(sgt(word) for word in freqdist.keys())\n",
    "    print(f\"\\nProbability for unseen words: {unseen_prob:.6f}\")\n",
    "    print(f\"Total probability mass for observed words: {total_observed_mass:.4f}\")\n",
    "    print(f\"Reserved mass for unseen events: {1 - total_observed_mass:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Simple Good-Turing model: {e}\")\n",
    "    print(\"This can happen with small datasets or specific frequency patterns\")\n",
    "    sgt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Models Setup\n",
    "\n",
    "For Kneser-Ney smoothing, we need to work with n-grams. Let's create bigram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigrams from our corpus\n",
    "\n",
    "\n",
    "def generate_bigrams(text_corpus):\n",
    "    \"\"\"Generate bigrams with sentence boundaries.\"\"\"\n",
    "    bigrams = []\n",
    "    for sentence in text_corpus:\n",
    "        words = [\"<s>\"] + sentence.split() + [\"</s>\"]  # Add sentence boundaries\n",
    "        for i in range(len(words) - 1):\n",
    "            bigrams.append((words[i], words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "bigrams = generate_bigrams(corpus)\n",
    "bigram_freqdist = Counter(bigrams)\n",
    "\n",
    "print(\"Bigram statistics:\")\n",
    "print(f\"Total bigrams: {len(bigrams)}\")\n",
    "print(f\"Unique bigrams: {len(bigram_freqdist)}\")\n",
    "\n",
    "print(\"\\nMost common bigrams:\")\n",
    "for bigram, count in bigram_freqdist.most_common(10):\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "# Also create context counts for Kneser-Ney\n",
    "context_counts = Counter()\n",
    "word_contexts = defaultdict(set)\n",
    "\n",
    "for (context, word), count in bigram_freqdist.items():\n",
    "    context_counts[context] += count\n",
    "    word_contexts[word].add(context)\n",
    "\n",
    "print(f\"\\nNumber of unique contexts: {len(context_counts)}\")\n",
    "print(\"Most frequent contexts:\")\n",
    "for context, count in context_counts.most_common(5):\n",
    "    print(f\"'{context}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kneser-Ney Smoothing\n",
    "\n",
    "Kneser-Ney is the gold standard for n-gram language models. It uses absolute discounting and continuation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kneser-Ney model\n",
    "try:\n",
    "    # Default discount of 0.75 is commonly used\n",
    "    kn = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=False)\n",
    "    print(\"Kneser-Ney model created successfully\")\n",
    "\n",
    "    # Test on various bigrams\n",
    "    test_bigrams = [\n",
    "        (\"the\", \"cat\"),\n",
    "        (\"the\", \"dog\"),\n",
    "        (\"in\", \"the\"),\n",
    "        (\"of\", \"animals\"),\n",
    "        (\"elephant\", \"runs\"),  # Unseen bigram\n",
    "    ]\n",
    "\n",
    "    print(\"\\nKneser-Ney probabilities:\")\n",
    "    for bigram in test_bigrams:\n",
    "        original_count = bigram_freqdist.get(bigram, 0)\n",
    "        kn_prob = kn(bigram)\n",
    "        print(f\"{bigram!s:<20} (count={original_count}): P = {kn_prob:.6f}\")\n",
    "\n",
    "    # Compare with bigram MLE\n",
    "    bigram_mle = freqprob.MLE(bigram_freqdist, logprob=False)\n",
    "\n",
    "    observed_bigrams = [bg for bg in test_bigrams if bg in bigram_freqdist]\n",
    "    kn_probs = [kn(bg) for bg in observed_bigrams]\n",
    "    mle_probs = [bigram_mle(bg) for bg in observed_bigrams]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(observed_bigrams))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width / 2, mle_probs, width, label=\"Bigram MLE\", alpha=0.8)\n",
    "    plt.bar(x + width / 2, kn_probs, width, label=\"Kneser-Ney\", alpha=0.8)\n",
    "\n",
    "    plt.title(\"Bigram MLE vs Kneser-Ney\")\n",
    "    plt.xlabel(\"Bigrams\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(x, [str(bg) for bg in observed_bigrams], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Demonstrate continuation probability concept\n",
    "    print(\"\\nContinuation probability insight:\")\n",
    "    words_to_analyze = [\"the\", \"cat\", \"forest\"]\n",
    "\n",
    "    for word in words_to_analyze:\n",
    "        # Count how many different contexts this word appears in\n",
    "        contexts = len(word_contexts[word])\n",
    "        total_count = sum(count for (ctx, w), count in bigram_freqdist.items() if w == word)\n",
    "        print(f\"'{word}': appears {total_count} times in {contexts} different contexts\")\n",
    "\n",
    "    print(\"\\nKneser-Ney favors words that appear in many different contexts!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Kneser-Ney model: {e}\")\n",
    "    kn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Kneser-Ney Smoothing\n",
    "\n",
    "Modified Kneser-Ney uses different discount values for different frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Modified Kneser-Ney model\n",
    "try:\n",
    "    mkn = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=False)\n",
    "    print(\"Modified Kneser-Ney model created successfully\")\n",
    "\n",
    "    # Compare KN vs MKN\n",
    "    if kn is not None:\n",
    "        print(\"\\nComparison: Kneser-Ney vs Modified Kneser-Ney\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "        for bigram in test_bigrams[:4]:  # Skip unseen for cleaner comparison\n",
    "            count = bigram_freqdist.get(bigram, 0)\n",
    "            kn_prob = kn(bigram)\n",
    "            mkn_prob = mkn(bigram)\n",
    "            print(f\"{bigram!s:<20} (c={count}): KN={kn_prob:.6f}, MKN={mkn_prob:.6f}\")\n",
    "\n",
    "        # Visualize differences\n",
    "        bigrams_to_plot = [bg for bg in test_bigrams[:4] if bg in bigram_freqdist]\n",
    "        kn_probs_plot = [kn(bg) for bg in bigrams_to_plot]\n",
    "        mkn_probs_plot = [mkn(bg) for bg in bigrams_to_plot]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x = np.arange(len(bigrams_to_plot))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.bar(x - width / 2, kn_probs_plot, width, label=\"Kneser-Ney\", alpha=0.8)\n",
    "        plt.bar(x + width / 2, mkn_probs_plot, width, label=\"Modified Kneser-Ney\", alpha=0.8)\n",
    "\n",
    "        plt.title(\"Kneser-Ney vs Modified Kneser-Ney\")\n",
    "        plt.xlabel(\"Bigrams\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.xticks(x, [str(bg) for bg in bigrams_to_plot], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Show discount values used by MKN\n",
    "        print(\"\\nModified Kneser-Ney uses count-dependent discounts:\")\n",
    "        print(\"- Different discount values for counts 1, 2, and 3+\")\n",
    "        print(\"- Automatically estimated from frequency-of-frequencies\")\n",
    "\n",
    "    else:\n",
    "        print(\"Regular Kneser-Ney not available for comparison\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Modified Kneser-Ney model: {e}\")\n",
    "    mkn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolated Smoothing\n",
    "\n",
    "Interpolated smoothing combines multiple models (e.g., trigram with bigram fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trigrams for interpolation example\n",
    "\n",
    "\n",
    "def generate_trigrams(text_corpus):\n",
    "    \"\"\"Generate trigrams with sentence boundaries.\"\"\"\n",
    "    trigrams = []\n",
    "    for sentence in text_corpus:\n",
    "        words = [\"<s>\", \"<s>\"] + sentence.split() + [\"</s>\", \"</s>\"]\n",
    "        for i in range(len(words) - 2):\n",
    "            trigrams.append((words[i], words[i + 1], words[i + 2]))\n",
    "    return trigrams\n",
    "\n",
    "\n",
    "trigrams = generate_trigrams(corpus)\n",
    "trigram_freqdist = Counter(trigrams)\n",
    "\n",
    "print(\"Trigram statistics:\")\n",
    "print(f\"Total trigrams: {len(trigrams)}\")\n",
    "print(f\"Unique trigrams: {len(trigram_freqdist)}\")\n",
    "\n",
    "print(\"\\nMost common trigrams:\")\n",
    "for trigram, count in trigram_freqdist.most_common(5):\n",
    "    print(f\"{trigram}: {count}\")\n",
    "\n",
    "# Create interpolated model (trigram + bigram)\n",
    "try:\n",
    "    # Lambda weight controls interpolation: Î» * P_high + (1-Î») * P_low\n",
    "    lambda_weight = 0.7  # Favor trigrams\n",
    "\n",
    "    interpolated = freqprob.InterpolatedSmoothing(\n",
    "        trigram_freqdist, bigram_freqdist, lambda_weight=lambda_weight, logprob=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInterpolated model created (Î» = {lambda_weight})\")\n",
    "\n",
    "    # Test on trigrams\n",
    "    test_trigrams = [\n",
    "        (\"the\", \"cat\", \"sits\"),\n",
    "        (\"in\", \"the\", \"forest\"),\n",
    "        (\"<s>\", \"the\", \"quick\"),\n",
    "        (\"animals\", \"in\", \"the\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\nInterpolated smoothing probabilities:\")\n",
    "    for trigram in test_trigrams:\n",
    "        count = trigram_freqdist.get(trigram, 0)\n",
    "        prob = interpolated(trigram)\n",
    "        print(f\"{trigram!s:<25} (count={count}): P = {prob:.6f}\")\n",
    "\n",
    "    # Compare different lambda values\n",
    "    lambda_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    test_trigram = (\"in\", \"the\", \"forest\")\n",
    "\n",
    "    probs_by_lambda = []\n",
    "    for lam in lambda_values:\n",
    "        interp_model = freqprob.InterpolatedSmoothing(\n",
    "            trigram_freqdist, bigram_freqdist, lambda_weight=lam, logprob=False\n",
    "        )\n",
    "        prob = interp_model(test_trigram)\n",
    "        probs_by_lambda.append(prob)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lambda_values, probs_by_lambda, \"o-\", linewidth=2, markersize=8)\n",
    "    plt.title(f\"Effect of Î» on P{test_trigram}\")\n",
    "    plt.xlabel(\"Lambda (Î») - Weight for Higher-order Model\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axvline(\n",
    "        x=lambda_weight,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Current Î» = {lambda_weight}\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAs Î» increases, we rely more on trigram model (more specific context)\")\n",
    "    print(\"As Î» decreases, we rely more on bigram model (more general, better coverage)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating interpolated model: {e}\")\n",
    "    interpolated = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Smoothing\n",
    "\n",
    "Bayesian smoothing uses a Dirichlet prior for theoretically principled probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bayesian smoothing models with different priors\n",
    "alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "bayesian_models = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    bayesian_models[alpha] = freqprob.BayesianSmoothing(freqdist, alpha=alpha, logprob=False)\n",
    "\n",
    "print(\"Bayesian Smoothing with different Î± (concentration parameters):\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "test_words = [\"the\", \"cat\", \"forest\", \"elephant\"]  # Last one unseen\n",
    "\n",
    "for word in test_words:\n",
    "    count = freqdist.get(word, 0)\n",
    "    print(f\"\\nWord: '{word}' (count = {count})\")\n",
    "    print(f\"{'Î±':<6} {'Probability':<12} {'Effect':<20}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        prob = bayesian_models[alpha](word)\n",
    "        if alpha == 0.1:\n",
    "            effect = \"Minimal smoothing\"\n",
    "        elif alpha == 1.0:\n",
    "            effect = \"Uniform prior (Laplace)\"\n",
    "        elif alpha > 1.0:\n",
    "            effect = \"Strong uniform bias\"\n",
    "        else:\n",
    "            effect = \"Light smoothing\"\n",
    "\n",
    "        print(f\"{alpha:<6.1f} {prob:<12.6f} {effect}\")\n",
    "\n",
    "# Visualize the effect of alpha\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, word in enumerate(test_words):\n",
    "    if i >= 4:\n",
    "        break\n",
    "\n",
    "    probs = [bayesian_models[alpha](word) for alpha in alpha_values]\n",
    "\n",
    "    axes[i].semilogx(alpha_values, probs, \"o-\", linewidth=2, markersize=8)\n",
    "    axes[i].set_title(f'P(\"{word}\") vs Î± (count = {freqdist.get(word, 0)})')\n",
    "    axes[i].set_xlabel(\"Alpha (Î±)\")\n",
    "    axes[i].set_ylabel(\"Probability\")\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare Bayesian with other methods\n",
    "print(\"\\nComparison with other smoothing methods:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create comparison models\n",
    "laplace_model = freqprob.Laplace(freqdist, bins=1000, logprob=False)\n",
    "bayesian_model = bayesian_models[1.0]  # Î± = 1 is equivalent to Laplace\n",
    "optimal_bayesian = bayesian_models[0.5]  # Often a good choice\n",
    "\n",
    "comparison_models = {\n",
    "    \"Laplace\": laplace_model,\n",
    "    \"Bayesian (Î±=1.0)\": bayesian_model,\n",
    "    \"Bayesian (Î±=0.5)\": optimal_bayesian,\n",
    "}\n",
    "\n",
    "for word in [\"the\", \"cat\", \"elephant\"]:\n",
    "    print(f\"\\nP('{word}'):\")\n",
    "    for name, model in comparison_models.items():\n",
    "        prob = model(word)\n",
    "        print(f\"  {name:<18}: {prob:.6f}\")\n",
    "\n",
    "print(\"\\nKey insight: Bayesian smoothing with Î±=1.0 is equivalent to Laplace!\")\n",
    "print(\"The Î± parameter controls the strength of the uniform prior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Evaluation\n",
    "\n",
    "Let's compare all advanced methods using perplexity on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set\n",
    "test_corpus = [\n",
    "    \"the elephant walks slowly through the dense jungle\",\n",
    "    \"wild animals search for food in the morning\",\n",
    "    \"cats climb trees to escape from dangerous predators\",\n",
    "]\n",
    "\n",
    "test_words = []\n",
    "for sentence in test_corpus:\n",
    "    test_words.extend(sentence.split())\n",
    "\n",
    "print(f\"Test set: {len(test_words)} words\")\n",
    "print(f\"Words: {test_words}\")\n",
    "\n",
    "# Evaluate unigram models\n",
    "print(\"\\nUnigram Model Evaluation (Perplexity):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "unigram_models = {\n",
    "    \"MLE\": freqprob.MLE(freqdist, logprob=True),\n",
    "    \"Laplace\": freqprob.Laplace(freqdist, bins=1000, logprob=True),\n",
    "    \"Bayesian (Î±=0.5)\": freqprob.BayesianSmoothing(freqdist, alpha=0.5, logprob=True),\n",
    "}\n",
    "\n",
    "if sgt is not None:\n",
    "    unigram_models[\"Simple Good-Turing\"] = freqprob.SimpleGoodTuring(freqdist, logprob=True)\n",
    "\n",
    "unigram_perplexities = {}\n",
    "for name, model in unigram_models.items():\n",
    "    try:\n",
    "        pp = freqprob.perplexity(model, test_words)\n",
    "        unigram_perplexities[name] = pp\n",
    "        print(f\"{name:<20}: {pp:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:<20}: Error - {str(e)[:30]}...\")\n",
    "\n",
    "# Evaluate bigram models (on bigram test data)\n",
    "test_bigrams = []\n",
    "for sentence in test_corpus:\n",
    "    words = [\"<s>\"] + sentence.split() + [\"</s>\"]\n",
    "    for i in range(len(words) - 1):\n",
    "        test_bigrams.append((words[i], words[i + 1]))\n",
    "\n",
    "print(\"\\nBigram Model Evaluation (Perplexity):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "bigram_models = {\"Bigram MLE\": freqprob.MLE(bigram_freqdist, logprob=True)}\n",
    "\n",
    "if kn is not None:\n",
    "    bigram_models[\"Kneser-Ney\"] = freqprob.KneserNey(bigram_freqdist, discount=0.75, logprob=True)\n",
    "\n",
    "if mkn is not None:\n",
    "    bigram_models[\"Modified Kneser-Ney\"] = freqprob.ModifiedKneserNey(bigram_freqdist, logprob=True)\n",
    "\n",
    "bigram_perplexities = {}\n",
    "for name, model in bigram_models.items():\n",
    "    try:\n",
    "        pp = freqprob.perplexity(model, test_bigrams)\n",
    "        bigram_perplexities[name] = pp\n",
    "        print(f\"{name:<20}: {pp:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:<20}: Error - {str(e)[:30]}...\")\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Unigram models\n",
    "if unigram_perplexities:\n",
    "    methods = list(unigram_perplexities.keys())\n",
    "    values = list(unigram_perplexities.values())\n",
    "\n",
    "    bars1 = ax1.bar(methods, values, alpha=0.8, color=plt.cm.Set3(np.linspace(0, 1, len(methods))))\n",
    "    ax1.set_title(\"Unigram Model Perplexity Comparison\")\n",
    "    ax1.set_ylabel(\"Perplexity (lower is better)\")\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, values, strict=False):\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.5,\n",
    "            f\"{value:.1f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "# Bigram models\n",
    "if bigram_perplexities:\n",
    "    methods = list(bigram_perplexities.keys())\n",
    "    values = list(bigram_perplexities.values())\n",
    "\n",
    "    bars2 = ax2.bar(methods, values, alpha=0.8, color=plt.cm.Set2(np.linspace(0, 1, len(methods))))\n",
    "    ax2.set_title(\"Bigram Model Perplexity Comparison\")\n",
    "    ax2.set_ylabel(\"Perplexity (lower is better)\")\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars2, values, strict=False):\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.5,\n",
    "            f\"{value:.1f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best models\n",
    "if unigram_perplexities:\n",
    "    best_unigram = min(unigram_perplexities.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nBest unigram model: {best_unigram[0]} (PP = {best_unigram[1]:.2f})\")\n",
    "\n",
    "if bigram_perplexities:\n",
    "    best_bigram = min(bigram_perplexities.items(), key=lambda x: x[1])\n",
    "    print(f\"Best bigram model: {best_bigram[0]} (PP = {best_bigram[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Methods: Key Insights\n",
    "\n",
    "Let's summarize the key insights from each advanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADVANCED SMOOTHING METHODS: KEY INSIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "print(\"ðŸ”µ SIMPLE GOOD-TURING:\")\n",
    "print(\"   â€¢ Uses frequency-of-frequencies statistics\")\n",
    "print(\"   â€¢ Estimates probability of unseen events from singletons\")\n",
    "print(\"   â€¢ Works well when frequency patterns are reliable\")\n",
    "print(\"   â€¢ Can fail with sparse data or irregular patterns\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸŸ¢ KNESER-NEY:\")\n",
    "print(\"   â€¢ Gold standard for n-gram language models\")\n",
    "print(\"   â€¢ Uses absolute discounting (subtract fixed amount)\")\n",
    "print(\"   â€¢ Continuation probability: how likely is word in new contexts?\")\n",
    "print(\"   â€¢ Particularly effective for bigrams and trigrams\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸŸ¡ MODIFIED KNESER-NEY:\")\n",
    "print(\"   â€¢ Enhanced version of Kneser-Ney\")\n",
    "print(\"   â€¢ Different discount values for different frequency counts\")\n",
    "print(\"   â€¢ Automatically estimates discounts from data\")\n",
    "print(\"   â€¢ Generally performs better than standard Kneser-Ney\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ”´ INTERPOLATED SMOOTHING:\")\n",
    "print(\"   â€¢ Combines multiple models (e.g., trigram + bigram)\")\n",
    "print(\"   â€¢ Linear interpolation: Î»*P_high + (1-Î»)*P_low\")\n",
    "print(\"   â€¢ Balances specificity with coverage\")\n",
    "print(\"   â€¢ Essential for practical n-gram systems\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸŸ£ BAYESIAN SMOOTHING:\")\n",
    "print(\"   â€¢ Theoretically principled using Dirichlet prior\")\n",
    "print(\"   â€¢ Î± parameter controls prior strength\")\n",
    "print(\"   â€¢ Î±=1.0 equivalent to Laplace smoothing\")\n",
    "print(\"   â€¢ Good theoretical foundation\")\n",
    "print()\n",
    "\n",
    "# Practical recommendations\n",
    "print(\"ðŸŽ¯ PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"=\" * 30)\n",
    "print()\n",
    "\n",
    "print(\"For Language Modeling:\")\n",
    "print(\"  1. Start with Modified Kneser-Ney for n-grams\")\n",
    "print(\"  2. Use interpolation for robustness\")\n",
    "print(\"  3. Consider neural models for large datasets\")\n",
    "print()\n",
    "\n",
    "print(\"For General Frequency Estimation:\")\n",
    "print(\"  1. Try Simple Good-Turing first\")\n",
    "print(\"  2. Fall back to Bayesian smoothing if SGT fails\")\n",
    "print(\"  3. Tune Î± parameter using validation data\")\n",
    "print()\n",
    "\n",
    "print(\"For Production Systems:\")\n",
    "print(\"  1. Use interpolated smoothing for robustness\")\n",
    "print(\"  2. Consider computational costs\")\n",
    "print(\"  3. Validate on domain-specific data\")\n",
    "print()\n",
    "\n",
    "# Show computational complexity\n",
    "print(\"âš¡ COMPUTATIONAL COMPLEXITY:\")\n",
    "print(\"=\" * 28)\n",
    "methods_complexity = {\n",
    "    \"Laplace/Bayesian\": \"O(1) per query\",\n",
    "    \"Simple Good-Turing\": \"O(V) preprocessing, O(1) query\",\n",
    "    \"Kneser-Ney\": \"O(N) preprocessing, O(1) query\",\n",
    "    \"Modified Kneser-Ney\": \"O(N) preprocessing, O(1) query\",\n",
    "    \"Interpolated\": \"O(k) per query (k models)\",\n",
    "}\n",
    "\n",
    "for method, complexity in methods_complexity.items():\n",
    "    print(f\"  {method:<18}: {complexity}\")\n",
    "\n",
    "print(\"\\n  V = vocabulary size, N = total n-grams, k = number of models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Advanced Method Selection\n",
    "\n",
    "Practice choosing the right advanced method for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXERCISE: METHOD SELECTION SCENARIOS\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        \"scenario\": \"Building a mobile keyboard autocomplete\",\n",
    "        \"constraints\": [\"Low memory\", \"Fast queries\", \"Good for common phrases\"],\n",
    "        \"recommendation\": \"Modified Kneser-Ney with interpolation\",\n",
    "        \"reasoning\": \"Excellent performance, reasonable memory, handles phrases well\",\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Academic paper word frequency analysis\",\n",
    "        \"constraints\": [\"Small dataset\", \"Statistical rigor\", \"Handle rare terms\"],\n",
    "        \"recommendation\": \"Simple Good-Turing or Bayesian smoothing\",\n",
    "        \"reasoning\": \"Theoretical foundation, good for sparse data, handles rare terms\",\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Real-time speech recognition scoring\",\n",
    "        \"constraints\": [\"Very fast queries\", \"Memory efficient\", \"Decent accuracy\"],\n",
    "        \"recommendation\": \"Interpolated Laplace (unigram + bigram)\",\n",
    "        \"reasoning\": \"Fast computation, simple implementation, reasonable performance\",\n",
    "    },\n",
    "    {\n",
    "        \"scenario\": \"Large-scale web search ranking\",\n",
    "        \"constraints\": [\"Huge vocabulary\", \"High accuracy\", \"Scalable\"],\n",
    "        \"recommendation\": \"Neural language models with traditional fallback\",\n",
    "        \"reasoning\": \"Scale and accuracy requirements exceed traditional methods\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(scenarios, 1):\n",
    "    print(f\"SCENARIO {i}: {scenario['scenario']}\")\n",
    "    print(f\"Constraints: {', '.join(scenario['constraints'])}\")\n",
    "    print(f\"Recommended: {scenario['recommendation']}\")\n",
    "    print(f\"Reasoning: {scenario['reasoning']}\")\n",
    "    print()\n",
    "\n",
    "# Interactive exercise\n",
    "print(\"YOUR TURN:\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Consider a machine translation system that:\")\n",
    "print(\"- Processes multiple languages\")\n",
    "print(\"- Needs good performance on rare phrases\")\n",
    "print(\"- Has moderate computational resources\")\n",
    "print(\"- Must handle domain adaptation\")\n",
    "print()\n",
    "print(\"Which smoothing method(s) would you choose and why?\")\n",
    "print(\"Consider:\")\n",
    "print(\"1. Primary smoothing method for the base model\")\n",
    "print(\"2. How to handle domain adaptation\")\n",
    "print(\"3. Multi-language considerations\")\n",
    "print(\"4. Performance vs. accuracy trade-offs\")\n",
    "print()\n",
    "print(\"Hint: Think about interpolation and multiple model orders!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about advanced smoothing methods that are essential for modern NLP:\n",
    "\n",
    "### Methods Covered:\n",
    "1. **Simple Good-Turing** - Uses frequency-of-frequencies for principled probability estimation\n",
    "2. **Kneser-Ney** - The gold standard for n-gram language models with continuation probabilities\n",
    "3. **Modified Kneser-Ney** - Enhanced version with count-dependent discounting\n",
    "4. **Interpolated Smoothing** - Combines multiple models for robustness\n",
    "5. **Bayesian Smoothing** - Theoretically principled approach with Dirichlet priors\n",
    "\n",
    "### Key Insights:\n",
    "- **Kneser-Ney** dominates for n-gram language modeling\n",
    "- **Good-Turing** provides theoretical foundation for unseen event estimation\n",
    "- **Interpolation** is crucial for practical systems\n",
    "- **Bayesian methods** offer principled parameter control\n",
    "- **Context matters** - different methods excel in different scenarios\n",
    "\n",
    "### Next Steps:\n",
    "- **Tutorial 3**: Computational Efficiency and Memory Management\n",
    "- **Tutorial 4**: Real-world Applications and Case Studies\n",
    "- Practice implementing these methods on your own datasets\n",
    "- Experiment with hyperparameter tuning\n",
    "\n",
    "**Remember**: The best method depends on your specific use case, data characteristics, and computational constraints!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
